{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7bb640b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "429c4397",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from lib.test.tracker.basetracker import BaseTracker\n",
    "import time\n",
    "import yaml\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils import checkpoint\n",
    "import math\n",
    "#from lib.test.tracker.utils import sample_target, transform_image_to_crop\n",
    "import cv2\n",
    "#from lib.utils.box_ops import box_xywh_to_xyxy, box_xyxy_to_cxcywh\n",
    "#from lib.test.utils.hann import hann2d\n",
    "#from lib.models.mcitrack import build_mcitrack\n",
    "#from lib.test.tracker.utils import Preprocessor\n",
    "#from lib.utils.box_ops import clip_box\n",
    "import numpy as np\n",
    "import os\n",
    "import fastitpn as fastitpn_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4bde567b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBase(nn.Module):\n",
    "\n",
    "    def __init__(self, encoder: nn.Module, train_encoder: bool, open_layers: list, num_channels: int):\n",
    "        super().__init__()\n",
    "        open_blocks = open_layers[2:]\n",
    "        open_items = open_layers[0:2]\n",
    "        for name, parameter in encoder.named_parameters():\n",
    "\n",
    "            if not train_encoder:\n",
    "                freeze = True\n",
    "                for open_block in open_blocks:\n",
    "                    if open_block in name:\n",
    "                        freeze = False\n",
    "                if name in open_items:\n",
    "                    freeze = False\n",
    "                if freeze == True:\n",
    "                    parameter.requires_grad_(False)  # here should allow users to specify which layers to freeze !\n",
    "\n",
    "        self.body = encoder\n",
    "        self.num_channels = num_channels\n",
    "\n",
    "    def forward(self, template_list, search_list, template_anno_list):\n",
    "        xs = self.body(template_list, search_list, template_anno_list)\n",
    "        return xs\n",
    "\n",
    "\n",
    "#fast_itpn_tiny_1600e_1k\n",
    "\n",
    "class Encoder(EncoderBase):\n",
    "    \"\"\"FastITPN encoder.\"\"\"\n",
    "    def __init__(self, name: str,\n",
    "                 train_encoder: bool,\n",
    "                 pretrain_type: str,\n",
    "                 search_size: int,\n",
    "                 search_number: int,\n",
    "                 template_size: int,\n",
    "                 template_number: int,\n",
    "                 open_layers: list,\n",
    "                 cfg=None):\n",
    "        if \"fastitpn\" in name.lower():\n",
    "            encoder = getattr(fastitpn_module, name)(\n",
    "                pretrained=True,\n",
    "                search_size=search_size,\n",
    "                template_size=template_size,\n",
    "                drop_rate=0.0,\n",
    "                drop_path_rate=0.1,\n",
    "                attn_drop_rate=0.0,\n",
    "                init_values=0.1,\n",
    "                drop_block_rate=None,\n",
    "                use_mean_pooling=True,\n",
    "                grad_ckpt=cfg.MODEL.ENCODER.GRAD_CKPT,\n",
    "                pos_type=cfg.MODEL.ENCODER.POS_TYPE,\n",
    "                token_type_indicate=cfg.MODEL.ENCODER.TOKEN_TYPE_INDICATE,\n",
    "                pretrain_type = cfg.MODEL.ENCODER.PRETRAIN_TYPE,\n",
    "            )\n",
    "            if \"itpnb\" in name:\n",
    "                num_channels = 512\n",
    "            elif \"itpnl\" in name:\n",
    "                num_channels = 768\n",
    "            elif \"itpnt\" in name:\n",
    "                num_channels = 384\n",
    "            elif \"itpns\" in name:\n",
    "                num_channels = 384\n",
    "            else:\n",
    "                num_channels = 512\n",
    "        else:\n",
    "            raise ValueError()\n",
    "        super().__init__(encoder, train_encoder, open_layers, num_channels)\n",
    "\n",
    "def build_encoder(cfg):\n",
    "    train_encoder = (cfg.TRAIN.ENCODER_MULTIPLIER > 0) and (cfg.TRAIN.FREEZE_ENCODER == False)\n",
    "    encoder = Encoder(cfg.MODEL.ENCODER.TYPE, train_encoder,\n",
    "                      cfg.MODEL.ENCODER.PRETRAIN_TYPE,\n",
    "                      cfg.DATA.SEARCH.SIZE, cfg.DATA.SEARCH.NUMBER,\n",
    "                      cfg.DATA.TEMPLATE.SIZE, cfg.DATA.TEMPLATE.NUMBER,\n",
    "                      cfg.TRAIN.ENCODER_OPEN, cfg)\n",
    "    return encoder\n",
    "\n",
    "class MambaBlock(nn.Module):\n",
    "    def __init__(self,dt_scale, d_model,d_inner,dt_rank,d_state,bias,d_conv,conv_bias,dt_init,dt_max,dt_min,dt_init_floor):\n",
    "        super().__init__()\n",
    "        #  projects block input from D to 2*ED (two branches)\n",
    "        self.dt_scale = dt_scale\n",
    "        self.d_model = d_model\n",
    "        self.d_inner = d_inner\n",
    "        self.dt_rank = dt_rank\n",
    "        self.d_state = d_state\n",
    "        self.in_proj = nn.Linear(self.d_model, 2 * self.d_inner, bias=bias)\n",
    "\n",
    "        self.conv1d = nn.Conv1d(in_channels=self.d_inner, out_channels=self.d_inner,\n",
    "                                kernel_size=d_conv, bias=conv_bias,\n",
    "                                groups=self.d_inner,\n",
    "                                padding=(d_conv - 1)//2)\n",
    "\n",
    "        #  projects x to input-dependent Δ, B, C\n",
    "        self.x_proj = nn.Linear(self.d_inner, self.dt_rank + 2 * self.d_state, bias=False)\n",
    "\n",
    "        #  projects Δ from dt_rank to d_inner\n",
    "        self.dt_proj = nn.Linear(self.dt_rank, self.d_inner, bias=True)\n",
    "\n",
    "        #  dt initialization\n",
    "        #  dt weights\n",
    "        dt_init_std = self.dt_rank ** -0.5 * self.dt_scale\n",
    "        if dt_init == \"constant\":\n",
    "            nn.init.constant_(self.dt_proj.weight, dt_init_std)\n",
    "        elif dt_init == \"random\":\n",
    "            nn.init.uniform_(self.dt_proj.weight, -dt_init_std, dt_init_std)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        # dt bias\n",
    "        dt = torch.exp(\n",
    "            torch.rand(self.d_inner) * (math.log(dt_max) - math.log(dt_min)) + math.log(dt_min)\n",
    "        ).clamp(min=dt_init_floor)\n",
    "        inv_dt = dt + torch.log(\n",
    "            -torch.expm1(-dt))  #  inverse of softplus: https://github.com/pytorch/pytorch/issues/72759\n",
    "        with torch.no_grad():\n",
    "            self.dt_proj.bias.copy_(inv_dt)\n",
    "        # self.dt_proj.bias._no_reinit = True # initialization would set all Linear.bias to zero, need to mark this one as _no_reinit\n",
    "        #  todo : explain why removed\n",
    "\n",
    "        # S4D real initialization\n",
    "        A = torch.arange(1, self.d_state + 1, dtype=torch.float32).repeat(self.d_inner, 1)\n",
    "        self.A_log = nn.Parameter(\n",
    "            torch.log(A))  # why store A in log ? to keep A < 0 (cf -torch.exp(...)) ? for gradient stability ?\n",
    "        self.D = nn.Parameter(torch.ones(self.d_inner))\n",
    "\n",
    "        #  projects block output from ED back to D\n",
    "        self.out_proj = nn.Linear(self.d_inner, self.d_model, bias=bias)\n",
    "    def forward(self, x, h):\n",
    "        #  x : (B,L, D)\n",
    "        # h : (B,L, ED, N)\n",
    "\n",
    "        #  y : (B, L, D)\n",
    "\n",
    "\n",
    "        xz = self.in_proj(x)  # (B, L,2*ED)\n",
    "        x, z = xz.chunk(2, dim=-1)  #  (B,L, ED), (B,L, ED)\n",
    "        x_cache = x.permute(0,2,1)#(B, ED,L)\n",
    "\n",
    "        #  x branch\n",
    "        x = self.conv1d( x_cache).permute(0,2,1) #  (B,L , ED)\n",
    "\n",
    "        x = F.silu(x)\n",
    "        y, h = self.ssm_step(x, h)\n",
    "        #y->B,L,ED;h->B,L,ED,N\n",
    "\n",
    "        #  z branch\n",
    "        z = F.silu(z)\n",
    "\n",
    "        output = y * z\n",
    "        output = self.out_proj(output)  #  (B, L, D)\n",
    "\n",
    "        return output, h\n",
    "\n",
    "    def ssm_step(self, x, h):\n",
    "        #  x : (B, L, ED)\n",
    "        #  h : (B, L, ED, N)\n",
    "\n",
    "        A = -torch.exp(\n",
    "            self.A_log.float())  # (ED, N) # todo : ne pas le faire tout le temps, puisque c'est indépendant de la timestep\n",
    "        D = self.D.float()\n",
    "        #  TODO remove .float()\n",
    "\n",
    "        deltaBC = self.x_proj(x)  #  (B, L, dt_rank+2*N)\n",
    "\n",
    "        delta, B, C = torch.split(deltaBC, [self.dt_rank, self.d_state, self.d_state],\n",
    "                                  dim=-1)  #  (B, L,dt_rank), (B, L, N), (B, L, N)\n",
    "        delta = F.softplus(self.dt_proj(delta))  #  (B, L, ED)\n",
    "\n",
    "        deltaA = torch.exp(delta.unsqueeze(-1) * A)  #  (B,L, ED, N)\n",
    "        deltaB = delta.unsqueeze(-1) * B.unsqueeze(2)  #  (B,L, ED, N)\n",
    "\n",
    "        BX = deltaB * (x.unsqueeze(-1))  #  (B, L,ED, N)\n",
    "\n",
    "        if h is None:\n",
    "            h = torch.zeros(x.size(0), x.size(1), self.d_inner, self.d_state, device=deltaA.device)  #  (B, L, ED, N)\n",
    "\n",
    "        h = deltaA * h + BX  #  (B, L, ED, N)\n",
    "\n",
    "        y = (h @ C.unsqueeze(-1)).squeeze(3)  #  (B, L, ED, N) @ (B, L, N, 1) -> (B, L, ED, 1)\n",
    "\n",
    "        y = y + D * x#B,L,ED\n",
    "\n",
    "        #  todo : pq h.squeeze(1) ??\n",
    "        return y, h\n",
    "    \n",
    "class DWConv(nn.Module):\n",
    "    def __init__(self, dim=768):\n",
    "        super().__init__()\n",
    "        self.dwconv = nn.Conv2d(dim, dim, 3, 1, 1, bias=True, groups=dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(1,0,2)\n",
    "        B, N, C = x.shape\n",
    "        x = x.transpose(1,2).view(B,C,int(N**0.5),int(N**0.5)).contiguous()\n",
    "        x = self.dwconv(x).flatten(2).transpose(1, 2)#B,N,C\n",
    "        x = x.permute(1,0,2)\n",
    "        return x\n",
    "class ConvFFN(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None,\n",
    "                 act_layer=nn.GELU, drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.dwconv = DWConv(hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.dwconv(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "    \n",
    "class ConvFFN(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None,\n",
    "                 act_layer=nn.GELU, drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.dwconv = DWConv(hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.dwconv(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "def drop_path(x, drop_prob: float = 0., training: bool = False, scale_by_keep: bool = True):\n",
    "    \"\"\"Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).\n",
    "\n",
    "    This is the same as the DropConnect impl I created for EfficientNet, etc networks, however,\n",
    "    the original name is misleading as 'Drop Connect' is a different form of dropout in a separate paper...\n",
    "    See discussion: https://github.com/tensorflow/tpu/issues/494#issuecomment-532968956 ... I've opted for\n",
    "    changing the layer and argument names to 'drop path' rather than mix DropConnect as a layer name and use\n",
    "    'survival rate' as the argument.\n",
    "\n",
    "    \"\"\"\n",
    "    if drop_prob == 0. or not training:\n",
    "        return x\n",
    "    keep_prob = 1 - drop_prob\n",
    "    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n",
    "    random_tensor = x.new_empty(shape).bernoulli_(keep_prob)\n",
    "    if keep_prob > 0.0 and scale_by_keep:\n",
    "        random_tensor.div_(keep_prob)\n",
    "    return x * random_tensor\n",
    "\n",
    "class DropPath(nn.Module):\n",
    "    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\n",
    "    \"\"\"\n",
    "    def __init__(self, drop_prob: float = 0., scale_by_keep: bool = True):\n",
    "        super(DropPath, self).__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.scale_by_keep = scale_by_keep\n",
    "\n",
    "    def forward(self, x):\n",
    "        return drop_path(x, self.drop_prob, self.training, self.scale_by_keep)\n",
    "\n",
    "    def extra_repr(self):\n",
    "        return f'drop_prob={round(self.drop_prob,3):0.3f}'\n",
    "    \n",
    "class partial:\n",
    "    \"\"\"New function with partial application of the given arguments\n",
    "    and keywords.\n",
    "    \"\"\"\n",
    "\n",
    "    __slots__ = \"func\", \"args\", \"keywords\", \"__dict__\", \"__weakref__\"\n",
    "\n",
    "    def __new__(cls, func, /, *args, **keywords):\n",
    "        if not callable(func):\n",
    "            raise TypeError(\"the first argument must be callable\")\n",
    "\n",
    "        if hasattr(func, \"func\"):\n",
    "            args = func.args + args\n",
    "            keywords = {**func.keywords, **keywords}\n",
    "            func = func.func\n",
    "\n",
    "        self = super(partial, cls).__new__(cls)\n",
    "\n",
    "        self.func = func\n",
    "        self.args = args\n",
    "        self.keywords = keywords\n",
    "        return self\n",
    "\n",
    "class Extractor(nn.Module):\n",
    "    def __init__(self, d_model, num_heads=8, dropout=0.1,drop_path=0.1,\n",
    "                 norm_layer=partial(nn.LayerNorm, eps=1e-6)):\n",
    "        super().__init__()\n",
    "        self.query_norm = norm_layer(d_model)\n",
    "        self.feat_norm = norm_layer(d_model)\n",
    "        self.attn = nn.MultiheadAttention(d_model, num_heads, dropout=dropout)\n",
    "        #convffn\n",
    "        self.ffn = ConvFFN(in_features=d_model, hidden_features=int(d_model * 0.25), drop=0.)\n",
    "        self.ffn_norm = norm_layer(d_model)\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "\n",
    "    def forward(self, query, feat):\n",
    "\n",
    "        def _inner_forward(query, feat):\n",
    "            # query:l,b,d;feat:l,b,d\n",
    "            attn = self.attn(self.query_norm(query),\n",
    "                             self.feat_norm(feat), self.feat_norm(feat))[0]\n",
    "            query = query + attn\n",
    "\n",
    "            query = query + self.drop_path(self.ffn(self.ffn_norm(query)))\n",
    "            return query\n",
    "\n",
    "        query = _inner_forward(query, feat)\n",
    "\n",
    "        return query\n",
    "    \n",
    "class Injector(nn.Module):\n",
    "    def __init__(self, d_model, n_heads=8,norm_layer=partial(nn.LayerNorm, eps=1e-6),  dropout=0.1,\n",
    "                 init_values=0.):\n",
    "        super().__init__()\n",
    "        self.query_norm = norm_layer(d_model)\n",
    "        self.feat_norm = norm_layer(d_model)\n",
    "        self.attn = nn.MultiheadAttention(d_model, n_heads,dropout=dropout)\n",
    "        self.gamma = nn.Parameter(init_values * torch.ones((d_model)), requires_grad=True)\n",
    "\n",
    "    def forward(self, query,feat):\n",
    "            #query:l,b,d;feat:l,b,d\n",
    "        def _inner_forward(query, feat):\n",
    "\n",
    "            attn = self.attn(self.query_norm(query),\n",
    "                             self.feat_norm(feat),self.feat_norm(feat))[0]\n",
    "            return query + self.gamma * attn\n",
    "        query = _inner_forward(query, feat)\n",
    "        return query\n",
    "    \n",
    "\n",
    "class InteractionBlock(nn.Module):\n",
    "    def __init__(self, d_model, extra_extractor, grad_ckpt):\n",
    "        super().__init__()\n",
    "        self.grad_ckpt = grad_ckpt\n",
    "        self.injector = Injector(d_model=d_model)\n",
    "        self.extractor = Extractor(d_model=d_model)\n",
    "        if extra_extractor:\n",
    "            self.extra_extractors = nn.Sequential(*[\n",
    "                Extractor(d_model=d_model)\n",
    "                for _ in range(2)])\n",
    "        else:\n",
    "            self.extra_extractors = None\n",
    "\n",
    "    def forward(self,x,xs,blocks):\n",
    "        x = self.injector(x.permute(1,0,2),xs.permute(1,0,2)).permute(1,0,2)\n",
    "        for idx,blk in enumerate(blocks):\n",
    "            x = checkpoint.checkpoint(blk, x, None,use_reentrant=False) if self.grad_ckpt else blk(x,None)\n",
    "        xs = checkpoint.checkpoint(self.extractor, xs.permute(1,0,2),x.permute(1,0,2),use_reentrant=False).permute(1,0,2) \\\n",
    "            if self.grad_ckpt else self.extractor(xs.permute(1, 0, 2), x.permute(1, 0, 2)).permute(1, 0, 2)  # b,n,c\n",
    "        # xs = self.extractor(xs.permute(1,0,2),x.permute(1,0,2)).permute(1,0,2)#b,n,c\n",
    "        if self.extra_extractors is not None:\n",
    "            for extractor in self.extra_extractors:\n",
    "                xs = checkpoint.checkpoint(extractor, xs.permute(1, 0, 2), x.permute(1, 0, 2), use_reentrant=False).permute(1, 0, 2) \\\n",
    "                    if self.grad_ckpt else extractor(xs.permute(1, 0, 2), x.permute(1, 0, 2)).permute(1, 0,2)  # b,n,c\n",
    "                # xs = extractor(xs.permute(1,0,2),x.permute(1,0,2)).permute(1,0,2)\n",
    "        return x,xs\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, d_model: int, eps: float = 1e-5):\n",
    "        super().__init__()\n",
    "\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(d_model))\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps) * self.weight\n",
    "\n",
    "        return output\n",
    "    \n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self,dt_scale, d_model,d_inner,dt_rank,d_state,bias,d_conv,conv_bias,dt_init,dt_max,dt_min,dt_init_floor,grad_ckpt):\n",
    "        super().__init__()\n",
    "\n",
    "        self.grad_ckpt = grad_ckpt\n",
    "        self.mixer = MambaBlock(dt_scale,d_model,d_inner,dt_rank,d_state,bias,d_conv,conv_bias,dt_init,dt_max,dt_min,dt_init_floor)\n",
    "        self.norm = RMSNorm(d_model)\n",
    "\n",
    "    def forward(self, x, h):\n",
    "        #  x : (B, L, D)\n",
    "        # h : (B, L, ED, N)\n",
    "        #  output : (B,L, D)\n",
    "\n",
    "        x = self.norm(x)\n",
    "        output, h = checkpoint.checkpoint(self.mixer,x,h,use_reentrant=False) if self.grad_ckpt else self.mixer(x, h)\n",
    "        output = output + x\n",
    "        return output, h\n",
    "    \n",
    "class Mamba_Neck(nn.Module):\n",
    "    def __init__(self, in_channel=512,d_model=512,d_inner=1024,bias=False,n_layers=4,dt_rank=32,d_state=16,d_conv=3,dt_min=0.001,\n",
    "                 dt_max=0.1,dt_init='random',dt_scale=1.0,conv_bias=True,dt_init_floor=0.0001,grad_ckpt=False):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_inner = d_inner\n",
    "        self.bias = bias\n",
    "        self.dt_rank = dt_rank\n",
    "        self.d_state = d_state\n",
    "        self.dt_scale = dt_scale\n",
    "        self.num_channels = self.d_model\n",
    "        self.layers = nn.ModuleList(\n",
    "            [ResidualBlock(dt_scale,d_model,d_inner,dt_rank,d_state,bias,d_conv,conv_bias,dt_init,dt_max,dt_min,dt_init_floor,grad_ckpt)\n",
    "             for _ in range(n_layers)])\n",
    "        self.interactions = nn.ModuleList([\n",
    "            InteractionBlock(d_model=d_model,extra_extractor=(True if i == n_layers - 1 else False),grad_ckpt=grad_ckpt)\n",
    "            for i in range(n_layers)\n",
    "        ])\n",
    "        # self.norm_f = RMSNorm(config.d_model)\n",
    "\n",
    "    def forward(self, x,xs,h,blocks,interaction_indexes):\n",
    "        #  x : (B, L, D)\n",
    "        #  caches : [cache(layer) for all layers], cache : (h, inputs)\n",
    "\n",
    "        #  y : (B, L, D)\n",
    "        #  caches : [cache(layer) for all layers], cache : (h, inputs)\n",
    "        for i,index in enumerate(interaction_indexes):\n",
    "            xs, h[i] = self.layers[i](xs, h[i])\n",
    "            x,xs = self.interactions[i](x,xs,blocks[index[0]:index[1]])\n",
    "\n",
    "        return x, xs, h\n",
    "def build_neck(cfg,encoder):\n",
    "    in_channel = encoder.num_channels\n",
    "    d_model = cfg.MODEL.NECK.D_MODEL\n",
    "    n_layers = cfg.MODEL.NECK.N_LAYERS\n",
    "    d_state = cfg.MODEL.NECK.D_STATE\n",
    "    grad_ckpt = cfg.MODEL.ENCODER.GRAD_CKPT\n",
    "    neck = Mamba_Neck(in_channel=in_channel,d_model=d_model,d_inner=2*d_model,n_layers=n_layers,dt_rank=d_model//16,d_state=d_state,grad_ckpt=grad_ckpt)\n",
    "    return neck\n",
    "\n",
    "def box_xyxy_to_cxcywh(x):\n",
    "    x0, y0, x1, y1 = x.unbind(-1)\n",
    "    b = [(x0 + x1) / 2, (y0 + y1) / 2,\n",
    "         (x1 - x0), (y1 - y0)]\n",
    "    return torch.stack(b, dim=-1)\n",
    "\n",
    "class MLPPredictor(nn.Module):\n",
    "    def __init__(self, inplanes=64, channel=256, feat_sz=20, stride=16):\n",
    "        super(MLPPredictor, self).__init__()\n",
    "        self.feat_sz = feat_sz\n",
    "        self.stride = stride\n",
    "        self.img_sz = self.feat_sz * self.stride\n",
    "\n",
    "        self.num_layers = 3\n",
    "        h = [channel] * (self.num_layers - 1)\n",
    "        self.layers_cls = nn.ModuleList(nn.Linear(n, k)\n",
    "                                        for n, k in zip([inplanes] + h, h + [1]))\n",
    "        self.layers_reg = nn.ModuleList(nn.Linear(n, k)\n",
    "                                        for n, k in zip([inplanes] + h, h + [4]))\n",
    "\n",
    "        # for p in self.parameters():\n",
    "        #     if p.dim() > 1:\n",
    "        #         nn.init.xavier_uniform_(p)\n",
    "\n",
    "    def forward(self, x, gt_score_map=None):\n",
    "        \"\"\" Forward pass with input x. \"\"\"\n",
    "        score_map, offset_map = self.get_score_map(x)\n",
    "\n",
    "        # assert gt_score_map is None\n",
    "        if gt_score_map is None:\n",
    "            bbox = self.cal_bbox(score_map, offset_map)\n",
    "        else:\n",
    "            bbox = self.cal_bbox(gt_score_map.unsqueeze(1), offset_map)\n",
    "\n",
    "        return score_map, bbox, offset_map\n",
    "\n",
    "    def cal_bbox(self, score_map, offset_map, return_score=False):\n",
    "        max_score, idx = torch.max(score_map.flatten(1), dim=1, keepdim=True)\n",
    "        idx_y = torch.div(idx, self.feat_sz, rounding_mode='floor')\n",
    "        idx_x = idx % self.feat_sz\n",
    "\n",
    "        idx = idx.unsqueeze(1).expand(idx.shape[0], 4, 1) # torch.Size([32, 4, 1])\n",
    "        offset = offset_map.flatten(2).gather(dim=2, index=idx).squeeze(-1)\n",
    "        # offset: (l,t,r,b)\n",
    "\n",
    "        # x1, y1, x2, y2\n",
    "        bbox = torch.cat([idx_x.to(torch.float) / self.feat_sz - offset[:, :1], # the offset should not divide the self.feat_sz, since I use the sigmoid to limit it in (0,1)\n",
    "                          idx_y.to(torch.float) / self.feat_sz - offset[:, 1:2],\n",
    "                          idx_x.to(torch.float) / self.feat_sz + offset[:, 2:3],\n",
    "                          idx_y.to(torch.float) / self.feat_sz + offset[:, 3:4],\n",
    "                          ], dim=1)\n",
    "        bbox = box_xyxy_to_cxcywh(bbox)\n",
    "        if return_score:\n",
    "            return bbox, max_score\n",
    "        return bbox\n",
    "\n",
    "    def get_score_map(self, x):\n",
    "\n",
    "        def _sigmoid(x):\n",
    "            y = torch.clamp(x.sigmoid_(), min=1e-4, max=1 - 1e-4)\n",
    "            return y\n",
    "\n",
    "        x_cls = x\n",
    "        for i, layer in enumerate(self.layers_cls):\n",
    "            x_cls = F.relu(layer(x_cls)) if i < self.num_layers - 1 else layer(x_cls)\n",
    "        x_cls = x_cls.permute(0,2,1).reshape(-1,1,self.feat_sz,self.feat_sz)\n",
    "\n",
    "        x_reg = x\n",
    "        for i, layer in enumerate(self.layers_reg):\n",
    "            x_reg = F.relu(layer(x_reg)) if i < self.num_layers - 1 else layer(x_reg)\n",
    "        x_reg = x_reg.permute(0, 2, 1).reshape(-1, 4, self.feat_sz, self.feat_sz)\n",
    "\n",
    "        return _sigmoid(x_cls), _sigmoid(x_reg)\n",
    "    \n",
    "class FrozenBatchNorm2d(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    BatchNorm2d where the batch statistics and the affine parameters are fixed.\n",
    "\n",
    "    Copy-paste from torchvision.misc.ops with added eps before rqsrt,\n",
    "    without which any other models than torchvision.models.resnet[18,34,50,101]\n",
    "    produce nans.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n):\n",
    "        super(FrozenBatchNorm2d, self).__init__()\n",
    "        self.register_buffer(\"weight\", torch.ones(n))\n",
    "        self.register_buffer(\"bias\", torch.zeros(n))\n",
    "        self.register_buffer(\"running_mean\", torch.zeros(n))\n",
    "        self.register_buffer(\"running_var\", torch.ones(n))\n",
    "\n",
    "    def _load_from_state_dict(self, state_dict, prefix, local_metadata, strict,\n",
    "                              missing_keys, unexpected_keys, error_msgs):\n",
    "        num_batches_tracked_key = prefix + 'num_batches_tracked'\n",
    "        if num_batches_tracked_key in state_dict:\n",
    "            del state_dict[num_batches_tracked_key]\n",
    "\n",
    "        super(FrozenBatchNorm2d, self)._load_from_state_dict(\n",
    "            state_dict, prefix, local_metadata, strict,\n",
    "            missing_keys, unexpected_keys, error_msgs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # move reshapes to the beginning\n",
    "        # to make it fuser-friendly\n",
    "        w = self.weight.reshape(1, -1, 1, 1)\n",
    "        b = self.bias.reshape(1, -1, 1, 1)\n",
    "        rv = self.running_var.reshape(1, -1, 1, 1)\n",
    "        rm = self.running_mean.reshape(1, -1, 1, 1)\n",
    "        eps = 1e-5\n",
    "        scale = w * (rv + eps).rsqrt()  # rsqrt(x): 1/sqrt(x), r: reciprocal\n",
    "        bias = b - rm * scale\n",
    "        return x * scale + bias\n",
    "\n",
    "def conv(in_planes, out_planes, kernel_size=3, stride=1, padding=1, dilation=1,\n",
    "         freeze_bn=False):\n",
    "    if freeze_bn:\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride,\n",
    "                      padding=padding, dilation=dilation, bias=True),\n",
    "            FrozenBatchNorm2d(out_planes),\n",
    "            nn.ReLU(inplace=True))\n",
    "    else:\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride,\n",
    "                      padding=padding, dilation=dilation, bias=True),\n",
    "            nn.BatchNorm2d(out_planes),\n",
    "            nn.ReLU(inplace=True))\n",
    "    \n",
    "def conv(in_planes, out_planes, kernel_size=3, stride=1, padding=1, dilation=1,\n",
    "         freeze_bn=False):\n",
    "    if freeze_bn:\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride,\n",
    "                      padding=padding, dilation=dilation, bias=True),\n",
    "            FrozenBatchNorm2d(out_planes),\n",
    "            nn.ReLU(inplace=True))\n",
    "    else:\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride,\n",
    "                      padding=padding, dilation=dilation, bias=True),\n",
    "            nn.BatchNorm2d(out_planes),\n",
    "            nn.ReLU(inplace=True))\n",
    "    \n",
    "class CenterPredictor(nn.Module, ):\n",
    "    def __init__(self, inplanes=64, channel=256, feat_sz=20, stride=16, freeze_bn=False):\n",
    "        super(CenterPredictor, self).__init__()\n",
    "        self.feat_sz = feat_sz\n",
    "        self.stride = stride\n",
    "        self.img_sz = self.feat_sz * self.stride\n",
    "\n",
    "        # corner predict\n",
    "        self.conv1_ctr = conv(inplanes, channel, freeze_bn=freeze_bn)\n",
    "        self.conv2_ctr = conv(channel, channel // 2, freeze_bn=freeze_bn)\n",
    "        self.conv3_ctr = conv(channel // 2, channel // 4, freeze_bn=freeze_bn)\n",
    "        self.conv4_ctr = conv(channel // 4, channel // 8, freeze_bn=freeze_bn)\n",
    "        self.conv5_ctr = nn.Conv2d(channel // 8, 1, kernel_size=1)\n",
    "\n",
    "        # size regress\n",
    "        self.conv1_offset = conv(inplanes, channel, freeze_bn=freeze_bn)\n",
    "        self.conv2_offset = conv(channel, channel // 2, freeze_bn=freeze_bn)\n",
    "        self.conv3_offset = conv(channel // 2, channel // 4, freeze_bn=freeze_bn)\n",
    "        self.conv4_offset = conv(channel // 4, channel // 8, freeze_bn=freeze_bn)\n",
    "        self.conv5_offset = nn.Conv2d(channel // 8, 2, kernel_size=1)\n",
    "\n",
    "        # size regress\n",
    "        self.conv1_size = conv(inplanes, channel, freeze_bn=freeze_bn)\n",
    "        self.conv2_size = conv(channel, channel // 2, freeze_bn=freeze_bn)\n",
    "        self.conv3_size = conv(channel // 2, channel // 4, freeze_bn=freeze_bn)\n",
    "        self.conv4_size = conv(channel // 4, channel // 8, freeze_bn=freeze_bn)\n",
    "        self.conv5_size = nn.Conv2d(channel // 8, 2, kernel_size=1)\n",
    "\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "\n",
    "    def forward(self, x, gt_score_map=None):\n",
    "        \"\"\" Forward pass with input x. \"\"\"\n",
    "        score_map_ctr, size_map, offset_map = self.get_score_map(x) # x: torch.Size([b, c, h, w])\n",
    "        # score_map_ctr: torch.Size([32, 1, 16, 16]) size_map: torch.Size([32, 2, 16, 16]) offset_map: torch.Size([32, 2, 16, 16])\n",
    "\n",
    "        # assert gt_score_map is None\n",
    "        if gt_score_map is None:\n",
    "            bbox = self.cal_bbox(score_map_ctr, size_map, offset_map)\n",
    "        else:\n",
    "            bbox = self.cal_bbox(gt_score_map.unsqueeze(1), size_map, offset_map)\n",
    "\n",
    "        return score_map_ctr, bbox, size_map, offset_map\n",
    "\n",
    "    def cal_bbox(self, score_map_ctr, size_map, offset_map, return_score=False):\n",
    "        max_score, idx = torch.max(score_map_ctr.flatten(1), dim=1, keepdim=True) # score_map_ctr.flatten(1): torch.Size([32, 256]) idx: torch.Size([32, 1]) max_score: torch.Size([32, 1])\n",
    "        idx_y = torch.div(idx, self.feat_sz, rounding_mode='floor')\n",
    "        idx_x = idx % self.feat_sz\n",
    "\n",
    "        idx = idx.unsqueeze(1).expand(idx.shape[0], 2, 1)\n",
    "        size = size_map.flatten(2).gather(dim=2, index=idx) # size_map: torch.Size([32, 2, 16, 16])  size_map.flatten(2): torch.Size([32, 2, 256])\n",
    "        offset = offset_map.flatten(2).gather(dim=2, index=idx).squeeze(-1)\n",
    "\n",
    "        # bbox = torch.cat([idx_x - size[:, 0] / 2, idx_y - size[:, 1] / 2,\n",
    "        #                   idx_x + size[:, 0] / 2, idx_y + size[:, 1] / 2], dim=1) / self.feat_sz\n",
    "        # cx, cy, w, h\n",
    "        bbox = torch.cat([(idx_x.to(torch.float) + offset[:, :1]) / self.feat_sz,\n",
    "                          (idx_y.to(torch.float) + offset[:, 1:]) / self.feat_sz,\n",
    "                          size.squeeze(-1)], dim=1)\n",
    "\n",
    "        if return_score:\n",
    "            return bbox, max_score\n",
    "        return bbox\n",
    "\n",
    "    def get_pred(self, score_map_ctr, size_map, offset_map):\n",
    "        max_score, idx = torch.max(score_map_ctr.flatten(1), dim=1, keepdim=True)\n",
    "        idx_y = idx // self.feat_sz\n",
    "        idx_x = idx % self.feat_sz\n",
    "\n",
    "        idx = idx.unsqueeze(1).expand(idx.shape[0], 2, 1)\n",
    "        size = size_map.flatten(2).gather(dim=2, index=idx)\n",
    "        offset = offset_map.flatten(2).gather(dim=2, index=idx).squeeze(-1)\n",
    "\n",
    "        # bbox = torch.cat([idx_x - size[:, 0] / 2, idx_y - size[:, 1] / 2,\n",
    "        #                   idx_x + size[:, 0] / 2, idx_y + size[:, 1] / 2], dim=1) / self.feat_sz\n",
    "        return size * self.feat_sz, offset\n",
    "\n",
    "    def get_score_map(self, x):\n",
    "\n",
    "        def _sigmoid(x):\n",
    "            y = torch.clamp(x.sigmoid_(), min=1e-4, max=1 - 1e-4)\n",
    "            return y\n",
    "\n",
    "        # ctr branch\n",
    "        x_ctr1 = self.conv1_ctr(x)\n",
    "        x_ctr2 = self.conv2_ctr(x_ctr1)\n",
    "        x_ctr3 = self.conv3_ctr(x_ctr2)\n",
    "        x_ctr4 = self.conv4_ctr(x_ctr3)\n",
    "        score_map_ctr = self.conv5_ctr(x_ctr4)\n",
    "\n",
    "        # offset branch\n",
    "        x_offset1 = self.conv1_offset(x)\n",
    "        x_offset2 = self.conv2_offset(x_offset1)\n",
    "        x_offset3 = self.conv3_offset(x_offset2)\n",
    "        x_offset4 = self.conv4_offset(x_offset3)\n",
    "        score_map_offset = self.conv5_offset(x_offset4)\n",
    "\n",
    "        # size branch\n",
    "        x_size1 = self.conv1_size(x)\n",
    "        x_size2 = self.conv2_size(x_size1)\n",
    "        x_size3 = self.conv3_size(x_size2)\n",
    "        x_size4 = self.conv4_size(x_size3)\n",
    "        score_map_size = self.conv5_size(x_size4)\n",
    "        return _sigmoid(score_map_ctr), _sigmoid(score_map_size), score_map_offset\n",
    "    \n",
    "def build_decoder(cfg, encoder):\n",
    "    num_channels_enc = encoder.num_channels\n",
    "    stride = cfg.MODEL.ENCODER.STRIDE\n",
    "    if cfg.MODEL.DECODER.TYPE == \"MLP\":\n",
    "        in_channel = num_channels_enc\n",
    "        hidden_dim = cfg.MODEL.DECODER.NUM_CHANNELS\n",
    "        feat_sz = int(cfg.DATA.SEARCH.SIZE / stride)\n",
    "        mlp_head = MLPPredictor(inplanes=in_channel, channel=hidden_dim,\n",
    "                                feat_sz=feat_sz, stride=stride)\n",
    "        return mlp_head\n",
    "    elif \"CORNER\" in cfg.MODEL.DECODER.TYPE:\n",
    "        feat_sz = int(cfg.DATA.SEARCH.SIZE / stride)\n",
    "        channel = getattr(cfg.MODEL, \"NUM_CHANNELS\", 256)\n",
    "        print(\"head channel: %d\" % channel)\n",
    "        if cfg.MODEL.HEAD.TYPE == \"CORNER\":\n",
    "            corner_head = Corner_Predictor(inplanes=cfg.MODEL.HIDDEN_DIM, channel=channel,\n",
    "                                           feat_sz=feat_sz, stride=stride)\n",
    "        else:\n",
    "            raise ValueError()\n",
    "        return corner_head\n",
    "    elif cfg.MODEL.DECODER.TYPE == \"CENTER\":\n",
    "        in_channel = num_channels_enc\n",
    "        out_channel = cfg.MODEL.DECODER.NUM_CHANNELS\n",
    "        feat_sz = int(cfg.DATA.SEARCH.SIZE / stride)\n",
    "        center_head = CenterPredictor(inplanes=in_channel, channel=out_channel,\n",
    "                                      feat_sz=feat_sz, stride=stride)\n",
    "        return center_head\n",
    "    else:\n",
    "        raise ValueError(\"HEAD TYPE %s is not supported.\" % cfg.MODEL.HEAD_TYPE)\n",
    "    \n",
    "class Corner_Predictor(nn.Module):\n",
    "    \"\"\" Corner Predictor module\"\"\"\n",
    "\n",
    "    def __init__(self, inplanes=64, channel=256, feat_sz=20, stride=16, freeze_bn=False):\n",
    "        super(Corner_Predictor, self).__init__()\n",
    "        self.feat_sz = feat_sz\n",
    "        self.stride = stride\n",
    "        self.img_sz = self.feat_sz * self.stride\n",
    "        '''top-left corner'''\n",
    "        self.conv1_tl = conv(inplanes, channel, freeze_bn=freeze_bn)\n",
    "        self.conv2_tl = conv(channel, channel // 2, freeze_bn=freeze_bn)\n",
    "        self.conv3_tl = conv(channel // 2, channel // 4, freeze_bn=freeze_bn)\n",
    "        self.conv4_tl = conv(channel // 4, channel // 8, freeze_bn=freeze_bn)\n",
    "        self.conv5_tl = nn.Conv2d(channel // 8, 1, kernel_size=1)\n",
    "\n",
    "        '''bottom-right corner'''\n",
    "        self.conv1_br = conv(inplanes, channel, freeze_bn=freeze_bn)\n",
    "        self.conv2_br = conv(channel, channel // 2, freeze_bn=freeze_bn)\n",
    "        self.conv3_br = conv(channel // 2, channel // 4, freeze_bn=freeze_bn)\n",
    "        self.conv4_br = conv(channel // 4, channel // 8, freeze_bn=freeze_bn)\n",
    "        self.conv5_br = nn.Conv2d(channel // 8, 1, kernel_size=1)\n",
    "\n",
    "        '''about coordinates and indexs'''\n",
    "        with torch.no_grad():\n",
    "            self.indice = torch.arange(0, self.feat_sz).view(-1, 1) * self.stride\n",
    "            # generate mesh-grid\n",
    "            self.coord_x = self.indice.repeat((self.feat_sz, 1)) \\\n",
    "                .view((self.feat_sz * self.feat_sz,)).float().cuda()\n",
    "            self.coord_y = self.indice.repeat((1, self.feat_sz)) \\\n",
    "                .view((self.feat_sz * self.feat_sz,)).float().cuda()\n",
    "\n",
    "    def forward(self, x, return_dist=False, softmax=True):\n",
    "        \"\"\" Forward pass with input x. \"\"\"\n",
    "        score_map_tl, score_map_br = self.get_score_map(x)\n",
    "        if return_dist:\n",
    "            coorx_tl, coory_tl, prob_vec_tl = self.soft_argmax(score_map_tl, return_dist=True, softmax=softmax)\n",
    "            coorx_br, coory_br, prob_vec_br = self.soft_argmax(score_map_br, return_dist=True, softmax=softmax)\n",
    "            return torch.stack((coorx_tl, coory_tl, coorx_br, coory_br), dim=1) / self.img_sz, prob_vec_tl, prob_vec_br\n",
    "        else:\n",
    "            coorx_tl, coory_tl = self.soft_argmax(score_map_tl)\n",
    "            coorx_br, coory_br = self.soft_argmax(score_map_br)\n",
    "            return torch.stack((coorx_tl, coory_tl, coorx_br, coory_br), dim=1) / self.img_sz\n",
    "\n",
    "    def get_score_map(self, x):\n",
    "        # top-left branch\n",
    "        x_tl1 = self.conv1_tl(x)\n",
    "        x_tl2 = self.conv2_tl(x_tl1)\n",
    "        x_tl3 = self.conv3_tl(x_tl2)\n",
    "        x_tl4 = self.conv4_tl(x_tl3)\n",
    "        score_map_tl = self.conv5_tl(x_tl4)\n",
    "\n",
    "        # bottom-right branch\n",
    "        x_br1 = self.conv1_br(x)\n",
    "        x_br2 = self.conv2_br(x_br1)\n",
    "        x_br3 = self.conv3_br(x_br2)\n",
    "        x_br4 = self.conv4_br(x_br3)\n",
    "        score_map_br = self.conv5_br(x_br4)\n",
    "        return score_map_tl, score_map_br\n",
    "\n",
    "    def soft_argmax(self, score_map, return_dist=False, softmax=True):\n",
    "        \"\"\" get soft-argmax coordinate for a given heatmap \"\"\"\n",
    "        score_vec = score_map.view((-1, self.feat_sz * self.feat_sz))  # (batch, feat_sz * feat_sz)\n",
    "        prob_vec = nn.functional.softmax(score_vec, dim=1)\n",
    "        exp_x = torch.sum((self.coord_x * prob_vec), dim=1)\n",
    "        exp_y = torch.sum((self.coord_y * prob_vec), dim=1)\n",
    "        if return_dist:\n",
    "            if softmax:\n",
    "                return exp_x, exp_y, prob_vec\n",
    "            else:\n",
    "                return exp_x, exp_y, score_vec\n",
    "        else:\n",
    "            return exp_x, exp_y\n",
    "        \n",
    "class Preprocessor(object):\n",
    "    def __init__(self):\n",
    "        self.mean = torch.tensor([0.485, 0.456, 0.406]).view((1, 3, 1, 1)).cuda()\n",
    "        self.std = torch.tensor([0.229, 0.224, 0.225]).view((1, 3, 1, 1)).cuda()\n",
    "        self.mm_mean = torch.tensor([0.485, 0.456, 0.406, 0.485, 0.456, 0.406]).view((1, 6, 1, 1)).cuda()\n",
    "        self.mm_std = torch.tensor([0.229, 0.224, 0.225, 0.229, 0.224, 0.225]).view((1, 6, 1, 1)).cuda()\n",
    "\n",
    "    def process(self, img_arr: np.ndarray):\n",
    "        if img_arr.shape[-1] == 6:\n",
    "            mean = self.mm_mean\n",
    "            std = self.mm_std\n",
    "        else:\n",
    "            mean = self.mean\n",
    "            std = self.std\n",
    "        # Deal with the image patch\n",
    "        img_tensor = torch.tensor(img_arr).cuda().float().permute((2,0,1)).unsqueeze(dim=0)\n",
    "        # img_tensor = torch.tensor(img_arr).float().permute((2,0,1)).unsqueeze(dim=0)\n",
    "        img_tensor_norm = ((img_tensor / 255.0) - mean) / std  # (1,3,H,W)\n",
    "        return img_tensor_norm\n",
    "    \n",
    "def hann1d(sz: int, centered = True) -> torch.Tensor:\n",
    "    \"\"\"1D cosine window.\"\"\"\n",
    "    if centered:\n",
    "        return 0.5 * (1 - torch.cos((2 * math.pi / (sz + 1)) * torch.arange(1, sz + 1).float()))\n",
    "    w = 0.5 * (1 + torch.cos((2 * math.pi / (sz + 2)) * torch.arange(0, sz//2 + 1).float()))\n",
    "    return torch.cat([w, w[1:sz-sz//2].flip((0,))])\n",
    "    \n",
    "def hann2d(sz: torch.Tensor, centered = True) -> torch.Tensor:\n",
    "    \"\"\"2D cosine window.\"\"\"\n",
    "    return hann1d(sz[0].item(), centered).reshape(1, 1, -1, 1) * hann1d(sz[1].item(), centered).reshape(1, 1, 1, -1)    \n",
    "\n",
    "def sample_target(im, target_bb, search_area_factor, output_sz=None):\n",
    "    \"\"\" Extracts a square crop centered at target_bb box, of area search_area_factor^2 times target_bb area\n",
    "\n",
    "    args:\n",
    "        im - cv image\n",
    "        target_bb - target box [x, y, w, h]\n",
    "        search_area_factor - Ratio of crop size to target size\n",
    "        output_sz - (float) Size to which the extracted crop is resized (always square). If None, no resizing is done.\n",
    "\n",
    "    returns:\n",
    "        cv image - extracted crop\n",
    "        float - the factor by which the crop has been resized to make the crop size equal output_size\n",
    "    \"\"\"\n",
    "    if not isinstance(target_bb, list):\n",
    "        x, y, w, h = target_bb.tolist()\n",
    "    else:\n",
    "        x, y, w, h = target_bb\n",
    "    # Crop image\n",
    "    crop_sz = math.ceil(math.sqrt(w * h) * search_area_factor)\n",
    "\n",
    "    if crop_sz < 1:\n",
    "        raise Exception('Too small bounding box.')\n",
    "\n",
    "    x1 = round(x + 0.5 * w - crop_sz * 0.5)\n",
    "    x2 = x1 + crop_sz\n",
    "\n",
    "    y1 = round(y + 0.5 * h - crop_sz * 0.5)\n",
    "    y2 = y1 + crop_sz\n",
    "\n",
    "    x1_pad = max(0, -x1)\n",
    "    x2_pad = max(x2 - im.shape[1] + 1, 0)\n",
    "\n",
    "    y1_pad = max(0, -y1)\n",
    "    y2_pad = max(y2 - im.shape[0] + 1, 0)\n",
    "\n",
    "    # Crop target\n",
    "    im_crop = im[y1 + y1_pad:y2 - y2_pad, x1 + x1_pad:x2 - x2_pad, :]\n",
    "\n",
    "    # Pad\n",
    "    im_crop_padded = cv.copyMakeBorder(im_crop, y1_pad, y2_pad, x1_pad, x2_pad, cv.BORDER_CONSTANT)\n",
    "    # deal with attention mask\n",
    "    H, W, _ = im_crop_padded.shape\n",
    "\n",
    "    if output_sz is not None:\n",
    "        resize_factor = output_sz / crop_sz\n",
    "        im_crop_padded = cv.resize(im_crop_padded, (output_sz, output_sz))\n",
    "\n",
    "        return im_crop_padded, resize_factor\n",
    "\n",
    "    else:\n",
    "        return im_crop_padded, 1.0\n",
    "def transform_image_to_crop(box_in: torch.Tensor, box_extract: torch.Tensor, resize_factor: float,\n",
    "                            crop_sz: torch.Tensor, normalize=False) -> torch.Tensor:\n",
    "    \"\"\" Transform the box co-ordinates from the original image co-ordinates to the co-ordinates of the cropped image\n",
    "    args:\n",
    "        box_in - the box for which the co-ordinates are to be transformed\n",
    "        box_extract - the box about which the image crop has been extracted.\n",
    "        resize_factor - the ratio between the original image scale and the scale of the image crop\n",
    "        crop_sz - size of the cropped image\n",
    "\n",
    "    returns:\n",
    "        torch.Tensor - transformed co-ordinates of box_in\n",
    "    \"\"\"\n",
    "    box_extract_center = box_extract[0:2] + 0.5 * box_extract[2:4]\n",
    "\n",
    "    box_in_center = box_in[0:2] + 0.5 * box_in[2:4]\n",
    "\n",
    "    box_out_center = (crop_sz - 1) / 2 + (box_in_center - box_extract_center) * resize_factor\n",
    "    box_out_wh = box_in[2:4] * resize_factor\n",
    "\n",
    "    box_out = torch.cat((box_out_center - 0.5 * box_out_wh, box_out_wh))\n",
    "    if normalize:\n",
    "        return box_out / (crop_sz[0]-1)\n",
    "    else:\n",
    "        return box_out\n",
    "def clip_box(box: list, H, W, margin=0):\n",
    "    x1, y1, w, h = box\n",
    "    x2, y2 = x1 + w, y1 + h\n",
    "    x1 = min(max(0, x1), W-margin)\n",
    "    x2 = min(max(margin, x2), W)\n",
    "    y1 = min(max(0, y1), H-margin)\n",
    "    y2 = min(max(margin, y2), H)\n",
    "    w = max(margin, x2-x1)\n",
    "    h = max(margin, y2-y1)\n",
    "    return [x1, y1, w, h]\n",
    "\n",
    "class MCITrack():\n",
    "    def __init__(self, params):\n",
    "        \n",
    "        self.params = params\n",
    "        self.visdom = None\n",
    "        network = build_mcitrack(params.cfg)\n",
    "        network.load_state_dict(torch.load(\"MCITRACK_ep0300.pth.tar\", map_location='cpu')['net'], strict=True)\n",
    "        \n",
    "        self.cfg = params.cfg\n",
    "        self.network = network.cuda()\n",
    "        self.network.eval()\n",
    "        self.preprocessor = Preprocessor()\n",
    "        self.state = None\n",
    "\n",
    "        self.fx_sz = self.cfg.TEST.SEARCH_SIZE // self.cfg.MODEL.ENCODER.STRIDE\n",
    "        if self.cfg.TEST.WINDOW == True:  # for window penalty\n",
    "            self.output_window = hann2d(torch.tensor([self.fx_sz, self.fx_sz]).long(), centered=True).cuda()\n",
    "\n",
    "        self.num_template = self.cfg.TEST.NUM_TEMPLATES\n",
    "\n",
    "   \n",
    "        self.frame_id = 0\n",
    "        # for update\n",
    "        self.h_state = [None] * self.cfg.MODEL.NECK.N_LAYERS\n",
    "\n",
    "\n",
    "\n",
    "        self.memory_bank = self.cfg.TEST.MB.DEFAULT\n",
    "        self.update_h_t = self.cfg.TEST.UPH.DEFAULT\n",
    "        self.update_threshold = self.cfg.TEST.UPT.DEFAULT\n",
    "        self.update_intervals = self.cfg.TEST.INTER.DEFAULT\n",
    "        print(\"Update threshold is: \", self.memory_bank)\n",
    "\n",
    "    def initialize(self, image, info: dict):\n",
    "\n",
    "\n",
    "        # get the initial templates\n",
    "        z_patch_arr, resize_factor = sample_target(image, info['init_bbox'], self.params.template_factor,\n",
    "                                                   output_sz=self.params.template_size)\n",
    "        z_patch_arr = z_patch_arr\n",
    "        template = self.preprocessor.process(z_patch_arr)\n",
    "        self.template_list = [template] * self.num_template\n",
    "\n",
    "        self.state = info['init_bbox']\n",
    "        prev_box_crop = transform_image_to_crop(torch.tensor(info['init_bbox']),\n",
    "                                                torch.tensor(info['init_bbox']),\n",
    "                                                resize_factor,\n",
    "                                                torch.Tensor([self.params.template_size, self.params.template_size]),\n",
    "                                                normalize=True)\n",
    "        self.template_anno_list = [prev_box_crop.to(template.device).unsqueeze(0)] * self.num_template\n",
    "        self.frame_id = 0\n",
    "        self.memory_template_list = self.template_list.copy()\n",
    "        self.memory_template_anno_list = self.template_anno_list.copy()\n",
    "\n",
    "\n",
    "    def track(self, image, info: dict = None):\n",
    "        H, W, _ = image.shape\n",
    "        self.frame_id += 1\n",
    "        x_patch_arr, resize_factor = sample_target(image, self.state, self.params.search_factor,\n",
    "                                                   output_sz=self.params.search_size)  # (x1, y1, w, h)\n",
    "        search = self.preprocessor.process(x_patch_arr)\n",
    "        search_list = [search]\n",
    "\n",
    "        # run the encoder\n",
    "        with torch.no_grad():\n",
    "            enc_opt = self.network.forward_encoder(self.template_list, search_list, self.template_anno_list)\n",
    "\n",
    "        # run the time neck\n",
    "        with torch.no_grad():\n",
    "            hidden_state = self.h_state.copy()\n",
    "            encoder_out,out_neck, h = self.network.forward_neck(enc_opt, hidden_state)\n",
    "        # run the decoder\n",
    "        with torch.no_grad():\n",
    "            out_dict = self.network.forward_decoder(feature=out_neck)\n",
    "\n",
    "        # add hann windows\n",
    "        pred_score_map = out_dict['score_map']\n",
    "        if self.cfg.TEST.WINDOW == True:  # for window penalty\n",
    "            response = self.output_window * pred_score_map\n",
    "        else:\n",
    "            response = pred_score_map\n",
    "        if 'size_map' in out_dict.keys():\n",
    "            pred_boxes, conf_score = self.network.decoder.cal_bbox(response, out_dict['size_map'],\n",
    "                                                                   out_dict['offset_map'], return_score=True)\n",
    "        else:\n",
    "            pred_boxes, conf_score = self.network.decoder.cal_bbox(response,\n",
    "                                                                   out_dict['offset_map'],\n",
    "                                                                   return_score=True)\n",
    "        pred_boxes = pred_boxes.view(-1, 4)\n",
    "        # Baseline: Take the mean of all pred boxes as the final result\n",
    "        pred_box = (pred_boxes.mean(dim=0) * self.params.search_size / resize_factor).tolist()  # (cx, cy, w, h) [0,1]\n",
    "        # get the final box result\n",
    "        self.state = clip_box(self.map_box_back(pred_box, resize_factor), H, W, margin=10)\n",
    "        # update hiden state\n",
    "        self.h_state = h\n",
    "        if conf_score.item() < self.update_h_t:\n",
    "            self.h_state = [None] * self.cfg.MODEL.NECK.N_LAYERS\n",
    "\n",
    "        # update the template\n",
    "        if self.num_template > 1:\n",
    "            if (conf_score > self.update_threshold):\n",
    "                z_patch_arr, resize_factor = sample_target(image, self.state, self.params.template_factor,\n",
    "                                                           output_sz=self.params.template_size)\n",
    "                template = self.preprocessor.process(z_patch_arr)\n",
    "                self.memory_template_list.append(template)\n",
    "                prev_box_crop = transform_image_to_crop(torch.tensor(self.state),\n",
    "                                                        torch.tensor(self.state),\n",
    "                                                        resize_factor,\n",
    "                                                        torch.Tensor(\n",
    "                                                            [self.params.template_size, self.params.template_size]),\n",
    "                                                        normalize=True)\n",
    "                self.memory_template_anno_list.append(prev_box_crop.to(template.device).unsqueeze(0))\n",
    "                if len(self.memory_template_list) > self.memory_bank:\n",
    "                    self.memory_template_list.pop(0)\n",
    "                    self.memory_template_anno_list.pop(0)\n",
    "        if (self.frame_id % self.update_intervals == 0):\n",
    "            assert len(self.memory_template_anno_list) == len(self.memory_template_list)\n",
    "            len_list = len(self.memory_template_anno_list)\n",
    "            interval = len_list // self.num_template\n",
    "            for i in range(1, self.num_template):\n",
    "                idx = interval * i\n",
    "                if idx > len_list:\n",
    "                    idx = len_list\n",
    "                self.template_list.append(self.memory_template_list[idx])\n",
    "                self.template_list.pop(1)\n",
    "                self.template_anno_list.append(self.memory_template_anno_list[idx])\n",
    "                self.template_anno_list.pop(1)\n",
    "        assert len(self.template_list) == self.num_template\n",
    "\n",
    "\n",
    "\n",
    "        return {\"target_bbox\": self.state,\n",
    "                \"best_score\": conf_score}\n",
    "\n",
    "    def map_box_back(self, pred_box: list, resize_factor: float):\n",
    "        cx_prev, cy_prev = self.state[0] + 0.5 * self.state[2], self.state[1] + 0.5 * self.state[3]\n",
    "        cx, cy, w, h = pred_box\n",
    "        half_side = 0.5 * self.params.search_size / resize_factor\n",
    "        cx_real = cx + (cx_prev - half_side)\n",
    "        cy_real = cy + (cy_prev - half_side)\n",
    "        return [cx_real - 0.5 * w, cy_real - 0.5 * h, w, h]\n",
    "\n",
    "    def map_box_back_batch(self, pred_box: torch.Tensor, resize_factor: float):\n",
    "        cx_prev, cy_prev = self.state[0] + 0.5 * self.state[2], self.state[1] + 0.5 * self.state[3]\n",
    "        cx, cy, w, h = pred_box.unbind(-1)  # (N,4) --> (N,)\n",
    "        half_side = 0.5 * self.params.search_size / resize_factor\n",
    "        cx_real = cx + (cx_prev - half_side)\n",
    "        cy_real = cy + (cy_prev - half_side)\n",
    "        return torch.stack([cx_real - 0.5 * w, cy_real - 0.5 * h, w, h], dim=-1)\n",
    "\n",
    "def build_mcitrack(cfg):\n",
    "    encoder = build_encoder(cfg)\n",
    "    neck = build_neck(cfg,encoder)\n",
    "    decoder = build_decoder(cfg, neck)\n",
    "    model = MCITrack(\n",
    "        encoder,\n",
    "        decoder,\n",
    "        neck,\n",
    "        cfg,\n",
    "        num_frames = cfg.DATA.SEARCH.NUMBER,\n",
    "        num_template = cfg.DATA.TEMPLATE.NUMBER,\n",
    "        decoder_type=cfg.MODEL.DECODER.TYPE,\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def get_tracker_class():\n",
    "    return MCITrack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aaf0dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"import importlib\n",
    "import os\n",
    "from collections import OrderedDict\n",
    "from lib.test.evaluation.environment import env_settings\n",
    "\n",
    "import cv2 as cv\n",
    "import sys\n",
    "\n",
    "from lib.utils.lmdb_utils import decode_img\n",
    "from pathlib import Path\n",
    "import numpy as np\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "407a71c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = {}\n",
    "\n",
    "# MODEL\n",
    "cfg[\"MODEL\"] = {}\n",
    "\n",
    "# MODEL.ENCODER\n",
    "cfg[\"MODEL\"][\"ENCODER\"] = {\n",
    "    \"TYPE\": \"dinov2_vitb14\",  # encoder model\n",
    "    \"DROP_PATH\": 0,\n",
    "    \"PRETRAIN_TYPE\": \"mae\",  # mae, default, or scratch. This parameter is not activated for dinov2.\n",
    "    \"USE_CHECKPOINT\": False,  # to save the memory.\n",
    "    \"STRIDE\": 14,\n",
    "    \"POS_TYPE\": 'interpolate',  # type of loading the positional encoding. \"interpolate\" or \"index\".\n",
    "    \"TOKEN_TYPE_INDICATE\": False,  # add a token_type_embedding to indicate the search, template_foreground, template_background\n",
    "    \"INTERACTION_INDEXES\": [[0, 6], [6, 12], [12, 18], [18, 24]],\n",
    "    \"GRAD_CKPT\": False\n",
    "}\n",
    "\n",
    "# MODEL.NECK\n",
    "cfg[\"MODEL\"][\"NECK\"] = {\n",
    "    \"N_LAYERS\": 4,\n",
    "    \"D_MODEL\": 512,\n",
    "    \"D_STATE\": 16  # MAMABA_HIDDEN_STATE\n",
    "}\n",
    "\n",
    "# MODEL.DECODER\n",
    "cfg[\"MODEL\"][\"DECODER\"] = {\n",
    "    \"TYPE\": \"CENTER\",  # MLP, CORNER, CENTER\n",
    "    \"NUM_CHANNELS\": 256\n",
    "}\n",
    "\n",
    "# TRAIN\n",
    "cfg[\"TRAIN\"] = {\n",
    "    \"LR\": 0.0001,\n",
    "    \"WEIGHT_DECAY\": 0.0001,\n",
    "    \"EPOCH\": 500,\n",
    "    \"LR_DROP_EPOCH\": 400,\n",
    "    \"BATCH_SIZE\": 8,\n",
    "    \"NUM_WORKER\": 8,\n",
    "    \"OPTIMIZER\": \"ADAMW\",\n",
    "    \"ENCODER_MULTIPLIER\": 0.1,  # encoder's LR = this factor * LR\n",
    "    \"FREEZE_ENCODER\": False,  # for freezing the parameters of encoder\n",
    "    \"ENCODER_OPEN\": [],  # only for debug, open some layers of encoder when FREEZE_ENCODER is True\n",
    "    \"CE_WEIGHT\": 1.0,  # weight for cross-entropy loss\n",
    "    \"GIOU_WEIGHT\": 2.0,\n",
    "    \"L1_WEIGHT\": 5.0,\n",
    "    \"PRINT_INTERVAL\": 50,  # interval to print the training log\n",
    "    \"GRAD_CLIP_NORM\": 0.1,\n",
    "    \"FIX_BN\": False,\n",
    "    \"ENCODER_W\": \"\",\n",
    "    \"TYPE\": \"normal\",  # normal, peft or fft\n",
    "    \"PRETRAINED_PATH\": None\n",
    "}\n",
    "\n",
    "# TRAIN.SCHEDULER\n",
    "cfg[\"TRAIN\"][\"SCHEDULER\"] = {\n",
    "    \"TYPE\": \"step\",\n",
    "    \"DECAY_RATE\": 0.1\n",
    "}\n",
    "\n",
    "# DATA\n",
    "cfg[\"DATA\"] = {\n",
    "    \"MEAN\": [0.485, 0.456, 0.406],\n",
    "    \"STD\": [0.229, 0.224, 0.225],\n",
    "    \"MAX_SAMPLE_INTERVAL\": 200,\n",
    "    \"SAMPLER_MODE\": \"order\",\n",
    "    \"LOADER\": \"tracking\"\n",
    "}\n",
    "\n",
    "# DATA.TRAIN\n",
    "cfg[\"DATA\"][\"TRAIN\"] = {\n",
    "    \"DATASETS_NAME\": [\"LASOT\", \"GOT10K_vottrain\"],\n",
    "    \"DATASETS_RATIO\": [1, 1],\n",
    "    \"SAMPLE_PER_EPOCH\": 60000\n",
    "}\n",
    "\n",
    "# DATA.SEARCH\n",
    "cfg[\"DATA\"][\"SEARCH\"] = {\n",
    "    \"NUMBER\": 1,  # number of search region, only support 1 for now.\n",
    "    \"SIZE\": 256,\n",
    "    \"FACTOR\": 4.0,\n",
    "    \"CENTER_JITTER\": 3.5,\n",
    "    \"SCALE_JITTER\": 0.5\n",
    "}\n",
    "\n",
    "# DATA.TEMPLATE\n",
    "cfg[\"DATA\"][\"TEMPLATE\"] = {\n",
    "    \"NUMBER\": 1,\n",
    "    \"SIZE\": 128,\n",
    "    \"FACTOR\": 2.0,\n",
    "    \"CENTER_JITTER\": 0,\n",
    "    \"SCALE_JITTER\": 0\n",
    "}\n",
    "\n",
    "# TEST\n",
    "cfg[\"TEST\"] = {\n",
    "    \"TEMPLATE_FACTOR\": 4.0,\n",
    "    \"TEMPLATE_SIZE\": 256,\n",
    "    \"SEARCH_FACTOR\": 2.0,\n",
    "    \"SEARCH_SIZE\": 128,\n",
    "    \"EPOCH\": 500,\n",
    "    \"WINDOW\": False,  # window penalty\n",
    "    \"NUM_TEMPLATES\": 1\n",
    "}\n",
    "\n",
    "# TEST.UPT\n",
    "cfg[\"TEST\"][\"UPT\"] = {\n",
    "    \"DEFAULT\": 1,\n",
    "    \"LASOT\": 0,\n",
    "    \"LASOT_EXTENSION_SUBSET\": 0,\n",
    "    \"TRACKINGNET\": 0,\n",
    "    \"TNL2K\": 0,\n",
    "    \"NFS\": 0,\n",
    "    \"UAV\": 0,\n",
    "    \"VOT20\": 0,\n",
    "    \"GOT10K_TEST\": 0\n",
    "}\n",
    "\n",
    "# TEST.UPH\n",
    "cfg[\"TEST\"][\"UPH\"] = {\n",
    "    \"DEFAULT\": 1,\n",
    "    \"LASOT\": 0,\n",
    "    \"LASOT_EXTENSION_SUBSET\": 0,\n",
    "    \"TRACKINGNET\": 0,\n",
    "    \"TNL2K\": 0,\n",
    "    \"NFS\": 0,\n",
    "    \"UAV\": 0,\n",
    "    \"VOT20\": 0,\n",
    "    \"GOT10K_TEST\": 0\n",
    "}\n",
    "\n",
    "# TEST.INTER\n",
    "cfg[\"TEST\"][\"INTER\"] = {\n",
    "    \"DEFAULT\": 999999,\n",
    "    \"LASOT\": 0,\n",
    "    \"LASOT_EXTENSION_SUBSET\": 0,\n",
    "    \"TRACKINGNET\": 0,\n",
    "    \"TNL2K\": 0,\n",
    "    \"NFS\": 0,\n",
    "    \"UAV\": 0,\n",
    "    \"VOT20\": 0,\n",
    "    \"GOT10K_TEST\": 0\n",
    "}\n",
    "\n",
    "# TEST.MB\n",
    "cfg[\"TEST\"][\"MB\"] = {\n",
    "    \"DEFAULT\": 500,\n",
    "    \"LASOT\": 0,\n",
    "    \"LASOT_EXTENSION_SUBSET\": 0,\n",
    "    \"TRACKINGNET\": 0,\n",
    "    \"TNL2K\": 0,\n",
    "    \"NFS\": 0,\n",
    "    \"UAV\": 0,\n",
    "    \"VOT20\": 0,\n",
    "    \"GOT10K_TEST\": 0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5cea04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "96fcdb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _update_config(base_cfg, exp_cfg):\n",
    "    \"\"\"Рекурсивно обновляет base_cfg значениями из exp_cfg.\"\"\"\n",
    "    if isinstance(base_cfg, dict) and isinstance(exp_cfg, dict):\n",
    "        for k, v in exp_cfg.items():\n",
    "            if k in base_cfg:\n",
    "                if not isinstance(v, dict):\n",
    "                    base_cfg[k] = v\n",
    "                else:\n",
    "                    _update_config(base_cfg[k], v)\n",
    "            else:\n",
    "                raise ValueError(f\"{k} not exist in config.py\")\n",
    "    else:\n",
    "        return\n",
    "\n",
    "def update_config_from_file(filename):\n",
    "    \"\"\"Обновляет конфиг cfg данными из YAML-файла.\"\"\"\n",
    "    with open(filename) as f:\n",
    "        exp_config = yaml.safe_load(f)  # Загружаем как обычный dict\n",
    "        _update_config(cfg, exp_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5f2804bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrackerParams:\n",
    "    \"\"\"Class for tracker parameters.\"\"\"\n",
    "    def set_default_values(self, default_vals: dict):\n",
    "        for name, val in default_vals.items():\n",
    "            if not hasattr(self, name):\n",
    "                setattr(self, name, val)\n",
    "\n",
    "    def get(self, name: str, *default):\n",
    "        \"\"\"Get a parameter value with the given name. If it does not exists, it return the default value given as a\n",
    "        second argument or returns an error if no default value is given.\"\"\"\n",
    "        if len(default) > 1:\n",
    "            raise ValueError('Can only give one default value.')\n",
    "\n",
    "        if not default:\n",
    "            return getattr(self, name)\n",
    "\n",
    "        return getattr(self, name, default[0])\n",
    "\n",
    "    def has(self, name: str):\n",
    "        \"\"\"Check if there exist a parameter with the given name.\"\"\"\n",
    "        return hasattr(self, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b212a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1d61ea2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parameters(yaml_name: str):\n",
    "    params = TrackerParams()\n",
    "\n",
    "    yaml_file = \"mcitrack_t224.yaml\"\n",
    "    update_config_from_file(yaml_file)\n",
    "    params.cfg = cfg\n",
    "    print(\"test config: \", cfg)\n",
    "\n",
    "    params.yaml_name = yaml_name\n",
    "    # template and search region\n",
    "    params.template_factor = cfg.TEST.TEMPLATE_FACTOR\n",
    "    params.template_size = cfg.TEST.TEMPLATE_SIZE\n",
    "    params.search_factor = cfg.TEST.SEARCH_FACTOR\n",
    "    params.search_size = cfg.TEST.SEARCH_SIZE\n",
    "\n",
    "    # Network checkpoint path\n",
    "    params.checkpoint = \"fast_itpn_tiny_1600e_1k.pt\"\n",
    "    # whether to save boxes from all queries\n",
    "    params.save_all_boxes = False\n",
    "\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a0637bab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test config:  {'MODEL': {'ENCODER': {'TYPE': 'fastitpnt', 'DROP_PATH': 0.1, 'PRETRAIN_TYPE': './fast_itpn_tiny_1600e_1k.pt', 'USE_CHECKPOINT': False, 'STRIDE': 16, 'POS_TYPE': 'index', 'TOKEN_TYPE_INDICATE': True, 'INTERACTION_INDEXES': [[4, 7], [7, 10], [10, 13], [13, 16]], 'GRAD_CKPT': False}, 'NECK': {'N_LAYERS': 4, 'D_MODEL': 384, 'D_STATE': 16}, 'DECODER': {'TYPE': 'CENTER', 'NUM_CHANNELS': 256}}, 'TRAIN': {'LR': 0.0004, 'WEIGHT_DECAY': 0.0001, 'EPOCH': 300, 'LR_DROP_EPOCH': 240, 'BATCH_SIZE': 64, 'NUM_WORKER': 10, 'OPTIMIZER': 'ADAMW', 'ENCODER_MULTIPLIER': 0.1, 'FREEZE_ENCODER': False, 'ENCODER_OPEN': [], 'CE_WEIGHT': 1.0, 'GIOU_WEIGHT': 2.0, 'L1_WEIGHT': 5.0, 'PRINT_INTERVAL': 50, 'GRAD_CLIP_NORM': 0.1, 'FIX_BN': False, 'ENCODER_W': '', 'TYPE': 'normal', 'PRETRAINED_PATH': None, 'SCHEDULER': {'TYPE': 'step', 'DECAY_RATE': 0.1}}, 'DATA': {'MEAN': [0.485, 0.456, 0.406], 'STD': [0.229, 0.224, 0.225], 'MAX_SAMPLE_INTERVAL': 400, 'SAMPLER_MODE': 'order', 'LOADER': 'tracking', 'TRAIN': {'DATASETS_NAME': ['LASOT', 'GOT10K_vottrain', 'COCO17', 'TRACKINGNET', 'VASTTRACK'], 'DATASETS_RATIO': [1, 1, 1, 1, 1], 'SAMPLE_PER_EPOCH': 60000}, 'SEARCH': {'NUMBER': 2, 'SIZE': 224, 'FACTOR': 4.0, 'CENTER_JITTER': 3.5, 'SCALE_JITTER': 0.5}, 'TEMPLATE': {'NUMBER': 5, 'SIZE': 112, 'FACTOR': 2.0, 'CENTER_JITTER': 0, 'SCALE_JITTER': 0}}, 'TEST': {'TEMPLATE_FACTOR': 2.0, 'TEMPLATE_SIZE': 112, 'SEARCH_FACTOR': 4.0, 'SEARCH_SIZE': 224, 'EPOCH': 300, 'WINDOW': True, 'NUM_TEMPLATES': 5, 'UPT': {'DEFAULT': 1, 'LASOT': 0.8, 'LASOT_EXTENSION_SUBSET': 0.85, 'TRACKINGNET': 0.5, 'TNL2K': 0.5, 'NFS': 0.8, 'UAV': 0.2, 'VOT20': 0.4, 'GOT10K_TEST': 0}, 'UPH': {'DEFAULT': 1, 'LASOT': 0.88, 'LASOT_EXTENSION_SUBSET': 0.97, 'TRACKINGNET': 0.9, 'TNL2K': 0.9, 'NFS': 0.92, 'UAV': 0.91, 'VOT20': 0.94, 'GOT10K_TEST': 0}, 'INTER': {'DEFAULT': 999999, 'LASOT': 70, 'LASOT_EXTENSION_SUBSET': 50, 'TRACKINGNET': 20, 'TNL2K': 20, 'NFS': 90, 'UAV': 1, 'VOT20': 1, 'GOT10K_TEST': 0}, 'MB': {'DEFAULT': 500, 'LASOT': 500, 'LASOT_EXTENSION_SUBSET': 500, 'TRACKINGNET': 200, 'TNL2K': 500, 'NFS': 500, 'UAV': 400, 'VOT20': 500, 'GOT10K_TEST': 0}}}\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'TEST'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m params \u001b[38;5;241m=\u001b[39m \u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./mcitrack_t224.yaml\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[23], line 11\u001b[0m, in \u001b[0;36mparameters\u001b[1;34m(yaml_name)\u001b[0m\n\u001b[0;32m      9\u001b[0m params\u001b[38;5;241m.\u001b[39myaml_name \u001b[38;5;241m=\u001b[39m yaml_name\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# template and search region\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m params\u001b[38;5;241m.\u001b[39mtemplate_factor \u001b[38;5;241m=\u001b[39m \u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTEST\u001b[49m\u001b[38;5;241m.\u001b[39mTEMPLATE_FACTOR\n\u001b[0;32m     12\u001b[0m params\u001b[38;5;241m.\u001b[39mtemplate_size \u001b[38;5;241m=\u001b[39m cfg\u001b[38;5;241m.\u001b[39mTEST\u001b[38;5;241m.\u001b[39mTEMPLATE_SIZE\n\u001b[0;32m     13\u001b[0m params\u001b[38;5;241m.\u001b[39msearch_factor \u001b[38;5;241m=\u001b[39m cfg\u001b[38;5;241m.\u001b[39mTEST\u001b[38;5;241m.\u001b[39mSEARCH_FACTOR\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'dict' object has no attribute 'TEST'"
     ]
    }
   ],
   "source": [
    "params = parameters(\"./mcitrack_t224.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4f256b16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network = build_mcitrack(params.cfg)\n",
    "network.load_state_dict(torch.load(\"MCITRACK_ep0300.pth.tar\", map_location='cpu')['net'], strict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63644534",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae331c67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update threshold is:  500\n"
     ]
    }
   ],
   "source": [
    "treacker = MCITrack(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae7ecd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import cv2\n",
    "#import sys\n",
    "\n",
    "file = \"0516.mp4\"\n",
    "video = cv2.VideoCapture(file)\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "fps=video.get(cv2.CAP_PROP_FPS)\n",
    "video_vriter = cv2.VideoWriter(file.split('.')[0]+\"_\"+\".avi\", fourcc, fps, (1920, 1080))\n",
    "\n",
    "\n",
    "ok, image = video.read()\n",
    "if not video.isOpened():\n",
    "    print(\"Could not open video\")\n",
    "    sys.exit()\n",
    "    \n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "x, y, w, h = cv2.selectROI( image, fromCenter=False)\n",
    "init_state = [x, y, w, h]\n",
    "def _build_init_info(box):\n",
    "            return {'init_bbox': box}\n",
    "treacker.initialize(image, _build_init_info(init_state))\n",
    "counter = 0\n",
    "while True:\n",
    "            ok, image = video.read()\n",
    "            if not ok:\n",
    "                print(\"Can't read frame\")\n",
    "                break\n",
    "\n",
    "            \n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            start = time.time() \n",
    "            out  = treacker.track(image)\n",
    "            state = [int(s) for s in out['target_bbox']]\n",
    "            best_score=out[\"best_score\"].cpu().numpy()[0][0]\n",
    "            end_time = (time.time() - start)\n",
    "            \n",
    "            \n",
    "            org = (50, 50)\n",
    "\n",
    "            # fontScale\n",
    "            fontScale = 1\n",
    "            font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "            # Blue color in BGR\n",
    "            color = (255, 0, 0)\n",
    "            # Line thickness of 2 px\n",
    "            thickness = 2              \n",
    "            # Using cv2.putText() method\n",
    "            image = cv2.putText(image, str(best_score), org, font, \n",
    "                            fontScale, color, thickness, cv2.LINE_AA)\n",
    "            image = cv2.putText(image, str(end_time), (50,100), font, \n",
    "                            fontScale, color, thickness, cv2.LINE_AA)\n",
    "\n",
    "            x, y, w, h = [int(x) for x in state]\n",
    "\n",
    "            color = (200, 0, 0)  # Цвет в формате BGR\n",
    "            cv2.rectangle(image, (x, y), (x + w, y + h), color, 2)\n",
    "\n",
    "\n",
    "            cv2.imshow(\"tracking\", image)\n",
    "            video_vriter.write(image)\n",
    "\n",
    "\n",
    "            k = cv2.waitKey(1)            \n",
    "            if k == 32:  # SPACE\n",
    "                ok, image = video.read()                             \n",
    "                x, y, w, h = cv.selectROI( image, fromCenter=False)\n",
    "                init_state = [x, y, w, h]\n",
    "                treacker.initialize(image, _build_init_info(init_state))\n",
    "            if k == 27:  # ESC\n",
    "                break\n",
    "        \n",
    "                \n",
    "                \n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "video.release()\n",
    "video_vriter.release()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e8df45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1d032c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3_10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
