{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' TRT_LOGGER = trt.Logger(trt.Logger.WARNING)\\n\\ndef load_engine(trt_runtime, engine_path):\\n    with open(engine_path, \"rb\") as f:\\n        engine_data = f.read()\\n    return trt_runtime.deserialize_cuda_engine(engine_data)\\n\\ntrt_runtime = trt.Runtime(TRT_LOGGER)\\nengine = load_engine(trt_runtime, \"MCITrack.trt\")\\nengine '"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorrt as trt\n",
    "import pycuda.driver as cuda\n",
    "import pycuda.autoinit  # автоматически инициализирует CUDA контекст\n",
    "import torch\n",
    "import cv2\n",
    "import sys\n",
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "import yaml\n",
    "import torch\n",
    "import numpy as np\n",
    "import onnx\n",
    "import onnxruntime as ort\n",
    "from onnx import numpy_helper\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\"\"\" TRT_LOGGER = trt.Logger(trt.Logger.WARNING)\n",
    "\n",
    "def load_engine(trt_runtime, engine_path):\n",
    "    with open(engine_path, \"rb\") as f:\n",
    "        engine_data = f.read()\n",
    "    return trt_runtime.deserialize_cuda_engine(engine_data)\n",
    "\n",
    "trt_runtime = trt.Runtime(TRT_LOGGER)\n",
    "engine = load_engine(trt_runtime, \"MCITrack.trt\")\n",
    "engine \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" def cal_bbox(score_map_ctr, size_map, offset_map, return_score=True):\n",
    "    \n",
    "    # 2. Получаем размеры feature map\n",
    "    feat_h, feat_w = score_map_ctr.shape[-2], score_map_ctr.shape[-1]\n",
    "    \n",
    "    # 3. Находим позицию с максимальным score (современный способ)\n",
    "    max_score, flat_idx = torch.max(score_map_ctr.flatten(1), dim=1)\n",
    "    idx = flat_idx.unsqueeze(1)\n",
    "    idx_y = torch.div(flat_idx, feat_w, rounding_mode='floor')\n",
    "    idx_x = flat_idx % feat_w\n",
    "    \n",
    "    # 4. Подготовка индексов для gather\n",
    "    gather_idx = idx.unsqueeze(1).expand(-1, 2, -1)\n",
    "    \n",
    "    # 5. Обработка size_map (расширяем если 1 канал)\n",
    "    if size_map.size(1) == 1:\n",
    "        size_map = size_map.expand(-1, 2, -1, -1)\n",
    "    \n",
    "    # 6. Получаем размеры и смещения\n",
    "    try:\n",
    "        size = size_map.flatten(2).gather(2, gather_idx)\n",
    "        offset = offset_map.flatten(2).gather(2, gather_idx).squeeze(-1)\n",
    "    except RuntimeError as e:\n",
    "        print(\"Ошибка размерностей:\")\n",
    "        print(f\"score_map_ctr: {score_map_ctr.shape}\")\n",
    "        print(f\"size_map: {size_map.shape}\")\n",
    "        print(f\"offset_map: {offset_map.shape}\")\n",
    "        print(f\"gather_idx: {gather_idx.shape}\")\n",
    "        raise\n",
    "    \n",
    "    # 7. Формируем bbox (cx, cy, w, h)\n",
    "    bbox = torch.cat([\n",
    "        (idx_x.to(torch.float) + offset[:, 0:1]) / feat_w,\n",
    "        (idx_y.to(torch.float) + offset[:, 1:2]) / feat_h,\n",
    "        size.squeeze(-1)\n",
    "    ], dim=1)\n",
    "    \n",
    "    return (bbox, max_score) if return_score else bbox \"\"\"\n",
    "\n",
    "def cal_bbox(score_map_ctr, size_map, offset_map, return_score=True):\n",
    "        feat_sz = 14\n",
    "        max_score, idx = torch.max(score_map_ctr.flatten(1), dim=1, keepdim=True) # score_map_ctr.flatten(1): torch.Size([32, 256]) idx: torch.Size([32, 1]) max_score: torch.Size([32, 1])\n",
    "        idx_y = torch.div(idx, feat_sz, rounding_mode='floor')\n",
    "        idx_x = idx % feat_sz\n",
    "       \n",
    "        \n",
    "\n",
    "        idx = idx.unsqueeze(1).expand(idx.shape[0], 2, 1)\n",
    "        size = size_map.flatten(2).gather(dim=2, index=idx) # size_map: torch.Size([32, 2, 16, 16])  size_map.flatten(2): torch.Size([32, 2, 256])\n",
    "        offset = offset_map.flatten(2).gather(dim=2, index=idx).squeeze(-1)\n",
    "\n",
    "        # bbox = torch.cat([idx_x - size[:, 0] / 2, idx_y - size[:, 1] / 2,\n",
    "        #                   idx_x + size[:, 0] / 2, idx_y + size[:, 1] / 2], dim=1) / self.feat_sz\n",
    "        # cx, cy, w, h\n",
    "        bbox = torch.cat([(idx_x.to(torch.float) + offset[:, :1]) / feat_sz,\n",
    "                          (idx_y.to(torch.float) + offset[:, 1:]) / feat_sz,\n",
    "                          size.squeeze(-1)], dim=1)\n",
    "\n",
    "        if return_score:\n",
    "            return bbox, max_score\n",
    "        return (bbox, max_score) if return_score else bbox\n",
    "        \n",
    "class Preprocessor(object):\n",
    "    def __init__(self):\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.mean = torch.tensor([0.485, 0.456, 0.406]).view((1, 3, 1, 1)).to(self.device)\n",
    "        self.std = torch.tensor([0.229, 0.224, 0.225]).view((1, 3, 1, 1)).to(self.device)\n",
    "        self.mm_mean = torch.tensor([0.485, 0.456, 0.406, 0.485, 0.456, 0.406]).view((1, 6, 1, 1)).to(self.device)\n",
    "        self.mm_std = torch.tensor([0.229, 0.224, 0.225, 0.229, 0.224, 0.225]).view((1, 6, 1, 1)).to(self.device)\n",
    "\n",
    "    def process(self, img_arr: np.ndarray):\n",
    "        if img_arr.shape[-1] == 6:\n",
    "            mean = self.mm_mean\n",
    "            std = self.mm_std\n",
    "        else:\n",
    "            mean = self.mean\n",
    "            std = self.std\n",
    "        # Deal with the image patch\n",
    "        img_tensor = torch.tensor(img_arr).to(self.device).float().permute((2,0,1)).unsqueeze(dim=0)        \n",
    "        img_tensor_norm = ((img_tensor / 255.0) - mean) / std  # (1,3,H,W)\n",
    "        return img_tensor_norm\n",
    "    \n",
    "def hann1d(sz: int, centered = True) -> torch.Tensor:\n",
    "    \"\"\"1D cosine window.\"\"\"\n",
    "    if centered:\n",
    "        return 0.5 * (1 - torch.cos((2 * math.pi / (sz + 1)) * torch.arange(1, sz + 1).float()))\n",
    "    w = 0.5 * (1 + torch.cos((2 * math.pi / (sz + 2)) * torch.arange(0, sz//2 + 1).float()))\n",
    "    return torch.cat([w, w[1:sz-sz//2].flip((0,))])\n",
    "    \n",
    "def hann2d(sz: torch.Tensor, centered = True) -> torch.Tensor:\n",
    "    \"\"\"2D cosine window.\"\"\"\n",
    "    return hann1d(sz[0].item(), centered).reshape(1, 1, -1, 1) * hann1d(sz[1].item(), centered).reshape(1, 1, 1, -1)    \n",
    "\n",
    "def sample_target(im, target_bb, search_area_factor, output_sz=None):\n",
    "   \n",
    "    if not isinstance(target_bb, list):\n",
    "        x, y, w, h = target_bb.tolist()\n",
    "    else:\n",
    "        x, y, w, h = target_bb\n",
    "    # Crop image\n",
    "    crop_sz = math.ceil(math.sqrt(w * h) * search_area_factor)\n",
    "\n",
    "    if crop_sz < 1:\n",
    "        raise Exception('Too small bounding box.')\n",
    "\n",
    "    x1 = round(x + 0.5 * w - crop_sz * 0.5)\n",
    "    x2 = x1 + crop_sz\n",
    "\n",
    "    y1 = round(y + 0.5 * h - crop_sz * 0.5)\n",
    "    y2 = y1 + crop_sz\n",
    "\n",
    "    x1_pad = max(0, -x1)\n",
    "    x2_pad = max(x2 - im.shape[1] + 1, 0)\n",
    "\n",
    "    y1_pad = max(0, -y1)\n",
    "    y2_pad = max(y2 - im.shape[0] + 1, 0)\n",
    "\n",
    "    # Crop target\n",
    "    im_crop = im[y1 + y1_pad:y2 - y2_pad, x1 + x1_pad:x2 - x2_pad, :]\n",
    "\n",
    "    # Pad\n",
    "    im_crop_padded = cv2.copyMakeBorder(im_crop, y1_pad, y2_pad, x1_pad, x2_pad, cv2.BORDER_CONSTANT)\n",
    "    # deal with attention mask\n",
    "    H, W, _ = im_crop_padded.shape\n",
    "\n",
    "    if output_sz is not None:\n",
    "        resize_factor = output_sz / crop_sz\n",
    "        im_crop_padded = cv2.resize(im_crop_padded, (output_sz, output_sz))\n",
    "\n",
    "        return im_crop_padded, resize_factor\n",
    "\n",
    "    else:\n",
    "        return im_crop_padded, 1.0\n",
    "def transform_image_to_crop(box_in: torch.Tensor, box_extract: torch.Tensor, resize_factor: float,\n",
    "                            crop_sz: torch.Tensor, normalize=False) -> torch.Tensor:\n",
    "   \n",
    "    box_extract_center = box_extract[0:2] + 0.5 * box_extract[2:4]\n",
    "\n",
    "    box_in_center = box_in[0:2] + 0.5 * box_in[2:4]\n",
    "\n",
    "    box_out_center = (crop_sz - 1) / 2 + (box_in_center - box_extract_center) * resize_factor\n",
    "    box_out_wh = box_in[2:4] * resize_factor\n",
    "\n",
    "    box_out = torch.cat((box_out_center - 0.5 * box_out_wh, box_out_wh))\n",
    "    if normalize:\n",
    "        return box_out / (crop_sz[0]-1)\n",
    "    else:\n",
    "        return box_out\n",
    "def clip_box(box: list, H, W, margin=0):\n",
    "    x1, y1, w, h = box\n",
    "    x2, y2 = x1 + w, y1 + h\n",
    "    x1 = min(max(0, x1), W-margin)\n",
    "    x2 = min(max(margin, x2), W)\n",
    "    y1 = min(max(0, y1), H-margin)\n",
    "    y2 = min(max(margin, y2), H)\n",
    "    w = max(margin, x2-x1)\n",
    "    h = max(margin, y2-y1)\n",
    "    return [x1, y1, w, h]\n",
    "\n",
    "class BaseTracker():\n",
    "    \"\"\"Base class for all trackers.\"\"\"\n",
    "\n",
    "    def __init__(self, params):\n",
    "        self.params = params\n",
    "        self.visdom = None\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    def predicts_segmentation_mask(self):\n",
    "        return False\n",
    "\n",
    "    def initialize(self, image, info: dict) -> dict:\n",
    "        \"\"\"Overload this function in your tracker. This should initialize the model.\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def track(self, image, info: dict = None) -> dict:\n",
    "        \"\"\"Overload this function in your tracker. This should track in the frame and update the model.\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def visdom_draw_tracking(self, image, box, segmentation=None):\n",
    "        # Упрощенная обработка box без OrderedDict\n",
    "        if isinstance(box, dict):  # Проверяем на обычный dict вместо OrderedDict\n",
    "            box = list(box.values())  # Берем только значения\n",
    "        elif not isinstance(box, (list, tuple)):  # Если не коллекция\n",
    "            box = (box,)  # Превращаем в кортеж\n",
    "        \n",
    "        # Визуализация\n",
    "        if segmentation is None:\n",
    "            self.visdom.register((image, *box), 'Tracking', 1, 'Tracking')\n",
    "        else:\n",
    "            self.visdom.register((image, *box, segmentation), 'Tracking', 1, 'Tracking')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = {}\n",
    "\n",
    "# MODEL\n",
    "cfg[\"MODEL\"] = {}\n",
    "\n",
    "# MODEL.ENCODER\n",
    "cfg[\"MODEL\"][\"ENCODER\"] = {\n",
    "    \"TYPE\": \"dinov2_vitb14\",  # encoder model\n",
    "    \"DROP_PATH\": 0,\n",
    "    \"PRETRAIN_TYPE\": \"mae\",  # mae, default, or scratch. This parameter is not activated for dinov2.\n",
    "    \"USE_CHECKPOINT\": False,  # to save the memory.\n",
    "    \"STRIDE\": 14,\n",
    "    \"POS_TYPE\": 'interpolate',  # type of loading the positional encoding. \"interpolate\" or \"index\".\n",
    "    \"TOKEN_TYPE_INDICATE\": False,  # add a token_type_embedding to indicate the search, template_foreground, template_background\n",
    "    \"INTERACTION_INDEXES\": [[0, 6], [6, 12], [12, 18], [18, 24]],\n",
    "    \"GRAD_CKPT\": False\n",
    "}\n",
    "\n",
    "# MODEL.NECK\n",
    "cfg[\"MODEL\"][\"NECK\"] = {\n",
    "    \"N_LAYERS\": 4,\n",
    "    \"D_MODEL\": 512,\n",
    "    \"D_STATE\": 16  # MAMABA_HIDDEN_STATE\n",
    "}\n",
    "\n",
    "# MODEL.DECODER\n",
    "cfg[\"MODEL\"][\"DECODER\"] = {\n",
    "    \"TYPE\": \"CENTER\",  # MLP, CORNER, CENTER\n",
    "    \"NUM_CHANNELS\": 256\n",
    "}\n",
    "\n",
    "# TRAIN\n",
    "cfg[\"TRAIN\"] = {\n",
    "    \"LR\": 0.0001,\n",
    "    \"WEIGHT_DECAY\": 0.0001,\n",
    "    \"EPOCH\": 500,\n",
    "    \"LR_DROP_EPOCH\": 400,\n",
    "    \"BATCH_SIZE\": 8,\n",
    "    \"NUM_WORKER\": 8,\n",
    "    \"OPTIMIZER\": \"ADAMW\",\n",
    "    \"ENCODER_MULTIPLIER\": 0.1,  # encoder's LR = this factor * LR\n",
    "    \"FREEZE_ENCODER\": False,  # for freezing the parameters of encoder\n",
    "    \"ENCODER_OPEN\": [],  # only for debug, open some layers of encoder when FREEZE_ENCODER is True\n",
    "    \"CE_WEIGHT\": 1.0,  # weight for cross-entropy loss\n",
    "    \"GIOU_WEIGHT\": 2.0,\n",
    "    \"L1_WEIGHT\": 5.0,\n",
    "    \"PRINT_INTERVAL\": 50,  # interval to print the training log\n",
    "    \"GRAD_CLIP_NORM\": 0.1,\n",
    "    \"FIX_BN\": False,\n",
    "    \"ENCODER_W\": \"\",\n",
    "    \"TYPE\": \"normal\",  # normal, peft or fft\n",
    "    \"PRETRAINED_PATH\": None\n",
    "}\n",
    "\n",
    "# TRAIN.SCHEDULER\n",
    "cfg[\"TRAIN\"][\"SCHEDULER\"] = {\n",
    "    \"TYPE\": \"step\",\n",
    "    \"DECAY_RATE\": 0.1\n",
    "}\n",
    "\n",
    "# DATA\n",
    "cfg[\"DATA\"] = {\n",
    "    \"MEAN\": [0.485, 0.456, 0.406],\n",
    "    \"STD\": [0.229, 0.224, 0.225],\n",
    "    \"MAX_SAMPLE_INTERVAL\": 200,\n",
    "    \"SAMPLER_MODE\": \"order\",\n",
    "    \"LOADER\": \"tracking\"\n",
    "}\n",
    "\n",
    "# DATA.TRAIN\n",
    "cfg[\"DATA\"][\"TRAIN\"] = {\n",
    "    \"DATASETS_NAME\": [\"LASOT\", \"GOT10K_vottrain\"],\n",
    "    \"DATASETS_RATIO\": [1, 1],\n",
    "    \"SAMPLE_PER_EPOCH\": 60000\n",
    "}\n",
    "\n",
    "# DATA.SEARCH\n",
    "cfg[\"DATA\"][\"SEARCH\"] = {\n",
    "    \"NUMBER\": 1,  # number of search region, only support 1 for now.\n",
    "    \"SIZE\": 256,\n",
    "    \"FACTOR\": 4.0,\n",
    "    \"CENTER_JITTER\": 3.5,\n",
    "    \"SCALE_JITTER\": 0.5\n",
    "}\n",
    "\n",
    "# DATA.TEMPLATE\n",
    "cfg[\"DATA\"][\"TEMPLATE\"] = {\n",
    "    \"NUMBER\": 1,\n",
    "    \"SIZE\": 128,\n",
    "    \"FACTOR\": 2.0,\n",
    "    \"CENTER_JITTER\": 0,\n",
    "    \"SCALE_JITTER\": 0\n",
    "}\n",
    "\n",
    "# TEST\n",
    "cfg[\"TEST\"] = {\n",
    "    \"TEMPLATE_FACTOR\": 4.0,\n",
    "    \"TEMPLATE_SIZE\": 256,\n",
    "    \"SEARCH_FACTOR\": 2.0,\n",
    "    \"SEARCH_SIZE\": 128,\n",
    "    \"EPOCH\": 500,\n",
    "    \"WINDOW\": False,  # window penalty\n",
    "    \"NUM_TEMPLATES\": 1\n",
    "}\n",
    "\n",
    "# TEST.UPT\n",
    "cfg[\"TEST\"][\"UPT\"] = {\n",
    "    \"DEFAULT\": 1,\n",
    "    \"LASOT\": 0,\n",
    "    \"LASOT_EXTENSION_SUBSET\": 0,\n",
    "    \"TRACKINGNET\": 0,\n",
    "    \"TNL2K\": 0,\n",
    "    \"NFS\": 0,\n",
    "    \"UAV\": 0,\n",
    "    \"VOT20\": 0,\n",
    "    \"GOT10K_TEST\": 0\n",
    "}\n",
    "\n",
    "# TEST.UPH\n",
    "cfg[\"TEST\"][\"UPH\"] = {\n",
    "    \"DEFAULT\": 1,\n",
    "    \"LASOT\": 0,\n",
    "    \"LASOT_EXTENSION_SUBSET\": 0,\n",
    "    \"TRACKINGNET\": 0,\n",
    "    \"TNL2K\": 0,\n",
    "    \"NFS\": 0,\n",
    "    \"UAV\": 0,\n",
    "    \"VOT20\": 0,\n",
    "    \"GOT10K_TEST\": 0\n",
    "}\n",
    "\n",
    "# TEST.INTER\n",
    "cfg[\"TEST\"][\"INTER\"] = {\n",
    "    \"DEFAULT\": 999999,\n",
    "    \"LASOT\": 0,\n",
    "    \"LASOT_EXTENSION_SUBSET\": 0,\n",
    "    \"TRACKINGNET\": 0,\n",
    "    \"TNL2K\": 0,\n",
    "    \"NFS\": 0,\n",
    "    \"UAV\": 0,\n",
    "    \"VOT20\": 0,\n",
    "    \"GOT10K_TEST\": 0\n",
    "}\n",
    "\n",
    "# TEST.MB\n",
    "cfg[\"TEST\"][\"MB\"] = {\n",
    "    \"DEFAULT\": 500,\n",
    "    \"LASOT\": 0,\n",
    "    \"LASOT_EXTENSION_SUBSET\": 0,\n",
    "    \"TRACKINGNET\": 0,\n",
    "    \"TNL2K\": 0,\n",
    "    \"NFS\": 0,\n",
    "    \"UAV\": 0,\n",
    "    \"VOT20\": 0,\n",
    "    \"GOT10K_TEST\": 0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test config:  {'MODEL': {'ENCODER': {'TYPE': 'fastitpnt', 'DROP_PATH': 0.1, 'PRETRAIN_TYPE': './fast_itpn_tiny_1600e_1k.pt', 'USE_CHECKPOINT': False, 'STRIDE': 16, 'POS_TYPE': 'index', 'TOKEN_TYPE_INDICATE': True, 'INTERACTION_INDEXES': [[4, 7], [7, 10], [10, 13], [13, 16]], 'GRAD_CKPT': False}, 'NECK': {'N_LAYERS': 4, 'D_MODEL': 384, 'D_STATE': 16}, 'DECODER': {'TYPE': 'CENTER', 'NUM_CHANNELS': 256}}, 'TRAIN': {'LR': 0.0004, 'WEIGHT_DECAY': 0.0001, 'EPOCH': 300, 'LR_DROP_EPOCH': 240, 'BATCH_SIZE': 64, 'NUM_WORKER': 10, 'OPTIMIZER': 'ADAMW', 'ENCODER_MULTIPLIER': 0.1, 'FREEZE_ENCODER': False, 'ENCODER_OPEN': [], 'CE_WEIGHT': 1.0, 'GIOU_WEIGHT': 2.0, 'L1_WEIGHT': 5.0, 'PRINT_INTERVAL': 50, 'GRAD_CLIP_NORM': 0.1, 'FIX_BN': False, 'ENCODER_W': '', 'TYPE': 'normal', 'PRETRAINED_PATH': None, 'SCHEDULER': {'TYPE': 'step', 'DECAY_RATE': 0.1}}, 'DATA': {'MEAN': [0.485, 0.456, 0.406], 'STD': [0.229, 0.224, 0.225], 'MAX_SAMPLE_INTERVAL': 400, 'SAMPLER_MODE': 'order', 'LOADER': 'tracking', 'TRAIN': {'DATASETS_NAME': ['LASOT', 'GOT10K_vottrain', 'COCO17', 'TRACKINGNET', 'VASTTRACK'], 'DATASETS_RATIO': [1, 1, 1, 1, 1], 'SAMPLE_PER_EPOCH': 60000}, 'SEARCH': {'NUMBER': 2, 'SIZE': 224, 'FACTOR': 4.0, 'CENTER_JITTER': 3.5, 'SCALE_JITTER': 0.5}, 'TEMPLATE': {'NUMBER': 5, 'SIZE': 112, 'FACTOR': 2.0, 'CENTER_JITTER': 0, 'SCALE_JITTER': 0}}, 'TEST': {'TEMPLATE_FACTOR': 2.0, 'TEMPLATE_SIZE': 112, 'SEARCH_FACTOR': 4.0, 'SEARCH_SIZE': 224, 'EPOCH': 300, 'WINDOW': True, 'NUM_TEMPLATES': 5, 'UPT': {'DEFAULT': 1, 'LASOT': 0.8, 'LASOT_EXTENSION_SUBSET': 0.85, 'TRACKINGNET': 0.5, 'TNL2K': 0.5, 'NFS': 0.8, 'UAV': 0.2, 'VOT20': 0.4, 'GOT10K_TEST': 0}, 'UPH': {'DEFAULT': 1, 'LASOT': 0.88, 'LASOT_EXTENSION_SUBSET': 0.97, 'TRACKINGNET': 0.9, 'TNL2K': 0.9, 'NFS': 0.92, 'UAV': 0.91, 'VOT20': 0.94, 'GOT10K_TEST': 0}, 'INTER': {'DEFAULT': 999999, 'LASOT': 70, 'LASOT_EXTENSION_SUBSET': 50, 'TRACKINGNET': 20, 'TNL2K': 20, 'NFS': 90, 'UAV': 1, 'VOT20': 1, 'GOT10K_TEST': 0}, 'MB': {'DEFAULT': 500, 'LASOT': 500, 'LASOT_EXTENSION_SUBSET': 500, 'TRACKINGNET': 200, 'TNL2K': 500, 'NFS': 500, 'UAV': 400, 'VOT20': 500, 'GOT10K_TEST': 0}}}\n"
     ]
    }
   ],
   "source": [
    "#Params\n",
    "class TrackerParams:\n",
    "    \"\"\"Class for tracker parameters.\"\"\"\n",
    "    def set_default_values(self, default_vals: dict):\n",
    "        for name, val in default_vals.items():\n",
    "            if not hasattr(self, name):\n",
    "                setattr(self, name, val)\n",
    "\n",
    "    def get(self, name: str, *default):\n",
    "        \"\"\"Get a parameter value with the given name. If it does not exists, it return the default value given as a\n",
    "        second argument or returns an error if no default value is given.\"\"\"\n",
    "        if len(default) > 1:\n",
    "            raise ValueError('Can only give one default value.')\n",
    "\n",
    "        if not default:\n",
    "            return getattr(self, name)\n",
    "\n",
    "        return getattr(self, name, default[0])\n",
    "\n",
    "    def has(self, name: str):\n",
    "        \"\"\"Check if there exist a parameter with the given name.\"\"\"\n",
    "        return hasattr(self, name)\n",
    "\n",
    "def _update_config(base_cfg, exp_cfg):\n",
    "    if isinstance(base_cfg, dict) and isinstance(exp_cfg, dict):\n",
    "        for k, v in exp_cfg.items():\n",
    "            if k in base_cfg:\n",
    "                if not isinstance(v, dict):\n",
    "                    base_cfg[k] = v\n",
    "                else:\n",
    "                    _update_config(base_cfg[k], v)\n",
    "            else:\n",
    "                raise ValueError(\"{} not exist in config.py\".format(k))\n",
    "    else:\n",
    "        return\n",
    "\n",
    "def update_config_from_file(filename):\n",
    "    exp_config = None\n",
    "    with open(filename) as f:\n",
    "        exp_config = yaml.safe_load(f)\n",
    "        _update_config(cfg, exp_config)\n",
    "    \n",
    "def parameters(yaml_name: str):\n",
    "    params = TrackerParams()\n",
    "\n",
    "    yaml_file = \"mcitrack_t224.yaml\"\n",
    "    update_config_from_file(yaml_file)\n",
    "    params.cfg = cfg\n",
    "    print(\"test config: \", cfg)\n",
    "\n",
    "    params.yaml_name = yaml_name\n",
    "    # template and search region\n",
    "    params.template_factor = cfg[\"TEST\"][\"TEMPLATE_FACTOR\"]\n",
    "    params.template_size = cfg[\"TEST\"][\"TEMPLATE_SIZE\"]\n",
    "    params.search_factor = cfg[\"TEST\"][\"SEARCH_FACTOR\"]\n",
    "    params.search_size = cfg[\"TEST\"][\"SEARCH_SIZE\"]\n",
    "\n",
    "    # Network checkpoint path\n",
    "    params.checkpoint = \"MCITrack.trt\"\n",
    "    # whether to save boxes from all queries\n",
    "    params.save_all_boxes = False\n",
    "\n",
    "    return params\n",
    "\n",
    "params = parameters(\"./mcitrack_t224.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Извлечение констант из ONNX модели\n",
    "def extract_model_constants(onnx_model):\n",
    "    constants = {}\n",
    "    for node in onnx_model.graph.node:\n",
    "        if node.op_type == \"Constant\":\n",
    "            tensor = numpy_helper.to_array(node.attribute[0].t)\n",
    "            constants[node.output[0]] = tensor\n",
    "    return constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRT Tracker\n",
    "TRT_LOGGER = trt.Logger(trt.Logger.WARNING)\n",
    "\n",
    "class MCITRACK(BaseTracker):\n",
    "    def __init__(self, params):\n",
    "        super(MCITRACK, self).__init__(params)\n",
    "        self.cfg = params.cfg\n",
    "        \n",
    "        # Загружаем engine (сохраняем как атрибут)\n",
    "        self.engine = self.load_engine(\"MCITrack.trt\")\n",
    "        if not self.engine:\n",
    "            raise RuntimeError(\"Не удалось загрузить TensorRT engine.\")\n",
    "        \n",
    "        # Сохраняем context\n",
    "        self.context = self.engine.create_execution_context()\n",
    "        \n",
    "        # Сохраняем выделенную память\n",
    "        self.initialize_memory()\n",
    "        \n",
    "        self.num_template = self.cfg[\"TEST\"][\"NUM_TEMPLATES\"]\n",
    "\n",
    "        self.preprocessor = Preprocessor()\n",
    "        self.state = None\n",
    "        self.frame_id = 0\n",
    "        self.save_all_boxes = params.save_all_boxes\n",
    "        self.z_dict1 = {}\n",
    "\n",
    "    def load_engine(self, engine_path):\n",
    "        with open(engine_path, \"rb\") as engine_file, trt.Runtime(TRT_LOGGER) as runtime:\n",
    "            engine = runtime.deserialize_cuda_engine(engine_file.read())\n",
    "        return engine\n",
    "\n",
    "    def initialize_memory(self):\n",
    "        # Входы\n",
    "        self.d_template_list = cuda.mem_alloc(int(np.prod([1, 3, 112, 112]) * np.dtype(np.float32).itemsize))\n",
    "        self.d_search_list = cuda.mem_alloc(int(np.prod([1, 3, 112, 112]) * np.dtype(np.float32).itemsize))\n",
    "        self.d_template_anno_list = cuda.mem_alloc(int(np.prod([1, 3, 112, 112]) * np.dtype(np.float32).itemsize))        \n",
    "        self.d_unsqueeze_3 = cuda.mem_alloc(int(np.prod([1, 3, 112, 112]) * np.dtype(np.float32).itemsize))\n",
    "        self.d_unsqueeze_4 = cuda.mem_alloc(int(np.prod([1, 3, 112, 112]) * np.dtype(np.float32).itemsize))\n",
    "        self.d_unsqueeze_5 = cuda.mem_alloc(int(np.prod([1, 3, 224, 224]) * np.dtype(np.float32).itemsize))\n",
    "        self.d_unsqueeze_6 = cuda.mem_alloc(int(np.prod([1, 4]) * np.dtype(np.float32).itemsize))\n",
    "        self.d_unsqueeze_7 = cuda.mem_alloc(int(np.prod([1, 4]) * np.dtype(np.float32).itemsize))\n",
    "        self.d_unsqueeze_8 = cuda.mem_alloc(int(np.prod([1, 4]) * np.dtype(np.float32).itemsize))\n",
    "        self.d_unsqueeze_9 = cuda.mem_alloc(int(np.prod([1, 4]) * np.dtype(np.float32).itemsize))\n",
    "        self.d_unsqueeze_10 = cuda.mem_alloc(int(np.prod([1, 4]) * np.dtype(np.float32).itemsize))\n",
    "        # Выходы\n",
    "        self.d_pred_boxes = cuda.mem_alloc(int(np.prod([1, 1, 4]) * np.dtype(np.float32).itemsize))\n",
    "        self.d_score_map = cuda.mem_alloc(int(np.prod([1, 1, 14, 14]) * np.dtype(np.float32).itemsize))\n",
    "        self.d_size_map = cuda.mem_alloc(int(np.prod([1, 2, 14, 14]) * np.dtype(np.float32).itemsize))\n",
    "        self.d_offset_map = cuda.mem_alloc(int(np.prod([1, 2, 14, 14]) * np.dtype(np.float32).itemsize))        \n",
    "        \n",
    "            \n",
    "    # def predict(\n",
    "    #     self,\n",
    "    #     context,\n",
    "    #     template_list,\n",
    "    #     search_list,\n",
    "    #     template_anno_list,\n",
    "    #     unsqueeze_3,\n",
    "    #     unsqueeze_4,\n",
    "    #     unsqueeze_5,\n",
    "    #     unsqueeze_6,\n",
    "    #     unsqueeze_7,\n",
    "    #     unsqueeze_8,\n",
    "    #     unsqueeze_9,\n",
    "    #     unsqueeze_10        \n",
    "    # ):\n",
    "    def predict(self, template_list, search_list, template_anno_list):\n",
    "                \n",
    "        # Копируем данные на GPU (используем выделенные буферы)\n",
    "        cuda.memcpy_htod(self.d_template_list, template_list.ravel())\n",
    "        cuda.memcpy_htod(self.d_search_list, search_list.ravel())\n",
    "        cuda.memcpy_htod(self.d_template_anno_list, template_anno_list.ravel())\n",
    "        cuda.memcpy_htod(self.d_unsqueeze_3, unsqueeze_3.ravel())\n",
    "        cuda.memcpy_htod(self.d_unsqueeze_4, unsqueeze_4.ravel())\n",
    "        cuda.memcpy_htod(self.d_unsqueeze_5, unsqueeze_5.ravel())\n",
    "        cuda.memcpy_htod(self.d_unsqueeze_6, unsqueeze_6.ravel())\n",
    "        cuda.memcpy_htod(self.d_unsqueeze_7, unsqueeze_7.ravel())\n",
    "        cuda.memcpy_htod(self.d_unsqueeze_8, unsqueeze_8.ravel())\n",
    "        cuda.memcpy_htod(self.d_unsqueeze_9, unsqueeze_9.ravel())\n",
    "        cuda.memcpy_htod(self.d_unsqueeze_10, unsqueeze_10.ravel())\n",
    "        \n",
    "        # Указываем правильные bindings\n",
    "        self.context.execute_v2(bindings=[\n",
    "            int(self.d_template_list),     # 0\n",
    "            int(self.d_search_list),       # 1\n",
    "            int(self.d_template_anno_list),# 2\n",
    "            int(self.d_unsqueeze_3),       # 3\n",
    "            int(self.d_unsqueeze_4),       # 4\n",
    "            int(self.d_unsqueeze_5),       # 5\n",
    "            int(self.d_unsqueeze_6),       # 6\n",
    "            int(self.d_unsqueeze_7),       # 7\n",
    "            int(self.d_unsqueeze_8),       # 8\n",
    "            int(self.d_unsqueeze_9),       # 9\n",
    "            int(self.d_unsqueeze_10),      # 10\n",
    "            int(self.d_pred_boxes),       # 11\n",
    "            int(self.d_score_map),        # 12\n",
    "            int(self.d_size_map),         # 13\n",
    "            int(self.d_offset_map)        # 14\n",
    "        ])\n",
    "        \n",
    "        # Копируем результат с GPU\n",
    "        pred_boxes = np.empty([1, 1, 4], dtype=np.float32)\n",
    "        cuda.memcpy_dtoh(pred_boxes, self.d_pred_boxes)\n",
    "\n",
    "        score_map = np.empty([1, 1, 14, 14], dtype=np.float32)\n",
    "        cuda.memcpy_dtoh(score_map, self.d_score_map)\n",
    "\n",
    "        size_map = np.empty([1, 2, 14, 14], dtype=np.float32)\n",
    "        cuda.memcpy_dtoh(size_map, self.d_size_map)\n",
    "\n",
    "        offset_map = np.empty([1, 2, 14, 14], dtype=np.float32)\n",
    "        cuda.memcpy_dtoh(offset_map, self.d_offset_map)\n",
    "\n",
    "        return pred_boxes, score_map, size_map, offset_map\n",
    "        \n",
    "    def initialize(self, image, info: dict):\n",
    "        z_patch_arr, resize_factor = sample_target(image, info['init_bbox'], \n",
    "                                self.params.template_factor,\n",
    "                                output_sz=self.params.template_size)\n",
    "        \n",
    "        # Шаблон (хранится как тензор на CPU)\n",
    "        self.template = self.preprocessor.process(z_patch_arr).cpu()\n",
    "        self.template_list = [self.template] * self.num_template  # Список тензоров\n",
    "        \n",
    "        # Аннотации (хранятся как тензоры на CPU)\n",
    "        prev_box_crop = transform_image_to_crop(torch.tensor(info['init_bbox']),\n",
    "                                            torch.tensor(info['init_bbox']),\n",
    "                                            resize_factor,\n",
    "                                            torch.Tensor([self.params.template_size, \n",
    "                                                        self.params.template_size]),\n",
    "                                            normalize=True).cpu()\n",
    "        self.template_anno_list = [prev_box_crop.unsqueeze(0)] * self.num_template\n",
    "        \n",
    "        self.state = info['init_bbox']\n",
    "        self.frame_id = 0\n",
    "\n",
    "\n",
    "    def track(self, image, info: dict = None):      \n",
    "        H, W, _ = image.shape\n",
    "        self.frame_id += 1\n",
    "\n",
    "        x_patch_arr, resize_factor = sample_target(image, self.state, \n",
    "                                                self.params.search_factor,\n",
    "                                                output_sz=self.params.search_size)\n",
    "        \n",
    "        # Обрабатываем поисковое изображение и перемещаем на CPU\n",
    "        search = self.preprocessor.process(x_patch_arr).cpu()\n",
    "        search_list = [search]\n",
    "\n",
    "        # Преобразуем тензоры в numpy массивы\n",
    "        template_list_np = torch.stack(self.template_list, dim=0).numpy()\n",
    "        search_list_np = torch.stack(search_list, dim=0).numpy()\n",
    "        template_anno_list_np = torch.stack(self.template_anno_list, dim=0).numpy()\n",
    "\n",
    "        # outputs =self.predictV2(self.context,search.cpu().numpy().astype(np.float32), self.template.cpu().numpy().astype(np.float32),self.d_input_z, self.d_input_x, self.d_output_cls)\n",
    "\n",
    "        outputs = self.predict(self, self.context, template_list_np, search_list_np, template_anno_list_np)\n",
    "        \n",
    "        pred_boxes, score_map, size_map, offset_map = outputs\n",
    "        out_dict = {\n",
    "            'pred_boxes': pred_boxes,\n",
    "            'score_map': score_map,\n",
    "            'size_map': size_map,\n",
    "            'offset_map': offset_map\n",
    "        }\n",
    "\n",
    "        # add hann windows\n",
    "        pred_score_map = out_dict['score_map']\n",
    "        if self.cfg[\"TEST\"][\"WINDOW\"] == True:  # for window penalty\n",
    "            response = self.output_window * pred_score_map\n",
    "        else:\n",
    "            response = pred_score_map\n",
    "\n",
    "        pred_boxes, conf_score = cal_bbox(response, out_dict['size_map'],\n",
    "                                                                   out_dict['offset_map'])\n",
    "\n",
    "        #pred_boxes = torch.from_numpy(outputs).view(-1, 4)\n",
    "        pred_boxes = pred_boxes.view(-1, 4)\n",
    "        pred_box = (pred_boxes.mean(dim=0) * self.params.search_size / resize_factor).tolist()  # (cx, cy, w, h) [0,1]   \n",
    "        # get the final box result     \n",
    "        self.state = clip_box(self.map_box_back(pred_box, resize_factor), H, W, margin=10)\n",
    "\n",
    "        return {\"target_bbox\": self.state,\n",
    "                \"best_score\": conf_score}\n",
    "    \n",
    "    def map_box_back(self, pred_box: list, resize_factor: float):\n",
    "        cx_prev, cy_prev = self.state[0] + 0.5 * self.state[2], self.state[1] + 0.5 * self.state[3]\n",
    "        cx, cy, w, h = pred_box\n",
    "        half_side = 0.5 * self.params.search_size / resize_factor\n",
    "        cx_real = cx + (cx_prev - half_side)\n",
    "        cy_real = cy + (cy_prev - half_side)\n",
    "        return [cx_real - 0.5 * w, cy_real - 0.5 * h, w, h]\n",
    "\n",
    "    def map_box_back_batch(self, pred_box: torch.Tensor, resize_factor: float):\n",
    "        cx_prev, cy_prev = self.state[0] + 0.5 * self.state[2], self.state[1] + 0.5 * self.state[3]\n",
    "        cx, cy, w, h = pred_box.unbind(-1)\n",
    "        half_side = 0.5 * self.params.search_size / resize_factor\n",
    "        cx_real = cx + (cx_prev - half_side)\n",
    "        cy_real = cy + (cy_prev - half_side)\n",
    "        return torch.stack([cx_real - 0.5 * w, cy_real - 0.5 * h, w, h], dim=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pycuda.driver as cuda\n",
    "import pycuda.autoinit\n",
    "import tensorrt as trt\n",
    "from onnx import numpy_helper\n",
    "import time\n",
    "\n",
    "TRT_LOGGER = trt.Logger(trt.Logger.WARNING)\n",
    "\n",
    "class MCITRACK(BaseTracker):\n",
    "    def __init__(self, params):\n",
    "        super(MCITRACK, self).__init__(params)\n",
    "        self.cfg = params.cfg\n",
    "        \n",
    "        # Сначала инициализируем все параметры\n",
    "        self.num_template = self.cfg[\"TEST\"][\"NUM_TEMPLATES\"]\n",
    "        self.preprocessor = Preprocessor()\n",
    "        self.state = None\n",
    "        self.frame_id = 0\n",
    "        \n",
    "        # Параметры обновления\n",
    "        self.h_state = [None] * self.cfg[\"MODEL\"][\"NECK\"][\"N_LAYERS\"]\n",
    "        self.memory_bank = self.cfg[\"TEST\"][\"MB\"][\"DEFAULT\"]\n",
    "        self.update_h_t = self.cfg[\"TEST\"][\"UPH\"][\"DEFAULT\"]\n",
    "        self.update_threshold = self.cfg[\"TEST\"][\"UPT\"][\"DEFAULT\"]\n",
    "        self.update_intervals = self.cfg[\"TEST\"][\"INTER\"][\"DEFAULT\"]\n",
    "        \n",
    "        # Инициализация окна Ханна\n",
    "        self.fx_sz = self.cfg[\"TEST\"][\"SEARCH_SIZE\"] // self.cfg[\"MODEL\"][\"ENCODER\"][\"STRIDE\"]\n",
    "        if self.cfg[\"TEST\"][\"WINDOW\"]:\n",
    "            self.output_window = hann2d(torch.tensor([self.fx_sz, self.fx_sz]).long(), centered=True).cuda()\n",
    "        \n",
    "        # Затем инициализация TensorRT\n",
    "        self.engine = self._load_engine(\"MCITrack.trt\")\n",
    "        self.context = self.engine.create_execution_context()\n",
    "        \n",
    "        # Определение размеров входов/выходов\n",
    "        self._setup_io_shapes()\n",
    "        \n",
    "        # Выделение памяти\n",
    "        self.input_buffers = {}\n",
    "        self.output_buffers = {}\n",
    "        self._allocate_buffers()\n",
    "\n",
    "    def _setup_io_shapes(self):\n",
    "        \"\"\"Определение размеров входов и выходов\"\"\"\n",
    "        self.input_shapes = {\n",
    "            'template_list': (self.num_template, 3, 112, 112),\n",
    "            'search_list': (1, 3, 112, 112),\n",
    "            'template_anno_list': (self.num_template, 4),\n",
    "            'onnx::Unsqueeze_3': (1, 3, 112, 112),\n",
    "            'onnx::Unsqueeze_4': (1, 3, 112, 112),\n",
    "            'onnx::Unsqueeze_5': (1, 3, 224, 224),\n",
    "            'onnx::Unsqueeze_6': (1, 4),\n",
    "            'onnx::Unsqueeze_7': (1, 4),\n",
    "            'onnx::Unsqueeze_8': (1, 4),\n",
    "            'onnx::Unsqueeze_9': (1, 4),\n",
    "            'onnx::Unsqueeze_10': (1, 4)\n",
    "        }\n",
    "        \n",
    "        self.output_shapes = {\n",
    "            'pred_boxes': (1, 1, 4),\n",
    "            'score_map': (1, 1, 14, 14),\n",
    "            'size_map': (1, 2, 14, 14),\n",
    "            'offset_map': (1, 2, 14, 14)\n",
    "        }\n",
    "\n",
    "    def _load_engine(self, engine_path):\n",
    "        \"\"\"Загрузка TensorRT engine\"\"\"\n",
    "        with open(engine_path, \"rb\") as f, trt.Runtime(TRT_LOGGER) as runtime:\n",
    "            return runtime.deserialize_cuda_engine(f.read())\n",
    "\n",
    "    \n",
    "    def _allocate_buffers(self):\n",
    "        \"\"\"Выделение памяти для буферов\"\"\"\n",
    "        for name, shape in self.input_shapes.items():\n",
    "            size = int(np.prod(shape)) * 4  # 4 bytes per float32\n",
    "            self.input_buffers[name] = cuda.mem_alloc(size)\n",
    "        \n",
    "        for name, shape in self.output_shapes.items():\n",
    "            size = int(np.prod(shape)) * 4\n",
    "            self.output_buffers[name] = cuda.mem_alloc(size)\n",
    "\n",
    "    def initialize(self, image, info: dict):\n",
    "        \"\"\"Инициализация трекера с первым кадром\"\"\"\n",
    "        # Подготовка шаблона\n",
    "        z_patch_arr, resize_factor = sample_target(\n",
    "            image, info['init_bbox'], \n",
    "            self.params.template_factor,\n",
    "            output_sz=self.params.template_size\n",
    "        )\n",
    "        \n",
    "        self.template = self.preprocessor.process(z_patch_arr)\n",
    "        self.template_list = [self.template] * self.num_template\n",
    "        \n",
    "        # Сохранение состояния\n",
    "        self.state = info['init_bbox']\n",
    "        prev_box_crop = transform_image_to_crop(\n",
    "            torch.tensor(info['init_bbox']),\n",
    "            torch.tensor(info['init_bbox']),\n",
    "            resize_factor,\n",
    "            torch.Tensor([self.params.template_size, self.params.template_size]),\n",
    "            normalize=True\n",
    "        )\n",
    "        \n",
    "        self.template_anno_list = [prev_box_crop.to(self.template.device).unsqueeze(0)] * self.num_template\n",
    "        self.frame_id = 0\n",
    "        self.memory_template_list = self.template_list.copy()\n",
    "        self.memory_template_anno_list = self.template_anno_list.copy()\n",
    "\n",
    "    def track(self, image, info: dict = None):\n",
    "        \"\"\"Обработка нового кадра\"\"\"\n",
    "        H, W, _ = image.shape\n",
    "        self.frame_id += 1\n",
    "        \n",
    "        # 1. Подготовка поисковой области\n",
    "        x_patch_arr, resize_factor = sample_target(\n",
    "            image, self.state, \n",
    "            self.params.search_factor,\n",
    "            output_sz=112  # Фиксированный размер для модели\n",
    "        )\n",
    "        \n",
    "        # 2. Предобработка\n",
    "        search_tensor = self.preprocessor.process(x_patch_arr)\n",
    "        \n",
    "        \n",
    "        # 3. Подготовка входных данных\n",
    "        template_np = torch.stack(self.template_list).squeeze(1).cpu().numpy().astype(np.float32)\n",
    "        \n",
    "        # Ресайз поискового изображения\n",
    "        if search_tensor.shape[-2:] != (112, 112):\n",
    "            search_tensor = F.interpolate(search_tensor, size=(112, 112), mode='bilinear', align_corners=False)\n",
    "        search_np = search_tensor.cpu().numpy().astype(np.float32)\n",
    "        \n",
    "        template_anno_np = torch.stack(self.template_anno_list).squeeze(1).cpu().numpy().astype(np.float32)\n",
    "        \n",
    "        # 4. Проверка размеров\n",
    "        self._check_input_shapes({\n",
    "            'template_list': template_np,\n",
    "            'search_list': search_np,\n",
    "            'template_anno_list': template_anno_np\n",
    "        })\n",
    "        \n",
    "        # 5. Подготовка константных тензоров\n",
    "        constant_inputs = {\n",
    "            'onnx::Unsqueeze_3': np.ones((1, 3, 112, 112), dtype=np.float32),\n",
    "            'onnx::Unsqueeze_4': np.zeros((1, 3, 112, 112), dtype=np.float32),\n",
    "            'onnx::Unsqueeze_5': np.random.randn(1, 3, 224, 224).astype(np.float32),\n",
    "            'onnx::Unsqueeze_6': np.array([[1.0, 1.0, 1.0, 1.0]], dtype=np.float32),\n",
    "            'onnx::Unsqueeze_7': np.array([[0.5, 0.5, 0.5, 0.5]], dtype=np.float32),\n",
    "            'onnx::Unsqueeze_8': np.array([[0.1, 0.1, 0.1, 0.1]], dtype=np.float32),\n",
    "            'onnx::Unsqueeze_9': np.array([[0.0, 0.0, 0.0, 0.0]], dtype=np.float32),\n",
    "            'onnx::Unsqueeze_10': np.array([[1.0, 1.0, 1.0, 1.0]], dtype=np.float32)\n",
    "        }\n",
    "        \n",
    "        # 6. Копирование данных в GPU\n",
    "        try:\n",
    "            # Основные входы\n",
    "            self._copy_to_gpu('template_list', template_np)\n",
    "            self._copy_to_gpu('search_list', search_np)\n",
    "            self._copy_to_gpu('template_anno_list', template_anno_np)\n",
    "            \n",
    "            # Константные входы\n",
    "            for name, data in constant_inputs.items():\n",
    "                self._copy_to_gpu(name, data)\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Ошибка копирования на GPU: {str(e)}\")\n",
    "        \n",
    "        # 7. Выполнение inference\n",
    "        bindings = []\n",
    "        \n",
    "        # Сначала входные буферы\n",
    "        for i in range(self.engine.num_bindings):\n",
    "            if self.engine.binding_is_input(i):\n",
    "                name = self.engine.get_binding_name(i)\n",
    "                bindings.append(int(self.input_buffers[name]))\n",
    "        \n",
    "        # Затем выходные буферы\n",
    "        for i in range(self.engine.num_bindings):\n",
    "            if not self.engine.binding_is_input(i):\n",
    "                name = self.engine.get_binding_name(i)\n",
    "                bindings.append(int(self.output_buffers[name]))\n",
    "        \n",
    "        self.context.execute_v2(bindings=bindings)\n",
    "        \n",
    "        # 8. Получение результатов\n",
    "        outputs = {\n",
    "            'pred_boxes': self._copy_from_gpu('pred_boxes'),\n",
    "            'score_map': self._copy_from_gpu('score_map'),\n",
    "            'size_map': self._copy_from_gpu('size_map'),\n",
    "            'offset_map': self._copy_from_gpu('offset_map')\n",
    "        }\n",
    "        \n",
    "        # 9. Постобработка\n",
    "        pred_score_map = torch.from_numpy(outputs['score_map']).cuda()\n",
    "        \n",
    "        if self.cfg[\"TEST\"][\"WINDOW\"]:\n",
    "            response = self.output_window * pred_score_map\n",
    "        else:\n",
    "            response = pred_score_map\n",
    "            \n",
    "        pred_boxes, conf_score = cal_bbox(\n",
    "            response,\n",
    "            torch.from_numpy(outputs['size_map']).cuda(),\n",
    "            torch.from_numpy(outputs['offset_map']).cuda()\n",
    "        )\n",
    "        \n",
    "        pred_boxes = pred_boxes.view(-1, 4)\n",
    "        pred_box = (pred_boxes.mean(dim=0) * 112 / resize_factor).tolist()  # 112 - размер поисковой области\n",
    "        \n",
    "        # 10. Обновление состояния\n",
    "        self.state = clip_box(self.map_box_back(pred_box, resize_factor), H, W, margin=10)\n",
    "        \n",
    "        # 11. Обновление шаблонов\n",
    "        self._update_templates(image, conf_score)\n",
    "        \n",
    "        return {\"target_bbox\": self.state, \"best_score\": conf_score}\n",
    "\n",
    "    def _check_input_shapes(self, inputs):\n",
    "        \"\"\"Проверка размеров входных данных\"\"\"\n",
    "        for name, arr in inputs.items():\n",
    "            if arr.shape != self.input_shapes[name]:\n",
    "                raise ValueError(\n",
    "                    f\"Неверная форма для {name}: ожидается {self.input_shapes[name]}, получено {arr.shape}\"\n",
    "                )\n",
    "\n",
    "    def _copy_to_gpu(self, name, data):\n",
    "        \"\"\"Безопасное копирование данных в GPU\"\"\"\n",
    "        expected_size = int(np.prod(self.input_shapes[name])) * 4\n",
    "        actual_size = data.size * data.itemsize\n",
    "        \n",
    "        if actual_size != expected_size:\n",
    "            raise ValueError(\n",
    "                f\"Несоответствие размеров для {name}: \"\n",
    "                f\"ожидается {expected_size} байт, получено {actual_size} байт\"\n",
    "            )\n",
    "        \n",
    "        cuda.memcpy_htod(self.input_buffers[name], data.ravel())\n",
    "\n",
    "    def _copy_from_gpu(self, name):\n",
    "        \"\"\"Копирование данных с GPU\"\"\"\n",
    "        arr = np.empty(self.output_shapes[name], dtype=np.float32)\n",
    "        cuda.memcpy_dtoh(arr, self.output_buffers[name])\n",
    "        return arr\n",
    "\n",
    "    def _update_templates(self, image, conf_score):\n",
    "        \"\"\"Обновление шаблонов\"\"\"\n",
    "        if self.num_template > 1 and conf_score > self.update_threshold:\n",
    "            z_patch_arr, resize_factor = sample_target(\n",
    "                image, self.state,\n",
    "                self.params.template_factor,\n",
    "                output_sz=self.params.template_size\n",
    "            )\n",
    "            template = self.preprocessor.process(z_patch_arr)\n",
    "            self.memory_template_list.append(template)\n",
    "            \n",
    "            prev_box_crop = transform_image_to_crop(\n",
    "                torch.tensor(self.state),\n",
    "                torch.tensor(self.state),\n",
    "                resize_factor,\n",
    "                torch.Tensor([self.params.template_size, self.params.template_size]),\n",
    "                normalize=True\n",
    "            )\n",
    "            self.memory_template_anno_list.append(prev_box_crop.to(template.device).unsqueeze(0))\n",
    "            \n",
    "            if len(self.memory_template_list) > self.memory_bank:\n",
    "                self.memory_template_list.pop(0)\n",
    "                self.memory_template_anno_list.pop(0)\n",
    "        \n",
    "        if self.frame_id % self.update_intervals == 0:\n",
    "            self._rotate_templates()\n",
    "\n",
    "    def _rotate_templates(self):\n",
    "        \"\"\"Ротация шаблонов в memory bank\"\"\"\n",
    "        assert len(self.memory_template_anno_list) == len(self.memory_template_list)\n",
    "        len_list = len(self.memory_template_anno_list)\n",
    "        interval = max(1, len_list // self.num_template)\n",
    "        \n",
    "        for i in range(1, self.num_template):\n",
    "            idx = min(interval * i, len_list - 1)\n",
    "            \n",
    "            self.template_list.append(self.memory_template_list[idx])\n",
    "            self.template_list.pop(1)\n",
    "            self.template_anno_list.append(self.memory_template_anno_list[idx])\n",
    "            self.template_anno_list.pop(1)\n",
    "        \n",
    "        assert len(self.template_list) == self.num_template\n",
    "\n",
    "    def map_box_back(self, pred_box: list, resize_factor: float):\n",
    "        \"\"\"Преобразование координат обратно в пространство изображения\"\"\"\n",
    "        cx_prev, cy_prev = self.state[0] + 0.5 * self.state[2], self.state[1] + 0.5 * self.state[3]\n",
    "        cx, cy, w, h = pred_box\n",
    "        half_side = 0.5 * 112 / resize_factor  # 112 - размер поисковой области\n",
    "        cx_real = cx + (cx_prev - half_side)\n",
    "        cy_real = cy + (cy_prev - half_side)\n",
    "        return [cx_real - 0.5 * w, cy_real - 0.5 * h, w, h]\n",
    "\n",
    "    def __del__(self):\n",
    "        \"\"\"Освобождение ресурсов\"\"\"\n",
    "        for buf in list(self.input_buffers.values()) + list(self.output_buffers.values()):\n",
    "            if buf:\n",
    "                buf.free()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "treacker = MCITRACK(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Неверная форма для search_list: ожидается (1, 3, 112, 112), получено (1, 1, 3, 112, 112)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 27\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Трекинг\u001b[39;00m\n\u001b[0;32m     26\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m---> 27\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mtreacker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtracker_image\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m state \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mint\u001b[39m(s) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m out[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget_bbox\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m#best_score = out[\"best_score\"]\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[11], line 140\u001b[0m, in \u001b[0;36mMCITRACK.track\u001b[1;34m(self, image, info)\u001b[0m\n\u001b[0;32m    137\u001b[0m template_anno_np \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtemplate_anno_list)\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m    139\u001b[0m \u001b[38;5;66;03m# 4. Проверка размеров\u001b[39;00m\n\u001b[1;32m--> 140\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_input_shapes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtemplate_list\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemplate_np\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    142\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msearch_list\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msearch_np\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    143\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtemplate_anno_list\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemplate_anno_np\u001b[49m\n\u001b[0;32m    144\u001b[0m \u001b[43m\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    146\u001b[0m \u001b[38;5;66;03m# 5. Подготовка константных тензоров\u001b[39;00m\n\u001b[0;32m    147\u001b[0m constant_inputs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    148\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124monnx::Unsqueeze_3\u001b[39m\u001b[38;5;124m'\u001b[39m: np\u001b[38;5;241m.\u001b[39mones((\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m112\u001b[39m, \u001b[38;5;241m112\u001b[39m), dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32),\n\u001b[0;32m    149\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124monnx::Unsqueeze_4\u001b[39m\u001b[38;5;124m'\u001b[39m: np\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m112\u001b[39m, \u001b[38;5;241m112\u001b[39m), dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    155\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124monnx::Unsqueeze_10\u001b[39m\u001b[38;5;124m'\u001b[39m: np\u001b[38;5;241m.\u001b[39marray([[\u001b[38;5;241m1.0\u001b[39m, \u001b[38;5;241m1.0\u001b[39m, \u001b[38;5;241m1.0\u001b[39m, \u001b[38;5;241m1.0\u001b[39m]], dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m    156\u001b[0m }\n",
      "Cell \u001b[1;32mIn[11], line 225\u001b[0m, in \u001b[0;36mMCITRACK._check_input_shapes\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, arr \u001b[38;5;129;01min\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m    224\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_shapes[name]:\n\u001b[1;32m--> 225\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    226\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mНеверная форма для \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: ожидается \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_shapes[name]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, получено \u001b[39m\u001b[38;5;132;01m{\u001b[39;00marr\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    227\u001b[0m         )\n",
      "\u001b[1;31mValueError\u001b[0m: Неверная форма для search_list: ожидается (1, 3, 112, 112), получено (1, 1, 3, 112, 112)"
     ]
    }
   ],
   "source": [
    "# Трекинг по видео\n",
    "file = \"0516.mp4\"\n",
    "video = cv2.VideoCapture(file)\n",
    "ok, image = video.read()\n",
    "if not video.isOpened():\n",
    "    print(\"Could not open video\")\n",
    "    sys.exit()\n",
    "    \n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "x, y, w, h = cv2.selectROI( image, fromCenter=False)\n",
    "init_state = [x, y, w, h]\n",
    "def _build_init_info(box):\n",
    "            return {'init_bbox': box}\n",
    "treacker.initialize(image, _build_init_info(init_state))\n",
    "counter = 0\n",
    "while True:\n",
    "    ok, image = video.read()\n",
    "    if not ok:\n",
    "        break\n",
    "\n",
    "    # Конвертация для трекера\n",
    "    tracker_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Трекинг\n",
    "    start_time = time.time()\n",
    "    out = treacker.track(tracker_image)\n",
    "    state = [int(s) for s in out['target_bbox']]\n",
    "    #best_score = out[\"best_score\"]\n",
    "    best_score = 1\n",
    "    fps = 1 / (time.time() - start_time + 1e-6)\n",
    "\n",
    "    # Визуализация\n",
    "    display_image = image.copy()\n",
    "    x, y, w, h = state\n",
    "    \n",
    "    # Динамический цвет рамки в зависимости от уверенности\n",
    "    color = (0, 255, 0) if best_score > 0.7 else (0, 255, 255) if best_score > 0.4 else (0, 0, 255)\n",
    "    thickness = 3 if best_score > 0.7 else 2\n",
    "    \n",
    "    # Рисуем bounding box с увеличенными размерами\n",
    "    cv2.rectangle(display_image, (x, y), (x + w, y + h), color, thickness)\n",
    "    \n",
    "    # Добавляем информационный текст\n",
    "    cv2.putText(display_image, f\"Score: {best_score:.2f}\", (x, y-10), \n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)\n",
    "    cv2.putText(display_image, f\"FPS: {fps:.1f}\", (20, 40), \n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)\n",
    "    \n",
    "    cv2.imshow(\"tracking\", display_image)\n",
    "    \n",
    "    # Обработка клавиш\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    if key == 32:  # SPACE - переинициализация\n",
    "        x, y, w, h = cv2.selectROI(\"Select ROI\", image, fromCenter=False)\n",
    "        if w > 10 and h > 10:  # Минимальный размер ROI\n",
    "            init_state = [x, y, w, h]\n",
    "            print(\"Переинициализация...\")\n",
    "            treacker.initialize(tracker_image, _build_init_info(init_state))\n",
    "    elif key == 27:  # ESC - выход\n",
    "        break\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
