{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33c9c5c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\python3_10\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import sys\n",
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "import yaml\n",
    "import pycuda.autoinit\n",
    "import onnx\n",
    "import onnxruntime as ort\n",
    "import tensorrt as trt\n",
    "import pycuda.driver as cuda\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235284a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_target(im, target_bb, search_area_factor, output_sz=None):\n",
    "   \n",
    "    if not isinstance(target_bb, list):\n",
    "        x, y, w, h = target_bb.tolist()\n",
    "    else:\n",
    "        x, y, w, h = target_bb\n",
    "    # Crop image\n",
    "    crop_sz = math.ceil(math.sqrt(w * h) * search_area_factor)\n",
    "\n",
    "    if crop_sz < 1:\n",
    "        raise Exception('Too small bounding box.')\n",
    "\n",
    "    x1 = round(x + 0.5 * w - crop_sz * 0.5)\n",
    "    x2 = x1 + crop_sz\n",
    "\n",
    "    y1 = round(y + 0.5 * h - crop_sz * 0.5)\n",
    "    y2 = y1 + crop_sz\n",
    "\n",
    "    x1_pad = max(0, -x1)\n",
    "    x2_pad = max(x2 - im.shape[1] + 1, 0)\n",
    "\n",
    "    y1_pad = max(0, -y1)\n",
    "    y2_pad = max(y2 - im.shape[0] + 1, 0)\n",
    "\n",
    "    # Crop target\n",
    "    im_crop = im[y1 + y1_pad:y2 - y2_pad, x1 + x1_pad:x2 - x2_pad, :]\n",
    "\n",
    "    # Pad\n",
    "    im_crop_padded = cv2.copyMakeBorder(im_crop, y1_pad, y2_pad, x1_pad, x2_pad, cv2.BORDER_CONSTANT)\n",
    "    # deal with attention mask\n",
    "    H, W, _ = im_crop_padded.shape\n",
    "\n",
    "    if output_sz is not None:\n",
    "        resize_factor = output_sz / crop_sz\n",
    "        im_crop_padded = cv2.resize(im_crop_padded, (output_sz, output_sz))\n",
    "\n",
    "        return im_crop_padded, resize_factor\n",
    "\n",
    "    else:\n",
    "        return im_crop_padded, 1.0\n",
    "    \n",
    "    \n",
    "def hann1d(sz: int, centered = True) -> torch.Tensor:\n",
    "    \"\"\"1D cosine window.\"\"\"\n",
    "    if centered:\n",
    "        return 0.5 * (1 - torch.cos((2 * math.pi / (sz + 1)) * torch.arange(1, sz + 1).float()))\n",
    "    w = 0.5 * (1 + torch.cos((2 * math.pi / (sz + 2)) * torch.arange(0, sz//2 + 1).float()))\n",
    "    return torch.cat([w, w[1:sz-sz//2].flip((0,))])\n",
    "    \n",
    "def hann2d(sz: torch.Tensor, centered = True) -> torch.Tensor:\n",
    "    \"\"\"2D cosine window.\"\"\"\n",
    "    return hann1d(sz[0].item(), centered).reshape(1, 1, -1, 1) * hann1d(sz[1].item(), centered).reshape(1, 1, 1, -1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c75b61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_image_to_crop(box_in: torch.Tensor, box_extract: torch.Tensor, resize_factor: float,\n",
    "                            crop_sz: torch.Tensor, normalize=False) -> torch.Tensor:\n",
    "    \"\"\" Transform the box co-ordinates from the original image co-ordinates to the co-ordinates of the cropped image\n",
    "    args:\n",
    "        box_in - the box for which the co-ordinates are to be transformed\n",
    "        box_extract - the box about which the image crop has been extracted.\n",
    "        resize_factor - the ratio between the original image scale and the scale of the image crop\n",
    "        crop_sz - size of the cropped image\n",
    "\n",
    "    returns:\n",
    "        torch.Tensor - transformed co-ordinates of box_in\n",
    "    \"\"\"\n",
    "    box_extract_center = box_extract[0:2] + 0.5 * box_extract[2:4]\n",
    "\n",
    "    box_in_center = box_in[0:2] + 0.5 * box_in[2:4]\n",
    "\n",
    "    box_out_center = (crop_sz - 1) / 2 + (box_in_center - box_extract_center) * resize_factor\n",
    "    box_out_wh = box_in[2:4] * resize_factor\n",
    "\n",
    "    box_out = torch.cat((box_out_center - 0.5 * box_out_wh, box_out_wh))\n",
    "    if normalize:\n",
    "        return box_out / (crop_sz[0]-1)\n",
    "    else:\n",
    "        return box_out\n",
    "def clip_box(box: list, H, W, margin=0):\n",
    "    x1, y1, w, h = box\n",
    "    x2, y2 = x1 + w, y1 + h\n",
    "    x1 = min(max(0, x1), W-margin)\n",
    "    x2 = min(max(margin, x2), W)\n",
    "    y1 = min(max(0, y1), H-margin)\n",
    "    y2 = min(max(margin, y2), H)\n",
    "    w = max(margin, x2-x1)\n",
    "    h = max(margin, y2-y1)\n",
    "    return [x1, y1, w, h]\n",
    "\n",
    "def box_xyxy_to_cxcywh(x):\n",
    "    x0, y0, x1, y1 = x.unbind(-1)\n",
    "    b = [(x0 + x1) / 2, (y0 + y1) / 2,\n",
    "         (x1 - x0), (y1 - y0)]\n",
    "    return torch.stack(b, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87d5d4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessor(object):\n",
    "    def __init__(self):\n",
    "        self.mean = torch.tensor([0.485, 0.456, 0.406]).view((1, 3, 1, 1)).cuda()\n",
    "        self.std = torch.tensor([0.229, 0.224, 0.225]).view((1, 3, 1, 1)).cuda()\n",
    "        self.mm_mean = torch.tensor([0.485, 0.456, 0.406, 0.485, 0.456, 0.406]).view((1, 6, 1, 1)).cuda()\n",
    "        self.mm_std = torch.tensor([0.229, 0.224, 0.225, 0.229, 0.224, 0.225]).view((1, 6, 1, 1)).cuda()\n",
    "\n",
    "    def process(self, img_arr: np.ndarray):\n",
    "        if img_arr.shape[-1] == 6:\n",
    "            mean = self.mm_mean\n",
    "            std = self.mm_std\n",
    "        else:\n",
    "            mean = self.mean\n",
    "            std = self.std\n",
    "        # Deal with the image patch\n",
    "        img_tensor = torch.tensor(img_arr).cuda().float().permute((2,0,1)).unsqueeze(dim=0)\n",
    "        # img_tensor = torch.tensor(img_arr).float().permute((2,0,1)).unsqueeze(dim=0)\n",
    "        img_tensor_norm = ((img_tensor / 255.0) - mean) / std  # (1,3,H,W)\n",
    "        return img_tensor_norm\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0eabcfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_bbox(score_map_ctr, size_map, offset_map, return_score=True):\n",
    "        feat_sz = 14\n",
    "        max_score, idx = torch.max(score_map_ctr.flatten(1), dim=1, keepdim=True) # score_map_ctr.flatten(1): torch.Size([32, 256]) idx: torch.Size([32, 1]) max_score: torch.Size([32, 1])\n",
    "        idx_y = torch.div(idx, feat_sz, rounding_mode='floor')\n",
    "        idx_x = idx % feat_sz\n",
    "       \n",
    "        \n",
    "\n",
    "        idx = idx.unsqueeze(1).expand(idx.shape[0], 2, 1)\n",
    "        size = size_map.flatten(2).gather(dim=2, index=idx) # size_map: torch.Size([32, 2, 16, 16])  size_map.flatten(2): torch.Size([32, 2, 256])\n",
    "        offset = offset_map.flatten(2).gather(dim=2, index=idx).squeeze(-1)\n",
    "\n",
    "        # bbox = torch.cat([idx_x - size[:, 0] / 2, idx_y - size[:, 1] / 2,\n",
    "        #                   idx_x + size[:, 0] / 2, idx_y + size[:, 1] / 2], dim=1) / self.feat_sz\n",
    "        # cx, cy, w, h\n",
    "        bbox = torch.cat([(idx_x.to(torch.float) + offset[:, :1]) / feat_sz,\n",
    "                          (idx_y.to(torch.float) + offset[:, 1:]) / feat_sz,\n",
    "                          size.squeeze(-1)], dim=1)\n",
    "\n",
    "        if return_score:\n",
    "            return bbox, max_score\n",
    "        return bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5ea379e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseTracker:\n",
    "    \"\"\"Base class for all trackers.\"\"\"\n",
    "\n",
    "    def __init__(self, params):\n",
    "        self.params = params\n",
    "        self.visdom = None\n",
    "\n",
    "    def predicts_segmentation_mask(self):\n",
    "        return False\n",
    "\n",
    "    def initialize(self, image, info: dict) -> dict:\n",
    "        \"\"\"Overload this function in your tracker. This should initialize the model.\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def track(self, image, info: dict = None) -> dict:\n",
    "        \"\"\"Overload this function in your tracker. This should track in the frame and update the model.\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def visdom_draw_tracking(self, image, box, segmentation=None):\n",
    "        # Упрощенная обработка box без OrderedDict\n",
    "        if isinstance(box, dict):  # Проверяем на обычный dict вместо OrderedDict\n",
    "            box = list(box.values())  # Берем только значения\n",
    "        elif not isinstance(box, (list, tuple)):  # Если не коллекция\n",
    "            box = (box,)  # Превращаем в кортеж\n",
    "        \n",
    "        # Визуализация\n",
    "        if segmentation is None:\n",
    "            self.visdom.register((image, *box), 'Tracking', 1, 'Tracking')\n",
    "        else:\n",
    "            self.visdom.register((image, *box, segmentation), 'Tracking', 1, 'Tracking')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b4b835",
   "metadata": {},
   "outputs": [],
   "source": [
    "#OLD\n",
    "class MCITRACK(BaseTracker):\n",
    "    def __init__(self, params):\n",
    "        \n",
    "        super(MCITRACK, self).__init__(params)\n",
    "        \n",
    "        \"\"\"Загрузка TensorRT модели\"\"\"\n",
    "        with open(\"MCITrac.trt\", \"rb\") as f:\n",
    "            engine_data = f.read()\n",
    "        self.runtime = trt.Runtime(trt.Logger())\n",
    "        self.engine = self.runtime.deserialize_cuda_engine(engine_data)\n",
    "        self.context = self.engine.create_execution_context()\n",
    "        print(f\"Context profile #: {self.context.engine.num_optimization_profiles}\")\n",
    "\n",
    "        # self.ort_session = ort.InferenceSession(\"MCITrack.onnx\", providers=[('TensorrtExecutionProvider', {'trt_engine_cache_enable': True,\n",
    "        #                                'trt_engine_cache_path': './trt',\n",
    "        #                                \"trt_fp16_enable\": True,\n",
    "        #                                'device_id': 0,\n",
    "        #                                }),('CUDAExecutionProvider')])\n",
    "        #self.ort_session = ort.InferenceSession(\"MCITrac.onnx\", providers=['CUDAExecutionProvider', 'CPUExecutionProvider'])\n",
    "        \n",
    "        # self.onnx_input_names = [inp.name for inp in self.ort_session.get_inputs()]\n",
    "        # self.onnx_output_names = [out.name for out in self.ort_session.get_outputs()]\n",
    "\n",
    "          # Получаем имена тензоров                \n",
    "        self.tensor_names = [self.engine.get_tensor_name(i) for i in range(self.engine.num_io_tensors)]\n",
    "        self.input_tensors = self.tensor_names[0:2]  # Исключаем выходные тензоры\n",
    "        self.input_shapes = [self.context.get_tensor_shape(name) for name in self.input_tensors]\n",
    "        self.output_tensors = self.tensor_names[2:]  # Исключаем входные тензоры\n",
    "        self.output_shapes = [self.context.get_tensor_shape(name) for name in self.output_tensors]\n",
    "\n",
    "         # переменные под GPU память\n",
    "        self.d_inputs = {}\n",
    "        self.d_outputs = {}\n",
    "\n",
    "        self.allocate_memory()\n",
    "\n",
    "    def allocate_memory(self):\n",
    "        \"\"\"Выделение правильного объема памяти в GPU\"\"\"\n",
    "\n",
    "        # Выделяем память под входные тензоры\n",
    "        for name, shape in zip(self.input_tensors, self.input_shapes):\n",
    "            size = int(np.prod(shape) * np.dtype(np.float32).itemsize)\n",
    "            self.d_inputs[name] = cuda.mem_alloc(size)\n",
    "\n",
    "        # Выделяем память под выходные тензоры\n",
    "        for name, shape in zip(self.output_tensors, self.output_shapes):\n",
    "            size = int(np.prod(shape) * np.dtype(np.float32).itemsize)\n",
    "            self.d_outputs[name] = cuda.mem_alloc(size)\n",
    "        \n",
    "    def run_inference(self, z_numpy, x_numpy):\n",
    "        \"\"\"Запуск инференса для текущего кадра\"\"\"\n",
    "        \n",
    "        z_numpy = z_numpy.astype(np.float32).copy(order='C')\n",
    "        x_numpy = x_numpy.astype(np.float32).copy(order='C')\n",
    "\n",
    "        cuda.memcpy_htod(self.d_inputs[\"z\"], z_numpy)\n",
    "        cuda.memcpy_htod(self.d_inputs[\"x\"], x_numpy)\n",
    "\n",
    "        # Создаем список всех входных и выходных буферов\n",
    "        bindings = [int(self.d_inputs[\"z\"]), int(self.d_inputs[\"x\"])] + [int(self.d_outputs[name]) for name in self.output_tensors]\n",
    "        \n",
    "        # Запускаем инференс\n",
    "        self.context.execute_v2(bindings)\n",
    "\n",
    "    def copy_results_to_cpu(self):\n",
    "        \"\"\"Извлекаем результаты инференса из GPU\"\"\"\n",
    "        cpu_outputs = {}\n",
    "        i = 0\n",
    "        for name in self.output_tensors:\n",
    "            cpu_outputs[KEYS[i]] = np.empty(self.output_shapes[self.output_tensors.index(name)], dtype=np.float32)\n",
    "            cuda.memcpy_dtoh(cpu_outputs[KEYS[i]], self.d_outputs[name])\n",
    "            i +=1\n",
    "        # print(f\"type(cpu_outputs): {type(cpu_outputs)}\") \n",
    "        return cpu_outputs\n",
    "\n",
    "        self.cfg = params.cfg\n",
    "\n",
    "        self.preprocessor = Preprocessor()\n",
    "        self.state = None\n",
    "\n",
    "        self.fx_sz = self.cfg[\"TEST\"][\"SEARCH_SIZE\"] // self.cfg[\"MODEL\"][\"ENCODER\"][\"STRIDE\"]\n",
    "        if self.cfg[\"TEST\"][\"WINDOW\"] == True:  # for window penalty\n",
    "            self.output_window = hann2d(torch.tensor([self.fx_sz, self.fx_sz]).long(), centered=True).cuda()\n",
    "\n",
    "        self.num_template = self.cfg[\"TEST\"][\"NUM_TEMPLATES\"]\n",
    "\n",
    "   \n",
    "        self.frame_id = 0\n",
    "        # for update\n",
    "        self.h_state = [None] * self.cfg[\"MODEL\"][\"NECK\"][\"N_LAYERS\"]\n",
    "\n",
    "\n",
    "\n",
    "        self.memory_bank = self.cfg[\"TEST\"][\"MB\"][\"DEFAULT\"]\n",
    "        self.update_h_t = self.cfg[\"TEST\"][\"UPH\"][\"DEFAULT\"]\n",
    "        self.update_threshold = self.cfg[\"TEST\"][\"UPT\"][\"DEFAULT\"]\n",
    "        self.update_intervals = self.cfg[\"TEST\"][\"INTER\"][\"DEFAULT\"]\n",
    "        print(\"Update threshold is: \", self.memory_bank)\n",
    "\n",
    "    def initialize(self, image, info: dict):\n",
    "\n",
    "\n",
    "        # get the initial templates\n",
    "        z_patch_arr, resize_factor = sample_target(image, info['init_bbox'], self.params.template_factor,\n",
    "                                                   output_sz=self.params.template_size)\n",
    "        z_patch_arr = z_patch_arr\n",
    "        template = self.preprocessor.process(z_patch_arr)\n",
    "        self.template_list = [template] * self.num_template\n",
    "\n",
    "        self.state = info['init_bbox']\n",
    "        prev_box_crop = transform_image_to_crop(torch.tensor(info['init_bbox']),\n",
    "                                                torch.tensor(info['init_bbox']),\n",
    "                                                resize_factor,\n",
    "                                                torch.Tensor([self.params.template_size, self.params.template_size]),\n",
    "                                                normalize=True)\n",
    "        self.template_anno_list = [prev_box_crop.to(template.device).unsqueeze(0)] * self.num_template\n",
    "        self.frame_id = 0\n",
    "        self.memory_template_list = self.template_list.copy()\n",
    "        self.memory_template_anno_list = self.template_anno_list.copy()\n",
    "\n",
    "\n",
    "    def track(self, image, info: dict = None):\n",
    "        H, W, _ = image.shape\n",
    "        self.frame_id += 1\n",
    "        x_patch_arr, resize_factor = sample_target(image, self.state, self.params.search_factor,\n",
    "                                                   output_sz=self.params.search_size)  # (x1, y1, w, h)\n",
    "        search = self.preprocessor.process(x_patch_arr)\n",
    "        search_list = [search]\n",
    "\n",
    "        # run the encoder\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            enc_opt = self.network.forward_encoder(self.template_list, search_list, self.template_anno_list)\n",
    "\n",
    "        # run the time neck\n",
    "        with torch.no_grad():\n",
    "            hidden_state = self.h_state.copy()\n",
    "            encoder_out,out_neck, h = self.network.forward_neck(enc_opt, hidden_state)\n",
    "        # run the decoder\n",
    "        with torch.no_grad():\n",
    "            out_dict = self.network.forward_decoder(feature=out_neck)\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        template_list_np = [t.cpu().numpy() for t in self.template_list]\n",
    "        search_list_np = [s.cpu().numpy() for s in search_list]\n",
    "        template_anno_list_np = [ta.cpu().numpy() for ta in self.template_anno_list]\n",
    "\n",
    "        # 2. Flatten inputs\n",
    "        all_inputs_np = template_list_np + search_list_np + template_anno_list_np\n",
    "        \n",
    "        input_names = [inp.name for inp in self.ort_session.get_inputs()]\n",
    "        \n",
    "        input_feed = {name: data for name, data in zip(input_names, all_inputs_np)}\n",
    "\n",
    "        # 5. Run inference\n",
    "        outputs = self.ort_session.run(None, input_feed)\n",
    "        #print(\"Expected input names:\", input_names)\n",
    "        output_names = [out.name for out in self.ort_session.get_outputs()]\n",
    "        out_dict_np = {name: arr for name, arr in zip(self.onnx_output_names, outputs)}\n",
    "        \n",
    "\n",
    "        # 3. Create the input feed dictionary\n",
    "        #input_feed = {name: data for name, data in zip(self.onnx_input_names, all_inputs_np)}\n",
    "        \n",
    "        #input_dict = {name: data for name, data in zip(self.onnx_input_names, all_inputs_np)}\n",
    "        #np.savez('onnx_test_inputs.npz', **input_dict)\n",
    "        # 4. Run ONNX inference\n",
    "        #onnx_outputs = self.ort_session.run(None, input_feed)\n",
    "\n",
    "        # 5. Reconstruct the output dictionary\n",
    "        #out_dict = {name: torch.from_numpy(arr) for name, arr in zip(self.onnx_output_names, onnx_outputs)}\n",
    "        \n",
    "        #out_dict_np = {name: arr for name, arr in zip(self.onnx_output_names, onnx_outputs)}\n",
    "\n",
    "        # If you need the outputs as torch tensors, convert them back.\n",
    "        # Note: The data will be on the CPU. Move it to a GPU if needed, e.g., .to('cuda').\n",
    "        out_dict = {\n",
    "            'pred_boxes': torch.from_numpy(out_dict_np['pred_boxes']).to('cuda'),\n",
    "            'score_map': torch.from_numpy(out_dict_np['score_map']).to('cuda'),\n",
    "            'size_map': torch.from_numpy(out_dict_np['size_map']).to('cuda'),\n",
    "            'offset_map': torch.from_numpy(out_dict_np['offset_map']).to('cuda'),\n",
    "                }\n",
    "\n",
    "        \n",
    "       # with torch.no_grad():\n",
    "       #     out_dict = self.network.forward(\n",
    "       #         template_list=self.template_list,\n",
    "       #         search_list=search_list,\n",
    "       #         template_anno_list=self.template_anno_list,\n",
    "       #         \n",
    "       #         gt_score_map=None\n",
    "       #     )\n",
    "            \n",
    "        \"\"\"print(len(self.template_list))\n",
    "        for i in self.template_list:\n",
    "            print(i.shape)\n",
    "        print(len(search_list))\n",
    "        for i in search_list:\n",
    "            print(i.shape)\n",
    "        print(len(self.template_anno_list))\n",
    "        for i in self.template_anno_list:\n",
    "            print(i.shape)\n",
    "        print(len(self.h_state))\n",
    "        \n",
    "        print(\"-\"*50)\n",
    "        \"\"\"\n",
    "        \n",
    "\n",
    "        # add hann windows\n",
    "        pred_score_map = out_dict['score_map']\n",
    "        if self.cfg[\"TEST\"][\"WINDOW\"] == True:  # for window penalty\n",
    "            response = self.output_window * pred_score_map\n",
    "        else:\n",
    "            response = pred_score_map\n",
    "        #if 'size_map' in out_dict.keys():\n",
    "        pred_boxes, conf_score = cal_bbox(response, out_dict['size_map'],\n",
    "                                                                   out_dict['offset_map'])\n",
    "        #else:\n",
    "        #    pred_boxes, conf_score = cal_bbox(response,\n",
    "        #                                                           out_dict['offset_map'],\n",
    "        #                                                           return_score=True)\n",
    "        pred_boxes = pred_boxes.view(-1, 4)\n",
    "        # Baseline: Take the mean of all pred boxes as the final result\n",
    "        pred_box = (pred_boxes.mean(dim=0) * self.params.search_size / resize_factor).tolist()  # (cx, cy, w, h) [0,1]\n",
    "        # get the final box result\n",
    "        self.state = clip_box(self.map_box_back(pred_box, resize_factor), H, W, margin=10)\n",
    "        # update hiden state\n",
    "        self.h_state = h\n",
    "        if conf_score.item() < self.update_h_t:\n",
    "            self.h_state = [None] * self.cfg[\"MODEL\"][\"NECK\"][\"N_LAYERS\"]\n",
    "\n",
    "        # update the template\n",
    "        if self.num_template > 1:\n",
    "            if (conf_score > self.update_threshold):\n",
    "                z_patch_arr, resize_factor = sample_target(image, self.state, self.params.template_factor,\n",
    "                                                           output_sz=self.params.template_size)\n",
    "                template = self.preprocessor.process(z_patch_arr)\n",
    "                self.memory_template_list.append(template)\n",
    "                prev_box_crop = transform_image_to_crop(torch.tensor(self.state),\n",
    "                                                        torch.tensor(self.state),\n",
    "                                                        resize_factor,\n",
    "                                                        torch.Tensor(\n",
    "                                                            [self.params.template_size, self.params.template_size]),\n",
    "                                                        normalize=True)\n",
    "                self.memory_template_anno_list.append(prev_box_crop.to(template.device).unsqueeze(0))\n",
    "                if len(self.memory_template_list) > self.memory_bank:\n",
    "                    self.memory_template_list.pop(0)\n",
    "                    self.memory_template_anno_list.pop(0)\n",
    "        if (self.frame_id % self.update_intervals == 0):\n",
    "            assert len(self.memory_template_anno_list) == len(self.memory_template_list)\n",
    "            len_list = len(self.memory_template_anno_list)\n",
    "            interval = len_list // self.num_template\n",
    "            for i in range(1, self.num_template):\n",
    "                idx = interval * i\n",
    "                if idx > len_list:\n",
    "                    idx = len_list\n",
    "                self.template_list.append(self.memory_template_list[idx])\n",
    "                self.template_list.pop(1)\n",
    "                self.template_anno_list.append(self.memory_template_anno_list[idx])\n",
    "                self.template_anno_list.pop(1)\n",
    "        assert len(self.template_list) == self.num_template\n",
    "\n",
    "\n",
    "\n",
    "        return {\"target_bbox\": self.state,\n",
    "                \"best_score\": conf_score}\n",
    "\n",
    "    def map_box_back(self, pred_box: list, resize_factor: float):\n",
    "        cx_prev, cy_prev = self.state[0] + 0.5 * self.state[2], self.state[1] + 0.5 * self.state[3]\n",
    "        cx, cy, w, h = pred_box\n",
    "        half_side = 0.5 * self.params.search_size / resize_factor\n",
    "        cx_real = cx + (cx_prev - half_side)\n",
    "        cy_real = cy + (cy_prev - half_side)\n",
    "        return [cx_real - 0.5 * w, cy_real - 0.5 * h, w, h]\n",
    "\n",
    "    def map_box_back_batch(self, pred_box: torch.Tensor, resize_factor: float):\n",
    "        cx_prev, cy_prev = self.state[0] + 0.5 * self.state[2], self.state[1] + 0.5 * self.state[3]\n",
    "        cx, cy, w, h = pred_box.unbind(-1)  # (N,4) --> (N,)\n",
    "        half_side = 0.5 * self.params.search_size / resize_factor\n",
    "        cx_real = cx + (cx_prev - half_side)\n",
    "        cy_real = cy + (cy_prev - half_side)\n",
    "        return torch.stack([cx_real - 0.5 * w, cy_real - 0.5 * h, w, h], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8d677e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0a4cb957",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "class MCITRACK(BaseTracker):\n",
    "    def __init__(self, params):\n",
    "        super(MCITRACK, self).__init__(params)\n",
    "        \n",
    "        # Проверяем и загружаем ONNX-модель\n",
    "        self.onnx_model = onnx.load(\"MCITrack.onnx\")\n",
    "        onnx.checker.check_model(self.onnx_model)\n",
    "        self.ort_session = ort.InferenceSession(\n",
    "            \"MCITrack.onnx\",\n",
    "            providers=[\n",
    "                ('TensorrtExecutionProvider', {\n",
    "                    'trt_engine_cache_enable': True,\n",
    "                    'trt_engine_cache_path': './trt',\n",
    "                    \"trt_fp16_enable\": True,\n",
    "                    'device_id': 0,\n",
    "                }),\n",
    "                ('CUDAExecutionProvider')\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.onnx_input_names = [inp.name for inp in self.ort_session.get_inputs()]\n",
    "        self.onnx_output_names = [out.name for out in self.ort_session.get_outputs()]\n",
    "\n",
    "        self.cfg = params.cfg\n",
    "        self.preprocessor = Preprocessor()\n",
    "        self.state = None\n",
    "\n",
    "        self.fx_sz = self.cfg[\"TEST\"][\"SEARCH_SIZE\"] // self.cfg[\"MODEL\"][\"ENCODER\"][\"STRIDE\"]\n",
    "        if self.cfg[\"TEST\"][\"WINDOW\"]:\n",
    "            self.output_window = hann2d(torch.tensor([self.fx_sz, self.fx_sz]).long(), centered=True).cuda()\n",
    "        self.num_template = self.cfg[\"TEST\"][\"NUM_TEMPLATES\"]\n",
    "\n",
    "        self.frame_id = 0\n",
    "        self.h_state = [None] * self.cfg[\"MODEL\"][\"NECK\"][\"N_LAYERS\"]\n",
    "        self.memory_bank = self.cfg[\"TEST\"][\"MB\"][\"DEFAULT\"]\n",
    "        self.update_h_t = self.cfg[\"TEST\"][\"UPH\"][\"DEFAULT\"]\n",
    "        self.update_threshold = self.cfg[\"TEST\"][\"UPT\"][\"DEFAULT\"]\n",
    "        self.update_intervals = self.cfg[\"TEST\"][\"INTER\"][\"DEFAULT\"]\n",
    "\n",
    "    def initialize(self, image, info: dict):\n",
    "        z_patch_arr, resize_factor = sample_target(image, info['init_bbox'], self.params.template_factor,\n",
    "                                                   output_sz=self.params.template_size)\n",
    "        template = self.preprocessor.process(z_patch_arr)\n",
    "        self.template_list = [template] * self.num_template\n",
    "\n",
    "        self.state = info['init_bbox']\n",
    "        prev_box_crop = transform_image_to_crop(torch.tensor(info['init_bbox']),\n",
    "                                                torch.tensor(info['init_bbox']),\n",
    "                                                resize_factor,\n",
    "                                                torch.Tensor([self.params.template_size, self.params.template_size]),\n",
    "                                                normalize=True)\n",
    "        self.template_anno_list = [prev_box_crop.to(template.device).unsqueeze(0)] * self.num_template\n",
    "        self.frame_id = 0\n",
    "        self.memory_template_list = self.template_list.copy()\n",
    "        self.memory_template_anno_list = self.template_anno_list.copy()\n",
    "\n",
    "    def track(self, image, info: dict = None):\n",
    "        H, W, _ = image.shape\n",
    "        self.frame_id += 1\n",
    "        x_patch_arr, resize_factor = sample_target(image, self.state, self.params.search_factor,\n",
    "                                                   output_sz=self.params.search_size)\n",
    "        search = self.preprocessor.process(x_patch_arr)\n",
    "        search_list = [search]\n",
    "\n",
    "        # Преобразуем входы в numpy\n",
    "        template_list_np = [t.cpu().numpy() for t in self.template_list]\n",
    "        search_list_np = [s.cpu().numpy() for s in search_list]\n",
    "        template_anno_list_np = [ta.cpu().numpy() for ta in self.template_anno_list]\n",
    "        all_inputs_np = template_list_np + search_list_np + template_anno_list_np\n",
    "\n",
    "        # Собираем словарь входов\n",
    "        input_feed = {name: data for name, data in zip(self.onnx_input_names, all_inputs_np)}\n",
    "\n",
    "        # Запускаем инференс\n",
    "        outputs = self.ort_session.run(None, input_feed)\n",
    "        out_dict_np = {name: arr for name, arr in zip(self.onnx_output_names, outputs)}\n",
    "\n",
    "        # Переводим в torch.Tensor\n",
    "        out_dict = {\n",
    "            'pred_boxes': torch.from_numpy(out_dict_np['pred_boxes']).to('cuda'),\n",
    "            'score_map': torch.from_numpy(out_dict_np['score_map']).to('cuda'),\n",
    "            'size_map': torch.from_numpy(out_dict_np['size_map']).to('cuda'),\n",
    "            'offset_map': torch.from_numpy(out_dict_np['offset_map']).to('cuda'),\n",
    "        }\n",
    "\n",
    "        # add hann windows\n",
    "        pred_score_map = out_dict['score_map']\n",
    "        if self.cfg[\"TEST\"][\"WINDOW\"]:\n",
    "            response = self.output_window * pred_score_map\n",
    "        else:\n",
    "            response = pred_score_map\n",
    "\n",
    "        pred_boxes, conf_score = cal_bbox(response, out_dict['size_map'], out_dict['offset_map'])\n",
    "        pred_boxes = pred_boxes.view(-1, 4)\n",
    "        pred_box = (pred_boxes.mean(dim=0) * self.params.search_size / resize_factor).tolist()\n",
    "        self.state = clip_box(self.map_box_back(pred_box, resize_factor), H, W, margin=10)\n",
    "       \n",
    "        \n",
    "\n",
    "\n",
    "        # update hiden state (если используете)\n",
    "        # self.h_state = ... (если нужно)\n",
    "        if conf_score.item() < self.update_h_t:\n",
    "            self.h_state = [None] * self.cfg[\"MODEL\"][\"NECK\"][\"N_LAYERS\"]\n",
    "\n",
    "        return {\n",
    "            \"target_bbox\": self.state,\n",
    "            \"best_score\": conf_score.item(),\n",
    "        }\n",
    "\n",
    "    def map_box_back(self, box, resize_factor):\n",
    "        cx, cy, w, h = box\n",
    "        x = cx - w / 2\n",
    "        y = cy - h / 2\n",
    "        x /= resize_factor\n",
    "        y /= resize_factor\n",
    "        w /= resize_factor\n",
    "        h /= resize_factor\n",
    "        return [x, y, w, h]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d393ae8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e0aebec",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = {}\n",
    "\n",
    "# MODEL\n",
    "cfg[\"MODEL\"] = {}\n",
    "\n",
    "# MODEL.ENCODER\n",
    "cfg[\"MODEL\"][\"ENCODER\"] = {\n",
    "    \"TYPE\": \"dinov2_vitb14\",  # encoder model\n",
    "    \"DROP_PATH\": 0,\n",
    "    \"PRETRAIN_TYPE\": \"mae\",  # mae, default, or scratch. This parameter is not activated for dinov2.\n",
    "    \"USE_CHECKPOINT\": False,  # to save the memory.\n",
    "    \"STRIDE\": 14,\n",
    "    \"POS_TYPE\": 'interpolate',  # type of loading the positional encoding. \"interpolate\" or \"index\".\n",
    "    \"TOKEN_TYPE_INDICATE\": False,  # add a token_type_embedding to indicate the search, template_foreground, template_background\n",
    "    \"INTERACTION_INDEXES\": [[0, 6], [6, 12], [12, 18], [18, 24]],\n",
    "    \"GRAD_CKPT\": False\n",
    "}\n",
    "\n",
    "# MODEL.NECK\n",
    "cfg[\"MODEL\"][\"NECK\"] = {\n",
    "    \"N_LAYERS\": 4,\n",
    "    \"D_MODEL\": 512,\n",
    "    \"D_STATE\": 16  # MAMABA_HIDDEN_STATE\n",
    "}\n",
    "\n",
    "# MODEL.DECODER\n",
    "cfg[\"MODEL\"][\"DECODER\"] = {\n",
    "    \"TYPE\": \"CENTER\",  # MLP, CORNER, CENTER\n",
    "    \"NUM_CHANNELS\": 256\n",
    "}\n",
    "\n",
    "# TRAIN\n",
    "cfg[\"TRAIN\"] = {\n",
    "    \"LR\": 0.0001,\n",
    "    \"WEIGHT_DECAY\": 0.0001,\n",
    "    \"EPOCH\": 500,\n",
    "    \"LR_DROP_EPOCH\": 400,\n",
    "    \"BATCH_SIZE\": 8,\n",
    "    \"NUM_WORKER\": 8,\n",
    "    \"OPTIMIZER\": \"ADAMW\",\n",
    "    \"ENCODER_MULTIPLIER\": 0.1,  # encoder's LR = this factor * LR\n",
    "    \"FREEZE_ENCODER\": False,  # for freezing the parameters of encoder\n",
    "    \"ENCODER_OPEN\": [],  # only for debug, open some layers of encoder when FREEZE_ENCODER is True\n",
    "    \"CE_WEIGHT\": 1.0,  # weight for cross-entropy loss\n",
    "    \"GIOU_WEIGHT\": 2.0,\n",
    "    \"L1_WEIGHT\": 5.0,\n",
    "    \"PRINT_INTERVAL\": 50,  # interval to print the training log\n",
    "    \"GRAD_CLIP_NORM\": 0.1,\n",
    "    \"FIX_BN\": False,\n",
    "    \"ENCODER_W\": \"\",\n",
    "    \"TYPE\": \"normal\",  # normal, peft or fft\n",
    "    \"PRETRAINED_PATH\": None\n",
    "}\n",
    "\n",
    "# TRAIN.SCHEDULER\n",
    "cfg[\"TRAIN\"][\"SCHEDULER\"] = {\n",
    "    \"TYPE\": \"step\",\n",
    "    \"DECAY_RATE\": 0.1\n",
    "}\n",
    "\n",
    "# DATA\n",
    "cfg[\"DATA\"] = {\n",
    "    \"MEAN\": [0.485, 0.456, 0.406],\n",
    "    \"STD\": [0.229, 0.224, 0.225],\n",
    "    \"MAX_SAMPLE_INTERVAL\": 200,\n",
    "    \"SAMPLER_MODE\": \"order\",\n",
    "    \"LOADER\": \"tracking\"\n",
    "}\n",
    "\n",
    "# DATA.TRAIN\n",
    "cfg[\"DATA\"][\"TRAIN\"] = {\n",
    "    \"DATASETS_NAME\": [\"LASOT\", \"GOT10K_vottrain\"],\n",
    "    \"DATASETS_RATIO\": [1, 1],\n",
    "    \"SAMPLE_PER_EPOCH\": 60000\n",
    "}\n",
    "\n",
    "# DATA.SEARCH\n",
    "cfg[\"DATA\"][\"SEARCH\"] = {\n",
    "    \"NUMBER\": 1,  # number of search region, only support 1 for now.\n",
    "    \"SIZE\": 256,\n",
    "    \"FACTOR\": 4.0,\n",
    "    \"CENTER_JITTER\": 3.5,\n",
    "    \"SCALE_JITTER\": 0.5\n",
    "}\n",
    "\n",
    "# DATA.TEMPLATE\n",
    "cfg[\"DATA\"][\"TEMPLATE\"] = {\n",
    "    \"NUMBER\": 1,\n",
    "    \"SIZE\": 128,\n",
    "    \"FACTOR\": 2.0,\n",
    "    \"CENTER_JITTER\": 0,\n",
    "    \"SCALE_JITTER\": 0\n",
    "}\n",
    "\n",
    "# TEST\n",
    "cfg[\"TEST\"] = {\n",
    "    \"TEMPLATE_FACTOR\": 4.0,\n",
    "    \"TEMPLATE_SIZE\": 256,\n",
    "    \"SEARCH_FACTOR\": 2.0,\n",
    "    \"SEARCH_SIZE\": 128,\n",
    "    \"EPOCH\": 500,\n",
    "    \"WINDOW\": False,  # window penalty\n",
    "    \"NUM_TEMPLATES\": 1\n",
    "}\n",
    "\n",
    "# TEST.UPT\n",
    "cfg[\"TEST\"][\"UPT\"] = {\n",
    "    \"DEFAULT\": 1,\n",
    "    \"LASOT\": 0,\n",
    "    \"LASOT_EXTENSION_SUBSET\": 0,\n",
    "    \"TRACKINGNET\": 0,\n",
    "    \"TNL2K\": 0,\n",
    "    \"NFS\": 0,\n",
    "    \"UAV\": 0,\n",
    "    \"VOT20\": 0,\n",
    "    \"GOT10K_TEST\": 0\n",
    "}\n",
    "\n",
    "# TEST.UPH\n",
    "cfg[\"TEST\"][\"UPH\"] = {\n",
    "    \"DEFAULT\": 1,\n",
    "    \"LASOT\": 0,\n",
    "    \"LASOT_EXTENSION_SUBSET\": 0,\n",
    "    \"TRACKINGNET\": 0,\n",
    "    \"TNL2K\": 0,\n",
    "    \"NFS\": 0,\n",
    "    \"UAV\": 0,\n",
    "    \"VOT20\": 0,\n",
    "    \"GOT10K_TEST\": 0\n",
    "}\n",
    "\n",
    "# TEST.INTER\n",
    "cfg[\"TEST\"][\"INTER\"] = {\n",
    "    \"DEFAULT\": 999999,\n",
    "    \"LASOT\": 0,\n",
    "    \"LASOT_EXTENSION_SUBSET\": 0,\n",
    "    \"TRACKINGNET\": 0,\n",
    "    \"TNL2K\": 0,\n",
    "    \"NFS\": 0,\n",
    "    \"UAV\": 0,\n",
    "    \"VOT20\": 0,\n",
    "    \"GOT10K_TEST\": 0\n",
    "}\n",
    "\n",
    "# TEST.MB\n",
    "cfg[\"TEST\"][\"MB\"] = {\n",
    "    \"DEFAULT\": 500,\n",
    "    \"LASOT\": 0,\n",
    "    \"LASOT_EXTENSION_SUBSET\": 0,\n",
    "    \"TRACKINGNET\": 0,\n",
    "    \"TNL2K\": 0,\n",
    "    \"NFS\": 0,\n",
    "    \"UAV\": 0,\n",
    "    \"VOT20\": 0,\n",
    "    \"GOT10K_TEST\": 0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c2c801cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test config:  {'MODEL': {'ENCODER': {'TYPE': 'fastitpnt', 'DROP_PATH': 0.1, 'PRETRAIN_TYPE': './fast_itpn_tiny_1600e_1k.pt', 'USE_CHECKPOINT': False, 'STRIDE': 16, 'POS_TYPE': 'index', 'TOKEN_TYPE_INDICATE': True, 'INTERACTION_INDEXES': [[4, 7], [7, 10], [10, 13], [13, 16]], 'GRAD_CKPT': False}, 'NECK': {'N_LAYERS': 4, 'D_MODEL': 384, 'D_STATE': 16}, 'DECODER': {'TYPE': 'CENTER', 'NUM_CHANNELS': 256}}, 'TRAIN': {'LR': 0.0004, 'WEIGHT_DECAY': 0.0001, 'EPOCH': 300, 'LR_DROP_EPOCH': 240, 'BATCH_SIZE': 64, 'NUM_WORKER': 10, 'OPTIMIZER': 'ADAMW', 'ENCODER_MULTIPLIER': 0.1, 'FREEZE_ENCODER': False, 'ENCODER_OPEN': [], 'CE_WEIGHT': 1.0, 'GIOU_WEIGHT': 2.0, 'L1_WEIGHT': 5.0, 'PRINT_INTERVAL': 50, 'GRAD_CLIP_NORM': 0.1, 'FIX_BN': False, 'ENCODER_W': '', 'TYPE': 'normal', 'PRETRAINED_PATH': None, 'SCHEDULER': {'TYPE': 'step', 'DECAY_RATE': 0.1}}, 'DATA': {'MEAN': [0.485, 0.456, 0.406], 'STD': [0.229, 0.224, 0.225], 'MAX_SAMPLE_INTERVAL': 400, 'SAMPLER_MODE': 'order', 'LOADER': 'tracking', 'TRAIN': {'DATASETS_NAME': ['LASOT', 'GOT10K_vottrain', 'COCO17', 'TRACKINGNET', 'VASTTRACK'], 'DATASETS_RATIO': [1, 1, 1, 1, 1], 'SAMPLE_PER_EPOCH': 60000}, 'SEARCH': {'NUMBER': 2, 'SIZE': 224, 'FACTOR': 4.0, 'CENTER_JITTER': 3.5, 'SCALE_JITTER': 0.5}, 'TEMPLATE': {'NUMBER': 5, 'SIZE': 112, 'FACTOR': 2.0, 'CENTER_JITTER': 0, 'SCALE_JITTER': 0}}, 'TEST': {'TEMPLATE_FACTOR': 2.0, 'TEMPLATE_SIZE': 112, 'SEARCH_FACTOR': 4.0, 'SEARCH_SIZE': 224, 'EPOCH': 300, 'WINDOW': True, 'NUM_TEMPLATES': 5, 'UPT': {'DEFAULT': 1, 'LASOT': 0.8, 'LASOT_EXTENSION_SUBSET': 0.85, 'TRACKINGNET': 0.5, 'TNL2K': 0.5, 'NFS': 0.8, 'UAV': 0.2, 'VOT20': 0.4, 'GOT10K_TEST': 0}, 'UPH': {'DEFAULT': 1, 'LASOT': 0.88, 'LASOT_EXTENSION_SUBSET': 0.97, 'TRACKINGNET': 0.9, 'TNL2K': 0.9, 'NFS': 0.92, 'UAV': 0.91, 'VOT20': 0.94, 'GOT10K_TEST': 0}, 'INTER': {'DEFAULT': 999999, 'LASOT': 70, 'LASOT_EXTENSION_SUBSET': 50, 'TRACKINGNET': 20, 'TNL2K': 20, 'NFS': 90, 'UAV': 1, 'VOT20': 1, 'GOT10K_TEST': 0}, 'MB': {'DEFAULT': 500, 'LASOT': 500, 'LASOT_EXTENSION_SUBSET': 500, 'TRACKINGNET': 200, 'TNL2K': 500, 'NFS': 500, 'UAV': 400, 'VOT20': 500, 'GOT10K_TEST': 0}}}\n"
     ]
    }
   ],
   "source": [
    "class TrackerParams:\n",
    "    \"\"\"Class for tracker parameters.\"\"\"\n",
    "    def set_default_values(self, default_vals: dict):\n",
    "        for name, val in default_vals.items():\n",
    "            if not hasattr(self, name):\n",
    "                setattr(self, name, val)\n",
    "\n",
    "    def get(self, name: str, *default):\n",
    "        \"\"\"Get a parameter value with the given name. If it does not exists, it return the default value given as a\n",
    "        second argument or returns an error if no default value is given.\"\"\"\n",
    "        if len(default) > 1:\n",
    "            raise ValueError('Can only give one default value.')\n",
    "\n",
    "        if not default:\n",
    "            return getattr(self, name)\n",
    "\n",
    "        return getattr(self, name, default[0])\n",
    "\n",
    "    def has(self, name: str):\n",
    "        \"\"\"Check if there exist a parameter with the given name.\"\"\"\n",
    "        return hasattr(self, name)\n",
    "\n",
    "def _update_config(base_cfg, exp_cfg):\n",
    "    if isinstance(base_cfg, dict) and isinstance(exp_cfg, dict):\n",
    "        for k, v in exp_cfg.items():\n",
    "            if k in base_cfg:\n",
    "                if not isinstance(v, dict):\n",
    "                    base_cfg[k] = v\n",
    "                else:\n",
    "                    _update_config(base_cfg[k], v)\n",
    "            else:\n",
    "                raise ValueError(\"{} not exist in config.py\".format(k))\n",
    "    else:\n",
    "        return\n",
    "\n",
    "def update_config_from_file(filename):\n",
    "    exp_config = None\n",
    "    with open(filename) as f:\n",
    "        exp_config = yaml.safe_load(f)\n",
    "        _update_config(cfg, exp_config)\n",
    "    \n",
    "def parameters(yaml_name: str):\n",
    "    params = TrackerParams()\n",
    "\n",
    "    yaml_file = \"mcitrack_t224.yaml\"\n",
    "    update_config_from_file(yaml_file)\n",
    "    params.cfg = cfg\n",
    "    print(\"test config: \", cfg)\n",
    "\n",
    "    params.yaml_name = yaml_name\n",
    "    # template and search region\n",
    "    params.template_factor = cfg[\"TEST\"][\"TEMPLATE_FACTOR\"]\n",
    "    params.template_size = cfg[\"TEST\"][\"TEMPLATE_SIZE\"]\n",
    "    params.search_factor = cfg[\"TEST\"][\"SEARCH_FACTOR\"]\n",
    "    params.search_size = cfg[\"TEST\"][\"SEARCH_SIZE\"]\n",
    "\n",
    "    # Network checkpoint path\n",
    "    params.checkpoint = \"fast_itpn_tiny_1600e_1k.pt\"\n",
    "    # whether to save boxes from all queries\n",
    "    params.save_all_boxes = False\n",
    "\n",
    "    return params\n",
    "\n",
    "params = parameters(\"./mcitrack_t224.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "11d9b72e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************** EP Error ***************\n",
      "EP Error D:\\a\\_work\\1\\s\\onnxruntime\\python\\onnxruntime_pybind_state.cc:505 onnxruntime::python::RegisterTensorRTPluginsAsCustomOps Please install TensorRT libraries as mentioned in the GPU requirements page, make sure they're in the PATH or LD_LIBRARY_PATH, and that your GPU is supported.\n",
      " when using [('TensorrtExecutionProvider', {'trt_engine_cache_enable': True, 'trt_engine_cache_path': './trt', 'trt_fp16_enable': True, 'device_id': 0}), 'CUDAExecutionProvider']\n",
      "Falling back to ['CPUExecutionProvider'] and retrying.\n",
      "****************************************\n"
     ]
    }
   ],
   "source": [
    "treacker = MCITRACK(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e91bfd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "77a1a4e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can't read frame\n"
     ]
    }
   ],
   "source": [
    "#Трекинг по видео\n",
    "file = \"test.mp4\"\n",
    "video = cv2.VideoCapture(file)\n",
    "#fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "#fps=video.get(cv2.CAP_PROP_FPS)\n",
    "#video_vriter = cv2.VideoWriter(file.split('.')[0]+\"_\"+\".avi\", fourcc, fps, (1920, 1080))\n",
    "\n",
    "\n",
    "ok, image = video.read()\n",
    "if not video.isOpened():\n",
    "    print(\"Could not open video\")\n",
    "    sys.exit()\n",
    "    \n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "x, y, w, h = cv2.selectROI( image, fromCenter=False)\n",
    "init_state = [x, y, w, h]\n",
    "def _build_init_info(box):\n",
    "            return {'init_bbox': box}\n",
    "treacker.initialize(image, _build_init_info(init_state))\n",
    "counter = 0\n",
    "while True:\n",
    "            ok, image = video.read()\n",
    "            if not ok:\n",
    "                print(\"Can't read frame\")\n",
    "                break\n",
    "\n",
    "            \n",
    "            out  = treacker.track(image)\n",
    "            state = [int(s) for s in out['target_bbox']]\n",
    "            best_score = out[\"best_score\"]\n",
    "            if isinstance(best_score, torch.Tensor):\n",
    "                best_score = best_score.item()\n",
    "         \n",
    "            x, y, w, h = [int(x) for x in state]\n",
    "\n",
    "            color = (0, 255, 0)  # Цвет в формате BGR\n",
    "            cv2.rectangle(image, (x, y), (x + w, y + h), color, 2)\n",
    "\n",
    "\n",
    "            cv2.imshow(\"tracking\", image)\n",
    "            #video_vriter.write(image)\n",
    "\n",
    "\n",
    "            k = cv2.waitKey(1)            \n",
    "            if k == 32:  # SPACE\n",
    "                ok, image = video.read()                             \n",
    "                x, y, w, h = cv2.selectROI( image, fromCenter=False)\n",
    "                init_state = [x, y, w, h]\n",
    "                treacker.initialize(image, _build_init_info(init_state))\n",
    "            if k == 27:  # ESC\n",
    "                break\n",
    "        \n",
    "                \n",
    "                \n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "video.release()\n",
    "#video_vriter.release()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "841a661b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tracked bbox: [1270, 710, 10, 10]\n",
      "Warning: bbox too small or invalid, skipping drawing\n",
      "Tracked bbox: [1, 0, 10, 10]\n",
      "Warning: bbox too small or invalid, skipping drawing\n",
      "Tracked bbox: [2, 2, 10, 10]\n",
      "Warning: bbox too small or invalid, skipping drawing\n",
      "Tracked bbox: [2, 3, 10, 10]\n",
      "Warning: bbox too small or invalid, skipping drawing\n",
      "Tracked bbox: [1, 3, 10, 10]\n",
      "Warning: bbox too small or invalid, skipping drawing\n",
      "Tracked bbox: [2, 2, 10, 10]\n",
      "Warning: bbox too small or invalid, skipping drawing\n",
      "Tracked bbox: [2, 3, 10, 10]\n",
      "Warning: bbox too small or invalid, skipping drawing\n",
      "Tracked bbox: [2, 2, 10, 10]\n",
      "Warning: bbox too small or invalid, skipping drawing\n",
      "Tracked bbox: [1, 2, 10, 10]\n",
      "Warning: bbox too small or invalid, skipping drawing\n",
      "Tracked bbox: [2, 3, 10, 10]\n",
      "Warning: bbox too small or invalid, skipping drawing\n",
      "Tracked bbox: [2, 2, 10, 10]\n",
      "Warning: bbox too small or invalid, skipping drawing\n",
      "Tracked bbox: [2, 3, 10, 10]\n",
      "Warning: bbox too small or invalid, skipping drawing\n",
      "Tracked bbox: [2, 2, 10, 10]\n",
      "Warning: bbox too small or invalid, skipping drawing\n",
      "Tracked bbox: [2, 2, 10, 10]\n",
      "Warning: bbox too small or invalid, skipping drawing\n",
      "Tracked bbox: [2, 2, 10, 10]\n",
      "Warning: bbox too small or invalid, skipping drawing\n",
      "Tracked bbox: [2, 1, 10, 10]\n",
      "Warning: bbox too small or invalid, skipping drawing\n",
      "Tracked bbox: [2, 2, 10, 10]\n",
      "Warning: bbox too small or invalid, skipping drawing\n",
      "Tracked bbox: [2, 1, 10, 10]\n",
      "Warning: bbox too small or invalid, skipping drawing\n",
      "Tracked bbox: [2, 2, 10, 10]\n",
      "Warning: bbox too small or invalid, skipping drawing\n",
      "Tracked bbox: [2, 3, 10, 10]\n",
      "Warning: bbox too small or invalid, skipping drawing\n",
      "Tracked bbox: [2, 1, 10, 10]\n",
      "Warning: bbox too small or invalid, skipping drawing\n",
      "Tracked bbox: [1, 2, 10, 10]\n",
      "Warning: bbox too small or invalid, skipping drawing\n",
      "Tracked bbox: [2, 1, 10, 10]\n",
      "Warning: bbox too small or invalid, skipping drawing\n",
      "Tracked bbox: [2, 3, 10, 10]\n",
      "Warning: bbox too small or invalid, skipping drawing\n",
      "Tracked bbox: [2, 2, 10, 10]\n",
      "Warning: bbox too small or invalid, skipping drawing\n",
      "Tracked bbox: [2, 1, 10, 10]\n",
      "Warning: bbox too small or invalid, skipping drawing\n",
      "Tracked bbox: [2, 2, 10, 10]\n",
      "Warning: bbox too small or invalid, skipping drawing\n",
      "Tracked bbox: [2, 2, 10, 10]\n",
      "Warning: bbox too small or invalid, skipping drawing\n",
      "Tracked bbox: [2, 2, 10, 10]\n",
      "Warning: bbox too small or invalid, skipping drawing\n",
      "Tracked bbox: [2, 2, 10, 10]\n",
      "Warning: bbox too small or invalid, skipping drawing\n",
      "Tracked bbox: [2, 2, 10, 10]\n",
      "Warning: bbox too small or invalid, skipping drawing\n",
      "Tracked bbox: [2, 2, 10, 10]\n",
      "Warning: bbox too small or invalid, skipping drawing\n",
      "Tracked bbox: [2, 2, 10, 10]\n",
      "Warning: bbox too small or invalid, skipping drawing\n",
      "Tracked bbox: [2, 2, 10, 10]\n",
      "Warning: bbox too small or invalid, skipping drawing\n",
      "Tracked bbox: [2, 2, 10, 10]\n",
      "Warning: bbox too small or invalid, skipping drawing\n",
      "Tracked bbox: [2, 2, 10, 10]\n",
      "Warning: bbox too small or invalid, skipping drawing\n",
      "Tracked bbox: [2, 2, 10, 10]\n",
      "Warning: bbox too small or invalid, skipping drawing\n",
      "Tracked bbox: [2, 2, 10, 10]\n",
      "Warning: bbox too small or invalid, skipping drawing\n",
      "Tracked bbox: [2, 3, 10, 10]\n",
      "Warning: bbox too small or invalid, skipping drawing\n",
      "Tracked bbox: [2, 1, 10, 10]\n",
      "Warning: bbox too small or invalid, skipping drawing\n",
      "Tracked bbox: [2, 2, 10, 10]\n",
      "Warning: bbox too small or invalid, skipping drawing\n",
      "Tracked bbox: [2, 2, 10, 10]\n",
      "Warning: bbox too small or invalid, skipping drawing\n",
      "Tracked bbox: [2, 2, 10, 10]\n",
      "Warning: bbox too small or invalid, skipping drawing\n",
      "Tracked bbox: [2, 2, 10, 10]\n",
      "Warning: bbox too small or invalid, skipping drawing\n",
      "Tracked bbox: [2, 2, 10, 10]\n",
      "Warning: bbox too small or invalid, skipping drawing\n",
      "Tracked bbox: [2, 2, 10, 10]\n",
      "Warning: bbox too small or invalid, skipping drawing\n",
      "Tracked bbox: [2, 2, 10, 10]\n",
      "Warning: bbox too small or invalid, skipping drawing\n",
      "Tracked bbox: [2, 2, 10, 10]\n",
      "Warning: bbox too small or invalid, skipping drawing\n",
      "Tracked bbox: [2, 2, 10, 10]\n",
      "Warning: bbox too small or invalid, skipping drawing\n",
      "Tracked bbox: [2, 2, 10, 10]\n",
      "Warning: bbox too small or invalid, skipping drawing\n",
      "Tracked bbox: [2, 2, 10, 10]\n",
      "Warning: bbox too small or invalid, skipping drawing\n",
      "Tracked bbox: [2, 2, 10, 10]\n",
      "Warning: bbox too small or invalid, skipping drawing\n",
      "Tracked bbox: [2, 2, 10, 10]\n",
      "Warning: bbox too small or invalid, skipping drawing\n",
      "Tracked bbox: [2, 2, 10, 10]\n",
      "Warning: bbox too small or invalid, skipping drawing\n",
      "Tracked bbox: [2, 2, 10, 10]\n",
      "Warning: bbox too small or invalid, skipping drawing\n",
      "Tracked bbox: [2, 2, 10, 10]\n",
      "Warning: bbox too small or invalid, skipping drawing\n",
      "Tracked bbox: [2, 2, 10, 10]\n",
      "Warning: bbox too small or invalid, skipping drawing\n",
      "Tracked bbox: [2, 2, 10, 10]\n",
      "Warning: bbox too small or invalid, skipping drawing\n",
      "Tracked bbox: [2, 2, 10, 10]\n",
      "Warning: bbox too small or invalid, skipping drawing\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "file = \"test.mp4\"\n",
    "video = cv2.VideoCapture(file)\n",
    "\n",
    "if not video.isOpened():\n",
    "    print(\"Could not open video\")\n",
    "    sys.exit()\n",
    "\n",
    "ok, frame = video.read()\n",
    "if not ok:\n",
    "    print(\"Can't read first frame\")\n",
    "    sys.exit()\n",
    "\n",
    "# Выбор ROI на BGR-изображении\n",
    "x, y, w, h = cv2.selectROI(\"Select ROI\", frame, fromCenter=False)\n",
    "cv2.destroyWindow(\"Select ROI\")\n",
    "\n",
    "def _build_init_info(box):\n",
    "    return {'init_bbox': box}\n",
    "\n",
    "frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "treacker.initialize(frame_rgb, _build_init_info([x, y, w, h]))\n",
    "\n",
    "while True:\n",
    "    ok, frame = video.read()\n",
    "    if not ok:\n",
    "        break\n",
    "\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    out = treacker.track(frame_rgb)\n",
    "\n",
    "    bbox = [int(v) for v in out['target_bbox']]\n",
    "    print(f\"Tracked bbox: {bbox}\")  # Вывод координат для отладки\n",
    "\n",
    "    # Проверяем координаты рамки\n",
    "    h_img, w_img, _ = frame.shape\n",
    "    x, y, bw, bh = bbox\n",
    "\n",
    "    # Коррекция координат, чтобы рамка была в пределах изображения\n",
    "    x = max(0, min(x, w_img - 1))\n",
    "    y = max(0, min(y, h_img - 1))\n",
    "    bw = max(1, min(bw, w_img - x))\n",
    "    bh = max(1, min(bh, h_img - y))\n",
    "\n",
    "    # Проверяем, что рамка не слишком мала\n",
    "    if bw > 10 and bh > 10:\n",
    "        cv2.rectangle(frame, (x, y), (x + bw, y + bh), (0, 255, 0), 2)\n",
    "    else:\n",
    "        print(\"Warning: bbox too small or invalid, skipping drawing\")\n",
    "\n",
    "    cv2.imshow(\"Tracking\", frame)\n",
    "\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    if key == 27:  # ESC\n",
    "        break\n",
    "    elif key == 32:  # SPACE - переинициализация ROI\n",
    "        x, y, w, h = cv2.selectROI(\"Select ROI\", frame, fromCenter=False)\n",
    "        cv2.destroyWindow(\"Select ROI\")\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        treacker.initialize(frame_rgb, _build_init_info([x, y, w, h]))\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "video.release()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f23353",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c1c535",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d8b93869",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Метрики\n",
    "import numpy as np\n",
    "\n",
    "def iou(boxA, boxB):\n",
    "    # boxA, boxB: [x, y, w, h]\n",
    "    xA = max(boxA[0], boxB[0])\n",
    "    yA = max(boxA[1], boxB[1])\n",
    "    xB = min(boxA[0] + boxA[2], boxB[0] + boxB[2])\n",
    "    yB = min(boxA[1] + boxA[3], boxB[1] + boxB[3])\n",
    "\n",
    "    interW = max(0, xB - xA)\n",
    "    interH = max(0, yB - yA)\n",
    "    interArea = interW * interH\n",
    "\n",
    "    boxAArea = boxA[2] * boxA[3]\n",
    "    boxBArea = boxB[2] * boxB[3]\n",
    "    unionArea = boxAArea + boxBArea - interArea\n",
    "\n",
    "    if unionArea == 0:\n",
    "        return 0.0\n",
    "    return interArea / unionArea\n",
    "\n",
    "def precision(boxA, boxB):\n",
    "    # центры bbox\n",
    "    centerA = (boxA[0] + boxA[2]/2, boxA[1] + boxA[3]/2)\n",
    "    centerB = (boxB[0] + boxB[2]/2, boxB[1] + boxB[3]/2)\n",
    "    dist = np.sqrt((centerA[0] - centerB[0])**2 + (centerA[1] - centerB[1])**2)\n",
    "    return dist\n",
    "sr_thresh = 0.5\n",
    "prec_thresh = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6587c09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Трекинг got10k с метриками TRT\n",
    "import glob\n",
    "import time\n",
    "import  os\n",
    "gt_bboxes = []\n",
    "pred_bboxes = []\n",
    "seq_path = \"val/GOT-10k_Val_000001\"\n",
    "txt_files = glob.glob(os.path.join(seq_path, '*.txt'))\n",
    "if not txt_files:\n",
    "    raise FileNotFoundError(f\"No .txt files found in {seq_path}\")\n",
    "\n",
    "img_files = sorted(glob.glob(os.path.join(seq_path, '*.jpg')))\n",
    "with open(txt_files[0], 'r') as f:\n",
    "    gt_bboxes = [list(map(float, line.strip().split(','))) for line in f]\n",
    "\n",
    "# Получаем размер первого изображения\n",
    "sample_img = cv2.imread(img_files[0])\n",
    "if sample_img is None:\n",
    "    raise ValueError(f\"Failed to read sample image: {img_files[0]}\")\n",
    "\n",
    "#height, width = sample_img.shape[:2]\n",
    "#fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "#output_filename = f\"{seq_path.split('/')[-1]}_output.avi\"\n",
    "#video_vriter = cv2.VideoWriter(output_filename, fourcc, 10, (width, height))  \n",
    "\n",
    "assert len(img_files) == len(gt_bboxes), \"Количество кадров и bbox'ов не совпадает\"\n",
    "\n",
    "x, y, w, h = map(int, gt_bboxes[0])\n",
    "init_state = [x, y, w, h]\n",
    "\n",
    "def _build_init_info(box):\n",
    "            return {'init_bbox': box}\n",
    "\n",
    "counter = 0\n",
    "\n",
    "\n",
    "#treacker.initialize(sample_img, _build_init_info(init_state))\n",
    "\n",
    "start_time = time.time()  # Начало замера\n",
    "\n",
    "for img_file, bbox in zip(img_files, gt_bboxes):\n",
    "        \n",
    "        # Читаем изображение\n",
    "        img = cv2.imread(img_file)\n",
    "        if img is None:\n",
    "            print(f\"Не удалось загрузить изображение: {img_file}\")\n",
    "            continue\n",
    "        \n",
    "        \n",
    "        out  = treacker.track(img)\n",
    "        state = [int(s) for s in out['target_bbox']]   \n",
    "                           \n",
    "        # Рисуем bounding box        \n",
    "        x, y, w, h = [int(x) for x in state]\n",
    "\n",
    "        cv2.rectangle(img, (x, y), (x + w, y + h), (0, 0, 200), 2)\n",
    "        \n",
    "        x1, y1, w1, h1 = map(int, bbox)\n",
    "        cv2.rectangle(img, (x1, y1), (x1+w1, y1+h1), (0, 200, 0), 2)\n",
    "        bbox_pred = x, y, w, h\n",
    "        \n",
    "        gt_bboxes.append(bbox)\n",
    "        pred_bboxes.append(bbox_pred)\n",
    "\n",
    "        #cv2.imshow(seq_path, img)\n",
    "        #video_vriter.write(img)\n",
    "        counter+=1\n",
    "\n",
    "\n",
    "        # Выход по нажатию 'q' или ESC\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "        if key == ord('q') or key == 27:\n",
    "            break\n",
    "       \n",
    "        \n",
    "                \n",
    "end_time = time.time()    # Конец замера    \n",
    "total_frames = counter       # Общее количество обработанных кадров\n",
    "total_time = end_time - start_time\n",
    "fps = total_frames / total_time\n",
    "ious = [iou(gt, pred) for gt, pred in zip(gt_bboxes, pred_bboxes)]\n",
    "ao = np.mean(ious)\n",
    "sr = np.mean([1 if val >= sr_thresh else 0 for val in ious])\n",
    "precisions = [precision(gt, pred) for gt, pred in zip(gt_bboxes, pred_bboxes)]\n",
    "prec = np.mean([1 if d <= prec_thresh else 0 for d in precisions])\n",
    "\n",
    "print(f\"GOT: {seq_path}\")\n",
    "print(f\"FPS: {fps:.2f}\")\n",
    "print(f'Success Rate (SR@0.5): {sr:.2f}')\n",
    "print(f'Average Overlap (AO): {ao:.2f}')\n",
    "print(f'Precision @20px: {prec:.2f}')\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "#video_vriter.release()\n",
    "#print(f\"Video saved as: {output_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148b69e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f8df5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8109b4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd61806",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc54fc7f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3_10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
