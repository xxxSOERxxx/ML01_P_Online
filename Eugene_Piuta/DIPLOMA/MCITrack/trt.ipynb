{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' TRT_LOGGER = trt.Logger(trt.Logger.WARNING)\\n\\ndef load_engine(trt_runtime, engine_path):\\n    with open(engine_path, \"rb\") as f:\\n        engine_data = f.read()\\n    return trt_runtime.deserialize_cuda_engine(engine_data)\\n\\ntrt_runtime = trt.Runtime(TRT_LOGGER)\\nengine = load_engine(trt_runtime, \"MCITrack.trt\")\\ncontext = engine.create_execution_context() '"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorrt as trt\n",
    "import pycuda.driver as cuda\n",
    "import pycuda.autoinit  # автоматически инициализирует CUDA контекст\n",
    "import torch\n",
    "import cv2\n",
    "import sys\n",
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "import yaml\n",
    "import torch\n",
    "import numpy as np\n",
    "\"\"\" TRT_LOGGER = trt.Logger(trt.Logger.WARNING)\n",
    "\n",
    "def load_engine(trt_runtime, engine_path):\n",
    "    with open(engine_path, \"rb\") as f:\n",
    "        engine_data = f.read()\n",
    "    return trt_runtime.deserialize_cuda_engine(engine_data)\n",
    "\n",
    "trt_runtime = trt.Runtime(TRT_LOGGER)\n",
    "engine = load_engine(trt_runtime, \"MCITrack.trt\")\n",
    "context = engine.create_execution_context() \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_bbox(score_map_ctr, size_map, offset_map, return_score=True):\n",
    "        feat_sz = 14\n",
    "        max_score, idx = torch.max(score_map_ctr.flatten(1), dim=1, keepdim=True) # score_map_ctr.flatten(1): torch.Size([32, 256]) idx: torch.Size([32, 1]) max_score: torch.Size([32, 1])\n",
    "        idx_y = torch.div(idx, feat_sz, rounding_mode='floor')\n",
    "        idx_x = idx % feat_sz\n",
    "       \n",
    "        \n",
    "\n",
    "        idx = idx.unsqueeze(1).expand(idx.shape[0], 2, 1)\n",
    "        size = size_map.flatten(2).gather(dim=2, index=idx) # size_map: torch.Size([32, 2, 16, 16])  size_map.flatten(2): torch.Size([32, 2, 256])\n",
    "        offset = offset_map.flatten(2).gather(dim=2, index=idx).squeeze(-1)\n",
    "\n",
    "        bbox = torch.cat([(idx_x.to(torch.float) + offset[:, :1]) / feat_sz,\n",
    "                          (idx_y.to(torch.float) + offset[:, 1:]) / feat_sz,\n",
    "                          size.squeeze(-1)], dim=1)\n",
    "\n",
    "        if return_score:\n",
    "            return bbox, max_score\n",
    "        return (bbox, max_score) if return_score else bbox\n",
    "        \n",
    "class Preprocessor(object):\n",
    "    def __init__(self):\n",
    "        self.mean = torch.tensor([0.485, 0.456, 0.406]).view((1, 3, 1, 1)).cuda()\n",
    "        self.std = torch.tensor([0.229, 0.224, 0.225]).view((1, 3, 1, 1)).cuda()\n",
    "        self.mm_mean = torch.tensor([0.485, 0.456, 0.406, 0.485, 0.456, 0.406]).view((1, 6, 1, 1)).cuda()\n",
    "        self.mm_std = torch.tensor([0.229, 0.224, 0.225, 0.229, 0.224, 0.225]).view((1, 6, 1, 1)).cuda()\n",
    "\n",
    "    def process(self, img_arr: np.ndarray):\n",
    "        if img_arr.shape[-1] == 6:\n",
    "            mean = self.mm_mean\n",
    "            std = self.mm_std\n",
    "        else:\n",
    "            mean = self.mean\n",
    "            std = self.std\n",
    "        # Deal with the image patch\n",
    "        img_tensor = torch.tensor(img_arr).cuda().float().permute((2,0,1)).unsqueeze(dim=0)\n",
    "        # img_tensor = torch.tensor(img_arr).float().permute((2,0,1)).unsqueeze(dim=0)\n",
    "        img_tensor_norm = ((img_tensor / 255.0) - mean) / std  # (1,3,H,W)\n",
    "        return img_tensor_norm\n",
    "    \n",
    "def hann1d(sz: int, centered = True) -> torch.Tensor:\n",
    "    \"\"\"1D cosine window.\"\"\"\n",
    "    if centered:\n",
    "        return 0.5 * (1 - torch.cos((2 * math.pi / (sz + 1)) * torch.arange(1, sz + 1).float()))\n",
    "    w = 0.5 * (1 + torch.cos((2 * math.pi / (sz + 2)) * torch.arange(0, sz//2 + 1).float()))\n",
    "    return torch.cat([w, w[1:sz-sz//2].flip((0,))])\n",
    "    \n",
    "def hann2d(sz: torch.Tensor, centered = True) -> torch.Tensor:\n",
    "    \"\"\"2D cosine window.\"\"\"\n",
    "    return hann1d(sz[0].item(), centered).reshape(1, 1, -1, 1) * hann1d(sz[1].item(), centered).reshape(1, 1, 1, -1)    \n",
    "\n",
    "def sample_target(im, target_bb, search_area_factor, output_sz=None):\n",
    "   \n",
    "    if not isinstance(target_bb, list):\n",
    "        x, y, w, h = target_bb.tolist()\n",
    "    else:\n",
    "        x, y, w, h = target_bb\n",
    "    # Crop image\n",
    "    crop_sz = math.ceil(math.sqrt(w * h) * search_area_factor)\n",
    "\n",
    "    if crop_sz < 1:\n",
    "        raise Exception('Too small bounding box.')\n",
    "\n",
    "    x1 = round(x + 0.5 * w - crop_sz * 0.5)\n",
    "    x2 = x1 + crop_sz\n",
    "\n",
    "    y1 = round(y + 0.5 * h - crop_sz * 0.5)\n",
    "    y2 = y1 + crop_sz\n",
    "\n",
    "    x1_pad = max(0, -x1)\n",
    "    x2_pad = max(x2 - im.shape[1] + 1, 0)\n",
    "\n",
    "    y1_pad = max(0, -y1)\n",
    "    y2_pad = max(y2 - im.shape[0] + 1, 0)\n",
    "\n",
    "    # Crop target\n",
    "    im_crop = im[y1 + y1_pad:y2 - y2_pad, x1 + x1_pad:x2 - x2_pad, :]\n",
    "\n",
    "    # Pad\n",
    "    im_crop_padded = cv2.copyMakeBorder(im_crop, y1_pad, y2_pad, x1_pad, x2_pad, cv2.BORDER_CONSTANT)\n",
    "    # deal with attention mask\n",
    "    H, W, _ = im_crop_padded.shape\n",
    "\n",
    "    if output_sz is not None:\n",
    "        resize_factor = output_sz / crop_sz\n",
    "        im_crop_padded = cv2.resize(im_crop_padded, (output_sz, output_sz))\n",
    "\n",
    "        return im_crop_padded, resize_factor\n",
    "\n",
    "    else:\n",
    "        return im_crop_padded, 1.0\n",
    "def transform_image_to_crop(box_in: torch.Tensor, box_extract: torch.Tensor, resize_factor: float,\n",
    "                            crop_sz: torch.Tensor, normalize=False) -> torch.Tensor:\n",
    "   \n",
    "    box_extract_center = box_extract[0:2] + 0.5 * box_extract[2:4]\n",
    "\n",
    "    box_in_center = box_in[0:2] + 0.5 * box_in[2:4]\n",
    "\n",
    "    box_out_center = (crop_sz - 1) / 2 + (box_in_center - box_extract_center) * resize_factor\n",
    "    box_out_wh = box_in[2:4] * resize_factor\n",
    "\n",
    "    box_out = torch.cat((box_out_center - 0.5 * box_out_wh, box_out_wh))\n",
    "    if normalize:\n",
    "        return box_out / (crop_sz[0]-1)\n",
    "    else:\n",
    "        return box_out\n",
    "def clip_box(box: list, H, W, margin=0):\n",
    "    x1, y1, w, h = box\n",
    "    x2, y2 = x1 + w, y1 + h\n",
    "    x1 = min(max(0, x1), W-margin)\n",
    "    x2 = min(max(margin, x2), W)\n",
    "    y1 = min(max(0, y1), H-margin)\n",
    "    y2 = min(max(margin, y2), H)\n",
    "    w = max(margin, x2-x1)\n",
    "    h = max(margin, y2-y1)\n",
    "    return [x1, y1, w, h]\n",
    "\n",
    "class BaseTracker():\n",
    "    \"\"\"Base class for all trackers.\"\"\"\n",
    "\n",
    "    def __init__(self, params):\n",
    "        self.params = params\n",
    "        self.visdom = None\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    def predicts_segmentation_mask(self):\n",
    "        return False\n",
    "\n",
    "    def initialize(self, image, info: dict) -> dict:\n",
    "        \"\"\"Overload this function in your tracker. This should initialize the model.\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def track(self, image, info: dict = None) -> dict:\n",
    "        \"\"\"Overload this function in your tracker. This should track in the frame and update the model.\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def visdom_draw_tracking(self, image, box, segmentation=None):\n",
    "        # Упрощенная обработка box без OrderedDict\n",
    "        if isinstance(box, dict):  # Проверяем на обычный dict вместо OrderedDict\n",
    "            box = list(box.values())  # Берем только значения\n",
    "        elif not isinstance(box, (list, tuple)):  # Если не коллекция\n",
    "            box = (box,)  # Превращаем в кортеж\n",
    "        \n",
    "        # Визуализация\n",
    "        if segmentation is None:\n",
    "            self.visdom.register((image, *box), 'Tracking', 1, 'Tracking')\n",
    "        else:\n",
    "            self.visdom.register((image, *box, segmentation), 'Tracking', 1, 'Tracking')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = {}\n",
    "\n",
    "# MODEL\n",
    "cfg[\"MODEL\"] = {}\n",
    "\n",
    "# MODEL.ENCODER\n",
    "cfg[\"MODEL\"][\"ENCODER\"] = {\n",
    "    \"TYPE\": \"dinov2_vitb14\",  # encoder model\n",
    "    \"DROP_PATH\": 0,\n",
    "    \"PRETRAIN_TYPE\": \"mae\",  # mae, default, or scratch. This parameter is not activated for dinov2.\n",
    "    \"USE_CHECKPOINT\": False,  # to save the memory.\n",
    "    \"STRIDE\": 14,\n",
    "    \"POS_TYPE\": 'interpolate',  # type of loading the positional encoding. \"interpolate\" or \"index\".\n",
    "    \"TOKEN_TYPE_INDICATE\": False,  # add a token_type_embedding to indicate the search, template_foreground, template_background\n",
    "    \"INTERACTION_INDEXES\": [[0, 6], [6, 12], [12, 18], [18, 24]],\n",
    "    \"GRAD_CKPT\": False\n",
    "}\n",
    "\n",
    "# MODEL.NECK\n",
    "cfg[\"MODEL\"][\"NECK\"] = {\n",
    "    \"N_LAYERS\": 4,\n",
    "    \"D_MODEL\": 512,\n",
    "    \"D_STATE\": 16  # MAMABA_HIDDEN_STATE\n",
    "}\n",
    "\n",
    "# MODEL.DECODER\n",
    "cfg[\"MODEL\"][\"DECODER\"] = {\n",
    "    \"TYPE\": \"CENTER\",  # MLP, CORNER, CENTER\n",
    "    \"NUM_CHANNELS\": 256\n",
    "}\n",
    "\n",
    "# TRAIN\n",
    "cfg[\"TRAIN\"] = {\n",
    "    \"LR\": 0.0001,\n",
    "    \"WEIGHT_DECAY\": 0.0001,\n",
    "    \"EPOCH\": 500,\n",
    "    \"LR_DROP_EPOCH\": 400,\n",
    "    \"BATCH_SIZE\": 8,\n",
    "    \"NUM_WORKER\": 8,\n",
    "    \"OPTIMIZER\": \"ADAMW\",\n",
    "    \"ENCODER_MULTIPLIER\": 0.1,  # encoder's LR = this factor * LR\n",
    "    \"FREEZE_ENCODER\": False,  # for freezing the parameters of encoder\n",
    "    \"ENCODER_OPEN\": [],  # only for debug, open some layers of encoder when FREEZE_ENCODER is True\n",
    "    \"CE_WEIGHT\": 1.0,  # weight for cross-entropy loss\n",
    "    \"GIOU_WEIGHT\": 2.0,\n",
    "    \"L1_WEIGHT\": 5.0,\n",
    "    \"PRINT_INTERVAL\": 50,  # interval to print the training log\n",
    "    \"GRAD_CLIP_NORM\": 0.1,\n",
    "    \"FIX_BN\": False,\n",
    "    \"ENCODER_W\": \"\",\n",
    "    \"TYPE\": \"normal\",  # normal, peft or fft\n",
    "    \"PRETRAINED_PATH\": None\n",
    "}\n",
    "\n",
    "# TRAIN.SCHEDULER\n",
    "cfg[\"TRAIN\"][\"SCHEDULER\"] = {\n",
    "    \"TYPE\": \"step\",\n",
    "    \"DECAY_RATE\": 0.1\n",
    "}\n",
    "\n",
    "# DATA\n",
    "cfg[\"DATA\"] = {\n",
    "    \"MEAN\": [0.485, 0.456, 0.406],\n",
    "    \"STD\": [0.229, 0.224, 0.225],\n",
    "    \"MAX_SAMPLE_INTERVAL\": 200,\n",
    "    \"SAMPLER_MODE\": \"order\",\n",
    "    \"LOADER\": \"tracking\"\n",
    "}\n",
    "\n",
    "# DATA.TRAIN\n",
    "cfg[\"DATA\"][\"TRAIN\"] = {\n",
    "    \"DATASETS_NAME\": [\"LASOT\", \"GOT10K_vottrain\"],\n",
    "    \"DATASETS_RATIO\": [1, 1],\n",
    "    \"SAMPLE_PER_EPOCH\": 60000\n",
    "}\n",
    "\n",
    "# DATA.SEARCH\n",
    "cfg[\"DATA\"][\"SEARCH\"] = {\n",
    "    \"NUMBER\": 1,  # number of search region, only support 1 for now.\n",
    "    \"SIZE\": 256,\n",
    "    \"FACTOR\": 4.0,\n",
    "    \"CENTER_JITTER\": 3.5,\n",
    "    \"SCALE_JITTER\": 0.5\n",
    "}\n",
    "\n",
    "# DATA.TEMPLATE\n",
    "cfg[\"DATA\"][\"TEMPLATE\"] = {\n",
    "    \"NUMBER\": 1,\n",
    "    \"SIZE\": 128,\n",
    "    \"FACTOR\": 2.0,\n",
    "    \"CENTER_JITTER\": 0,\n",
    "    \"SCALE_JITTER\": 0\n",
    "}\n",
    "\n",
    "# TEST\n",
    "cfg[\"TEST\"] = {\n",
    "    \"TEMPLATE_FACTOR\": 4.0,\n",
    "    \"TEMPLATE_SIZE\": 256,\n",
    "    \"SEARCH_FACTOR\": 2.0,\n",
    "    \"SEARCH_SIZE\": 128,\n",
    "    \"EPOCH\": 500,\n",
    "    \"WINDOW\": False,  # window penalty\n",
    "    \"NUM_TEMPLATES\": 1\n",
    "}\n",
    "\n",
    "# TEST.UPT\n",
    "cfg[\"TEST\"][\"UPT\"] = {\n",
    "    \"DEFAULT\": 1,\n",
    "    \"LASOT\": 0,\n",
    "    \"LASOT_EXTENSION_SUBSET\": 0,\n",
    "    \"TRACKINGNET\": 0,\n",
    "    \"TNL2K\": 0,\n",
    "    \"NFS\": 0,\n",
    "    \"UAV\": 0,\n",
    "    \"VOT20\": 0,\n",
    "    \"GOT10K_TEST\": 0\n",
    "}\n",
    "\n",
    "# TEST.UPH\n",
    "cfg[\"TEST\"][\"UPH\"] = {\n",
    "    \"DEFAULT\": 1,\n",
    "    \"LASOT\": 0,\n",
    "    \"LASOT_EXTENSION_SUBSET\": 0,\n",
    "    \"TRACKINGNET\": 0,\n",
    "    \"TNL2K\": 0,\n",
    "    \"NFS\": 0,\n",
    "    \"UAV\": 0,\n",
    "    \"VOT20\": 0,\n",
    "    \"GOT10K_TEST\": 0\n",
    "}\n",
    "\n",
    "# TEST.INTER\n",
    "cfg[\"TEST\"][\"INTER\"] = {\n",
    "    \"DEFAULT\": 999999,\n",
    "    \"LASOT\": 0,\n",
    "    \"LASOT_EXTENSION_SUBSET\": 0,\n",
    "    \"TRACKINGNET\": 0,\n",
    "    \"TNL2K\": 0,\n",
    "    \"NFS\": 0,\n",
    "    \"UAV\": 0,\n",
    "    \"VOT20\": 0,\n",
    "    \"GOT10K_TEST\": 0\n",
    "}\n",
    "\n",
    "# TEST.MB\n",
    "cfg[\"TEST\"][\"MB\"] = {\n",
    "    \"DEFAULT\": 500,\n",
    "    \"LASOT\": 0,\n",
    "    \"LASOT_EXTENSION_SUBSET\": 0,\n",
    "    \"TRACKINGNET\": 0,\n",
    "    \"TNL2K\": 0,\n",
    "    \"NFS\": 0,\n",
    "    \"UAV\": 0,\n",
    "    \"VOT20\": 0,\n",
    "    \"GOT10K_TEST\": 0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test config:  {'MODEL': {'ENCODER': {'TYPE': 'fastitpnt', 'DROP_PATH': 0.1, 'PRETRAIN_TYPE': './fast_itpn_tiny_1600e_1k.pt', 'USE_CHECKPOINT': False, 'STRIDE': 16, 'POS_TYPE': 'index', 'TOKEN_TYPE_INDICATE': True, 'INTERACTION_INDEXES': [[4, 7], [7, 10], [10, 13], [13, 16]], 'GRAD_CKPT': False}, 'NECK': {'N_LAYERS': 4, 'D_MODEL': 384, 'D_STATE': 16}, 'DECODER': {'TYPE': 'CENTER', 'NUM_CHANNELS': 256}}, 'TRAIN': {'LR': 0.0004, 'WEIGHT_DECAY': 0.0001, 'EPOCH': 300, 'LR_DROP_EPOCH': 240, 'BATCH_SIZE': 64, 'NUM_WORKER': 10, 'OPTIMIZER': 'ADAMW', 'ENCODER_MULTIPLIER': 0.1, 'FREEZE_ENCODER': False, 'ENCODER_OPEN': [], 'CE_WEIGHT': 1.0, 'GIOU_WEIGHT': 2.0, 'L1_WEIGHT': 5.0, 'PRINT_INTERVAL': 50, 'GRAD_CLIP_NORM': 0.1, 'FIX_BN': False, 'ENCODER_W': '', 'TYPE': 'normal', 'PRETRAINED_PATH': None, 'SCHEDULER': {'TYPE': 'step', 'DECAY_RATE': 0.1}}, 'DATA': {'MEAN': [0.485, 0.456, 0.406], 'STD': [0.229, 0.224, 0.225], 'MAX_SAMPLE_INTERVAL': 400, 'SAMPLER_MODE': 'order', 'LOADER': 'tracking', 'TRAIN': {'DATASETS_NAME': ['LASOT', 'GOT10K_vottrain', 'COCO17', 'TRACKINGNET', 'VASTTRACK'], 'DATASETS_RATIO': [1, 1, 1, 1, 1], 'SAMPLE_PER_EPOCH': 60000}, 'SEARCH': {'NUMBER': 2, 'SIZE': 224, 'FACTOR': 4.0, 'CENTER_JITTER': 3.5, 'SCALE_JITTER': 0.5}, 'TEMPLATE': {'NUMBER': 5, 'SIZE': 112, 'FACTOR': 2.0, 'CENTER_JITTER': 0, 'SCALE_JITTER': 0}}, 'TEST': {'TEMPLATE_FACTOR': 2.0, 'TEMPLATE_SIZE': 112, 'SEARCH_FACTOR': 4.0, 'SEARCH_SIZE': 224, 'EPOCH': 300, 'WINDOW': True, 'NUM_TEMPLATES': 5, 'UPT': {'DEFAULT': 1, 'LASOT': 0.8, 'LASOT_EXTENSION_SUBSET': 0.85, 'TRACKINGNET': 0.5, 'TNL2K': 0.5, 'NFS': 0.8, 'UAV': 0.2, 'VOT20': 0.4, 'GOT10K_TEST': 0}, 'UPH': {'DEFAULT': 1, 'LASOT': 0.88, 'LASOT_EXTENSION_SUBSET': 0.97, 'TRACKINGNET': 0.9, 'TNL2K': 0.9, 'NFS': 0.92, 'UAV': 0.91, 'VOT20': 0.94, 'GOT10K_TEST': 0}, 'INTER': {'DEFAULT': 999999, 'LASOT': 70, 'LASOT_EXTENSION_SUBSET': 50, 'TRACKINGNET': 20, 'TNL2K': 20, 'NFS': 90, 'UAV': 1, 'VOT20': 1, 'GOT10K_TEST': 0}, 'MB': {'DEFAULT': 500, 'LASOT': 500, 'LASOT_EXTENSION_SUBSET': 500, 'TRACKINGNET': 200, 'TNL2K': 500, 'NFS': 500, 'UAV': 400, 'VOT20': 500, 'GOT10K_TEST': 0}}}\n"
     ]
    }
   ],
   "source": [
    "#Params\n",
    "class TrackerParams:\n",
    "    \"\"\"Class for tracker parameters.\"\"\"\n",
    "    def set_default_values(self, default_vals: dict):\n",
    "        for name, val in default_vals.items():\n",
    "            if not hasattr(self, name):\n",
    "                setattr(self, name, val)\n",
    "\n",
    "    def get(self, name: str, *default):\n",
    "        \"\"\"Get a parameter value with the given name. If it does not exists, it return the default value given as a\n",
    "        second argument or returns an error if no default value is given.\"\"\"\n",
    "        if len(default) > 1:\n",
    "            raise ValueError('Can only give one default value.')\n",
    "\n",
    "        if not default:\n",
    "            return getattr(self, name)\n",
    "\n",
    "        return getattr(self, name, default[0])\n",
    "\n",
    "    def has(self, name: str):\n",
    "        \"\"\"Check if there exist a parameter with the given name.\"\"\"\n",
    "        return hasattr(self, name)\n",
    "\n",
    "def _update_config(base_cfg, exp_cfg):\n",
    "    if isinstance(base_cfg, dict) and isinstance(exp_cfg, dict):\n",
    "        for k, v in exp_cfg.items():\n",
    "            if k in base_cfg:\n",
    "                if not isinstance(v, dict):\n",
    "                    base_cfg[k] = v\n",
    "                else:\n",
    "                    _update_config(base_cfg[k], v)\n",
    "            else:\n",
    "                raise ValueError(\"{} not exist in config.py\".format(k))\n",
    "    else:\n",
    "        return\n",
    "\n",
    "def update_config_from_file(filename):\n",
    "    exp_config = None\n",
    "    with open(filename) as f:\n",
    "        exp_config = yaml.safe_load(f)\n",
    "        _update_config(cfg, exp_config)\n",
    "    \n",
    "def parameters(yaml_name: str):\n",
    "    params = TrackerParams()\n",
    "\n",
    "    yaml_file = \"mcitrack_t224.yaml\"\n",
    "    update_config_from_file(yaml_file)\n",
    "    params.cfg = cfg\n",
    "    print(\"test config: \", cfg)\n",
    "\n",
    "    params.yaml_name = yaml_name\n",
    "    # template and search region\n",
    "    params.template_factor = cfg[\"TEST\"][\"TEMPLATE_FACTOR\"]\n",
    "    params.template_size = cfg[\"TEST\"][\"TEMPLATE_SIZE\"]\n",
    "    params.search_factor = cfg[\"TEST\"][\"SEARCH_FACTOR\"]\n",
    "    params.search_size = cfg[\"TEST\"][\"SEARCH_SIZE\"]\n",
    "\n",
    "    # Network checkpoint path\n",
    "    params.checkpoint = \"MCITrack.trt\"\n",
    "    # whether to save boxes from all queries\n",
    "    params.save_all_boxes = False\n",
    "\n",
    "    return params\n",
    "\n",
    "params = parameters(\"./mcitrack_t224.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRT Tracker\n",
    "\n",
    "TRT_LOGGER = trt.Logger(trt.Logger.WARNING)\n",
    "\n",
    "class MCITRACK(BaseTracker):\n",
    "    def __init__(self, params):        \n",
    "        super(MCITRACK, self).__init__(params)\n",
    "        self.cfg = params.cfg        \n",
    "        # Загружаем engine (сохраняем как атрибут)\n",
    "        self.engine = self.load_engine(\"MCITrack.trt\")\n",
    "        if not self.engine:\n",
    "            raise RuntimeError(\"Не удалось загрузить TensorRT engine.\")    \n",
    "        # Сохраняем context\n",
    "        self.context = self.engine.create_execution_context()\n",
    "        \n",
    "        # Сохраняем выделенную память\n",
    "        self.initialize_memory()\n",
    "\n",
    "        self.fx_sz = self.cfg[\"TEST\"][\"SEARCH_SIZE\"] // self.cfg[\"MODEL\"][\"ENCODER\"][\"STRIDE\"]\n",
    "        if self.cfg[\"TEST\"][\"WINDOW\"] == True:  # for window penalty\n",
    "            self.output_window = hann2d(torch.tensor([self.fx_sz, self.fx_sz]).long(), centered=True).cuda()\n",
    "            \n",
    "        self.num_template = self.cfg[\"TEST\"][\"NUM_TEMPLATES\"]\n",
    "\n",
    "        self.preprocessor = Preprocessor()\n",
    "        self.state = None\n",
    "\n",
    "        if self.cfg[\"TEST\"][\"WINDOW\"] == True:  # for window penalty\n",
    "            self.output_window = hann2d(torch.tensor([self.fx_sz, self.fx_sz]).long(), centered=True).cuda()\n",
    "        self.frame_id = 0\n",
    "        self.save_all_boxes = params.save_all_boxes\n",
    "        self.z_dict1 = {}\n",
    "         # for update\n",
    "        self.h_state = [None] * self.cfg[\"MODEL\"][\"NECK\"][\"N_LAYERS\"]\n",
    "\n",
    "        self.memory_bank = self.cfg[\"TEST\"][\"MB\"][\"DEFAULT\"]\n",
    "        self.update_h_t = self.cfg[\"TEST\"][\"UPH\"][\"DEFAULT\"]\n",
    "        self.update_threshold = self.cfg[\"TEST\"][\"UPT\"][\"DEFAULT\"]\n",
    "        self.update_intervals = self.cfg[\"TEST\"][\"INTER\"][\"DEFAULT\"]\n",
    "        \n",
    "        print(f\"Engine initialized: {self.engine is not None}\")\n",
    "        print(f\"Context created: {self.context is not None}\")\n",
    "        print(f\"CUDA buffers allocated: {hasattr(self, 'd_template_list')}\")\n",
    "        print(f\"Current CUDA context: {cuda.Context.get_current()}\")\n",
    "          \n",
    "    def load_engine(self, engine_path):        \n",
    "        with open(engine_path, \"rb\") as engine_file, trt.Runtime(TRT_LOGGER) as runtime:\n",
    "            engine = runtime.deserialize_cuda_engine(engine_file.read())\n",
    "        return engine\n",
    "\n",
    "    def initialize_memory(self):        \n",
    "        # Входы\n",
    "        self.d_template_list = cuda.mem_alloc(int(np.prod([1, 3, 112, 112]) * np.dtype(np.float32).itemsize))\n",
    "        self.d_search_list = cuda.mem_alloc(int(np.prod([1, 3, 112, 112]) * np.dtype(np.float32).itemsize))\n",
    "        self.d_template_anno_list = cuda.mem_alloc(int(np.prod([1, 3, 112, 112]) * np.dtype(np.float32).itemsize))        \n",
    "        self.d_unsqueeze_3 = cuda.mem_alloc(int(np.prod([1, 3, 112, 112]) * np.dtype(np.float32).itemsize))\n",
    "        self.d_unsqueeze_4 = cuda.mem_alloc(int(np.prod([1, 3, 112, 112]) * np.dtype(np.float32).itemsize))\n",
    "        self.d_unsqueeze_5 = cuda.mem_alloc(int(np.prod([1, 3, 224, 224]) * np.dtype(np.float32).itemsize))\n",
    "        self.d_unsqueeze_6 = cuda.mem_alloc(int(np.prod([1, 4]) * np.dtype(np.float32).itemsize))\n",
    "        self.d_unsqueeze_7 = cuda.mem_alloc(int(np.prod([1, 4]) * np.dtype(np.float32).itemsize))\n",
    "        self.d_unsqueeze_8 = cuda.mem_alloc(int(np.prod([1, 4]) * np.dtype(np.float32).itemsize))\n",
    "        self.d_unsqueeze_9 = cuda.mem_alloc(int(np.prod([1, 4]) * np.dtype(np.float32).itemsize))\n",
    "        self.d_unsqueeze_10 = cuda.mem_alloc(int(np.prod([1, 4]) * np.dtype(np.float32).itemsize))\n",
    "        # Выходы\n",
    "        self.d_pred_boxes = cuda.mem_alloc(int(np.prod([1, 1, 4]) * np.dtype(np.float32).itemsize))\n",
    "        self.d_score_map = cuda.mem_alloc(int(np.prod([1, 1, 14, 14]) * np.dtype(np.float32).itemsize))\n",
    "        self.d_size_map = cuda.mem_alloc(int(np.prod([1, 2, 14, 14]) * np.dtype(np.float32).itemsize))\n",
    "        self.d_offset_map = cuda.mem_alloc(int(np.prod([1, 2, 14, 14]) * np.dtype(np.float32).itemsize))        \n",
    "        \n",
    "\n",
    "    def predict(self, input_feed):\n",
    "        \n",
    "        # print(\"\\nEngine bindings inspection:\")\n",
    "        # for i in range(self.engine.num_io_tensors):\n",
    "        #     name = self.engine.get_tensor_name(i)\n",
    "        #     shape = self.engine.get_tensor_shape(name)  # Передаем имя вместо индекса\n",
    "        #     dtype = self.engine.get_tensor_dtype(name)\n",
    "        #     print(f\"Binding {i}: {name} | Shape: {shape} | Type: {dtype}\")\n",
    "             \n",
    "        \n",
    "        \n",
    "        template_list = input_feed['template_list']\n",
    "        search_list = input_feed['search_list']\n",
    "        template_anno_list = input_feed['template_anno_list']\n",
    "        unsqueeze_3 = input_feed['onnx::Unsqueeze_3']\n",
    "        unsqueeze_4 = input_feed['onnx::Unsqueeze_4']\n",
    "        unsqueeze_5 = input_feed['onnx::Unsqueeze_5']\n",
    "        unsqueeze_6 = input_feed['onnx::Unsqueeze_6']\n",
    "        unsqueeze_7 = input_feed['onnx::Unsqueeze_7']\n",
    "        unsqueeze_8 = input_feed['onnx::Unsqueeze_8']\n",
    "        unsqueeze_9 = input_feed['onnx::Unsqueeze_9']\n",
    "        unsqueeze_10 = input_feed['onnx::Unsqueeze_10']\n",
    "\n",
    "\n",
    "                \n",
    "        # Копируем данные на GPU (используем выделенные буферы)\n",
    "        cuda.memcpy_htod(self.d_template_list, template_list.ravel())\n",
    "        cuda.memcpy_htod(self.d_search_list, search_list.ravel())\n",
    "        cuda.memcpy_htod(self.d_template_anno_list, template_anno_list.ravel())\n",
    "        cuda.memcpy_htod(self.d_unsqueeze_3, unsqueeze_3.ravel())\n",
    "        cuda.memcpy_htod(self.d_unsqueeze_4, unsqueeze_4.ravel())\n",
    "        cuda.memcpy_htod(self.d_unsqueeze_5, unsqueeze_5.ravel())\n",
    "        cuda.memcpy_htod(self.d_unsqueeze_6, unsqueeze_6.ravel())\n",
    "        cuda.memcpy_htod(self.d_unsqueeze_7, unsqueeze_7.ravel())\n",
    "        cuda.memcpy_htod(self.d_unsqueeze_8, unsqueeze_8.ravel())\n",
    "        cuda.memcpy_htod(self.d_unsqueeze_9, unsqueeze_9.ravel())\n",
    "        cuda.memcpy_htod(self.d_unsqueeze_10, unsqueeze_10.ravel())\n",
    "      \n",
    "      \n",
    "            # Пример проверки входных данных перед копированием\n",
    "        # print(\"Проверка входных данных:\")\n",
    "        # print(f\"template_list: min={template_list.min()}, max={template_list.max()}\")\n",
    "        # print(f\"search_list: min={search_list.min()}, max={search_list.max()}\")\n",
    "        # print(f\"unsqueeze_3: min={unsqueeze_3.min()}, max={unsqueeze_3.max()}\")\n",
    "        # print(f\"unsqueeze_4: min={unsqueeze_4.min()}, max={unsqueeze_4.max()}\")\n",
    "        # print(f\"unsqueeze_5: min={unsqueeze_5.min()}, max={unsqueeze_5.max()}\")\n",
    "        # print(f\"unsqueeze_6: {unsqueeze_6}\")\n",
    "        # print(f\"unsqueeze_7: {unsqueeze_7}\")\n",
    "        # print(f\"unsqueeze_8: {unsqueeze_8}\")\n",
    "        # print(f\"unsqueeze_9: {unsqueeze_9}\")\n",
    "        # print(f\"unsqueeze_10: {unsqueeze_10}\")\n",
    "      \n",
    "      \n",
    "           \n",
    "      \n",
    "        # Указываем правильные bindings        \n",
    "        self.context.execute_v2(bindings=[\n",
    "            int(self.d_template_list),     # 0\n",
    "            int(self.d_search_list),       # 1\n",
    "            int(self.d_template_anno_list),# 2\n",
    "            int(self.d_unsqueeze_3),       # 3\n",
    "            int(self.d_unsqueeze_4),       # 4\n",
    "            int(self.d_unsqueeze_5),       # 5\n",
    "            int(self.d_unsqueeze_6),       # 6\n",
    "            int(self.d_unsqueeze_7),       # 7\n",
    "            int(self.d_unsqueeze_8),       # 8\n",
    "            int(self.d_unsqueeze_9),       # 9\n",
    "            int(self.d_unsqueeze_10),      # 10\n",
    "            int(self.d_pred_boxes),       # 11\n",
    "            int(self.d_score_map),        # 12\n",
    "            int(self.d_size_map),         # 13\n",
    "            int(self.d_offset_map)        # 14\n",
    "        ])\n",
    "        \n",
    "        # Копируем результат с GPU\n",
    "        pred_boxes = np.empty([1, 1, 4], dtype=np.float32)\n",
    "        print(pred_boxes)\n",
    "        cuda.memcpy_dtoh(pred_boxes, self.d_pred_boxes)\n",
    "        \n",
    "        score_map = np.empty([1, 1, 14, 14], dtype=np.float32)\n",
    "        cuda.memcpy_dtoh(score_map, self.d_score_map)\n",
    "\n",
    "        size_map = np.empty([1, 2, 14, 14], dtype=np.float32)\n",
    "        cuda.memcpy_dtoh(size_map, self.d_size_map)\n",
    "\n",
    "        offset_map = np.empty([1, 2, 14, 14], dtype=np.float32)\n",
    "        cuda.memcpy_dtoh(offset_map, self.d_offset_map)\n",
    "\n",
    "        return pred_boxes, score_map, size_map, offset_map\n",
    "        \n",
    "    def initialize(self, image, info: dict):        \n",
    "        z_patch_arr, resize_factor = sample_target(image, info['init_bbox'], \n",
    "                                self.params.template_factor,\n",
    "                                output_sz=self.params.template_size)\n",
    "        \n",
    "        # Шаблон (хранится как тензор на CPU)\n",
    "        self.template = self.preprocessor.process(z_patch_arr)\n",
    "        self.template_list = [self.template] * self.num_template  # Список тензоров\n",
    "        \n",
    "        # Аннотации (хранятся как тензоры на CPU)\n",
    "        prev_box_crop = transform_image_to_crop(torch.tensor(info['init_bbox']),\n",
    "                                            torch.tensor(info['init_bbox']),\n",
    "                                            resize_factor,\n",
    "                                            torch.Tensor([self.params.template_size, \n",
    "                                                        self.params.template_size]),\n",
    "                                            normalize=True)\n",
    "        self.template_anno_list = [prev_box_crop.unsqueeze(0)] * self.num_template\n",
    "        \n",
    "        self.state = info['init_bbox']\n",
    "        self.frame_id = 0\n",
    "\n",
    "\n",
    "    def track(self, image, info: dict = None):              \n",
    "        H, W, _ = image.shape\n",
    "        self.frame_id += 1\n",
    "\n",
    "        x_patch_arr, resize_factor = sample_target(image, self.state, \n",
    "                                                self.params.search_factor,\n",
    "                                                output_sz=self.params.search_size)\n",
    "        \n",
    "        # Обрабатываем поисковое изображение и перемещаем на CPU\n",
    "        search = self.preprocessor.process(x_patch_arr)\n",
    "        search_list = [search]\n",
    "    \n",
    "        template_list_np = [t.cpu().numpy() for t in self.template_list]\n",
    "        search_list_np = [s.cpu().numpy() for s in search_list]\n",
    "        template_anno_list_np = [ta.cpu().numpy() for ta in self.template_anno_list]\n",
    "        \n",
    "        \n",
    "\n",
    "        all_inputs_np = template_list_np + search_list_np + template_anno_list_np\n",
    "\n",
    "        # # Получаем количество тензоров\n",
    "        num_bindings = self.engine.num_io_tensors\n",
    "\n",
    "        # # Собираем информацию о входах\n",
    "        input_names = []\n",
    "        expected_shapes = {}\n",
    "                \n",
    "        for i in range(num_bindings):\n",
    "            name = self.engine.get_tensor_name(i)\n",
    "            \n",
    "            # Проверяем, является ли входом\n",
    "            if hasattr(self.engine, 'binding_is_input'):\n",
    "                is_input = self.engine.binding_is_input(i)\n",
    "            else:\n",
    "                is_input = (self.engine.get_tensor_mode(name) == trt.TensorIOMode.INPUT)\n",
    "            \n",
    "            if is_input:\n",
    "                input_names.append(name)\n",
    "                # Получаем ожидаемую форму входа\n",
    "                if hasattr(self.engine, 'get_binding_shape'):\n",
    "                    expected_shapes[name] = tuple(self.engine.get_binding_shape(i))\n",
    "                else:\n",
    "                    expected_shapes[name] = tuple(self.engine.get_tensor_shape(name))\n",
    "\n",
    "        # print(\"\\nОжидаемые входы модели:\")\n",
    "        # for name, shape in expected_shapes.items():\n",
    "        #     print(f\"{name}: shape={shape}\")\n",
    "\n",
    "        # # Создаем словарь входных данных\n",
    "        input_feed = {name: data for name, data in zip(input_names, all_inputs_np)}\n",
    "\n",
    "        # print(\"\\nФактические передаваемые данные:\")\n",
    "        # for name, data in input_feed.items():\n",
    "        #     print(f\"{name}: shape={data.shape} vs Ожидается: {expected_shapes[name]}\")\n",
    "        #     if data.shape != expected_shapes[name]:\n",
    "        #         print(f\"  !!! Несоответствие размеров для входа {name} !!!\")\n",
    "                        \n",
    "\n",
    "        outputs = self.predict(input_feed)        \n",
    "         # Преобразуем выходы в torch тензоры и перемещаем на GPU\n",
    "        pred_boxes = torch.from_numpy(outputs[0]).cuda()\n",
    "        score_map = torch.from_numpy(outputs[1]).cuda()\n",
    "        size_map = torch.from_numpy(outputs[2]).cuda()\n",
    "        offset_map = torch.from_numpy(outputs[3]).cuda()\n",
    "\n",
    "        out_dict = {\n",
    "            'pred_boxes': pred_boxes,\n",
    "            'score_map': score_map,\n",
    "            'size_map': size_map,\n",
    "            'offset_map': offset_map\n",
    "        }\n",
    "\n",
    "        # Обработка результатов с учетом CUDA тензоров\n",
    "        pred_score_map = out_dict['score_map']\n",
    "        if self.cfg[\"TEST\"][\"WINDOW\"]:\n",
    "            # Убедимся, что output_window тоже на GPU\n",
    "            if not self.output_window.is_cuda:\n",
    "                self.output_window = self.output_window.cuda()\n",
    "            response = self.output_window * pred_score_map\n",
    "        else:\n",
    "            response = pred_score_map\n",
    "\n",
    "        pred_boxes, conf_score = cal_bbox(response, out_dict['size_map'], out_dict['offset_map'])\n",
    "        pred_boxes = pred_boxes.view(-1, 4)\n",
    "        \n",
    "        # Перемещаем вычисления на CPU для окончательного результата\n",
    "        pred_box = (pred_boxes.mean(dim=0) * self.params.search_size / resize_factor).cpu().numpy().tolist()\n",
    "        \n",
    "        self.state = clip_box(self.map_box_back(pred_box, resize_factor), H, W, margin=10)        \n",
    "\n",
    "        return {\n",
    "            \"target_bbox\": self.state,\n",
    "            \"best_score\": conf_score.cpu().item() if conf_score.is_cuda else conf_score\n",
    "        }\n",
    "    \n",
    "    def map_box_back(self, pred_box: list, resize_factor: float):\n",
    "        cx_prev, cy_prev = self.state[0] + 0.5 * self.state[2], self.state[1] + 0.5 * self.state[3]\n",
    "        cx, cy, w, h = pred_box\n",
    "        half_side = 0.5 * self.params.search_size / resize_factor\n",
    "        cx_real = cx + (cx_prev - half_side)\n",
    "        cy_real = cy + (cy_prev - half_side)\n",
    "        return [cx_real - 0.5 * w, cy_real - 0.5 * h, w, h]\n",
    "\n",
    "    def map_box_back_batch(self, pred_box: torch.Tensor, resize_factor: float):\n",
    "        cx_prev, cy_prev = self.state[0] + 0.5 * self.state[2], self.state[1] + 0.5 * self.state[3]\n",
    "        cx, cy, w, h = pred_box.unbind(-1)\n",
    "        half_side = 0.5 * self.params.search_size / resize_factor\n",
    "        cx_real = cx + (cx_prev - half_side)\n",
    "        cy_real = cy + (cy_prev - half_side)\n",
    "        return torch.stack([cx_real - 0.5 * w, cy_real - 0.5 * h, w, h], dim=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engine initialized: True\n",
      "Context created: True\n",
      "CUDA buffers allocated: True\n",
      "Current CUDA context: <pycuda._driver.Context object at 0x00000244E5554F90>\n"
     ]
    }
   ],
   "source": [
    "tracker = MCITRACK(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[1.e-45 3.e-45 2.e-44 2.e-44]]]\n",
      "[[[0. 0. 0. 0.]]]\n",
      "[[[0. 0. 0. 0.]]]\n",
      "[[[0. 0. 0. 0.]]]\n",
      "[[[0. 0. 0. 0.]]]\n",
      "[[[0. 0. 0. 0.]]]\n",
      "[[[0. 0. 0. 0.]]]\n",
      "[[[0. 0. 0. 0.]]]\n",
      "[[[0. 0. 0. 0.]]]\n",
      "[[[0. 0. 0. 0.]]]\n",
      "[[[0. 0. 0. 0.]]]\n",
      "[[[0. 0. 0. 0.]]]\n",
      "[[[0. 0. 0. 0.]]]\n",
      "[[[0. 0. 0. 0.]]]\n",
      "[[[0. 0. 0. 0.]]]\n",
      "[[[0. 0. 0. 0.]]]\n",
      "[[[0. 0. 0. 0.]]]\n",
      "[[[0. 0. 0. 0.]]]\n",
      "[[[0. 0. 0. 0.]]]\n",
      "[[[0. 0. 0. 0.]]]\n",
      "[[[0. 0. 0. 0.]]]\n",
      "[[[0. 0. 0. 0.]]]\n",
      "[[[0. 0. 0. 0.]]]\n",
      "[[[0. 0. 0. 0.]]]\n",
      "[[[0. 0. 0. 0.]]]\n",
      "[[[0. 0. 0. 0.]]]\n",
      "[[[0. 0. 0. 0.]]]\n",
      "[[[0. 0. 0. 0.]]]\n",
      "[[[0. 0. 0. 0.]]]\n",
      "[[[0. 0. 0. 0.]]]\n",
      "[[[0. 0. 0. 0.]]]\n",
      "[[[0. 0. 0. 0.]]]\n",
      "[[[0. 0. 0. 0.]]]\n",
      "[[[0. 0. 0. 0.]]]\n",
      "[[[0. 0. 0. 0.]]]\n",
      "[[[0. 0. 0. 0.]]]\n",
      "[[[0. 0. 0. 0.]]]\n",
      "[[[0. 0. 0. 0.]]]\n",
      "[[[0. 0. 0. 0.]]]\n",
      "[[[0. 0. 0. 0.]]]\n",
      "[[[0. 0. 0. 0.]]]\n",
      "[[[0. 0. 0. 0.]]]\n",
      "[[[0. 0. 0. 0.]]]\n",
      "[[[0. 0. 0. 0.]]]\n",
      "[[[0. 0. 0. 0.]]]\n",
      "[[[0. 0. 0. 0.]]]\n",
      "[[[0. 0. 0. 0.]]]\n",
      "[[[0. 0. 0. 0.]]]\n",
      "[[[0. 0. 0. 0.]]]\n",
      "[[[0. 0. 0. 0.]]]\n",
      "[[[0. 0. 0. 0.]]]\n",
      "[[[0. 0. 0. 0.]]]\n",
      "[[[0. 0. 0. 0.]]]\n",
      "[[[0. 0. 0. 0.]]]\n",
      "[[[0. 0. 0. 0.]]]\n",
      "[[[0. 0. 0. 0.]]]\n",
      "[[[0. 0. 0. 0.]]]\n",
      "[[[0. 0. 0. 0.]]]\n",
      "[[[0. 0. 0. 0.]]]\n",
      "[[[0. 0. 0. 0.]]]\n",
      "[[[0. 0. 0. 0.]]]\n",
      "[[[0. 0. 0. 0.]]]\n",
      "[[[0. 0. 0. 0.]]]\n",
      "[[[0. 0. 0. 0.]]]\n",
      "[[[0. 0. 0. 0.]]]\n",
      "[[[0. 0. 0. 0.]]]\n",
      "[[[0. 0. 0. 0.]]]\n",
      "[[[0. 0. 0. 0.]]]\n",
      "[[[0. 0. 0. 0.]]]\n",
      "[[[0. 0. 0. 0.]]]\n",
      "[[[0. 0. 0. 0.]]]\n",
      "[[[0. 0. 0. 0.]]]\n",
      "[[[0. 0. 0. 0.]]]\n",
      "[[[0. 0. 0. 0.]]]\n",
      "[[[0. 0. 0. 0.]]]\n",
      "[[[0. 0. 0. 0.]]]\n",
      "[[[0. 0. 0. 0.]]]\n",
      "[[[0. 0. 0. 0.]]]\n",
      "[[[0. 0. 0. 0.]]]\n",
      "[[[0. 0. 0. 0.]]]\n",
      "[[[0. 0. 0. 0.]]]\n",
      "[[[0. 0. 0. 0.]]]\n",
      "[[[0. 0. 0. 0.]]]\n",
      "[[[0. 0. 0. 0.]]]\n",
      "[[[0. 0. 0. 0.]]]\n",
      "[[[0. 0. 0. 0.]]]\n",
      "[[[0. 0. 0. 0.]]]\n",
      "[[[0. 0. 0. 0.]]]\n",
      "[[[0. 0. 0. 0.]]]\n",
      "[[[0. 0. 0. 0.]]]\n",
      "[[[0. 0. 0. 0.]]]\n",
      "[[[0. 0. 0. 0.]]]\n",
      "[[[0. 0. 0. 0.]]]\n",
      "[[[0. 0. 0. 0.]]]\n",
      "[[[0. 0. 0. 0.]]]\n",
      "[[[0. 0. 0. 0.]]]\n",
      "[[[0. 0. 0. 0.]]]\n",
      "[[[0. 0. 0. 0.]]]\n",
      "[[[0. 0. 0. 0.]]]\n",
      "[[[0. 0. 0. 0.]]]\n",
      "[[[0. 0. 0. 0.]]]\n",
      "[[[0. 0. 0. 0.]]]\n",
      "[[[0. 0. 0. 0.]]]\n",
      "[[[0. 0. 0. 0.]]]\n",
      "[[[0. 0. 0. 0.]]]\n",
      "[[[0. 0. 0. 0.]]]\n",
      "[[[0. 0. 0. 0.]]]\n",
      "[[[0. 0. 0. 0.]]]\n",
      "[[[0. 0. 0. 0.]]]\n",
      "[[[0. 0. 0. 0.]]]\n",
      "[[[0. 0. 0. 0.]]]\n",
      "[[[0. 0. 0. 0.]]]\n",
      "[[[0. 0. 0. 0.]]]\n",
      "[[[0. 0. 0. 0.]]]\n",
      "[[[0. 0. 0. 0.]]]\n",
      "[[[0. 0. 0. 0.]]]\n",
      "[[[0. 0. 0. 0.]]]\n",
      "[[[0. 0. 0. 0.]]]\n",
      "[[[0. 0. 0. 0.]]]\n",
      "[[[0. 0. 0. 0.]]]\n",
      "[[[0. 0. 0. 0.]]]\n",
      "[[[0. 0. 0. 0.]]]\n",
      "[[[0. 0. 0. 0.]]]\n",
      "[[[0. 0. 0. 0.]]]\n",
      "[[[0. 0. 0. 0.]]]\n",
      "[[[0. 0. 0. 0.]]]\n",
      "[[[0. 0. 0. 0.]]]\n",
      "[[[0. 0. 0. 0.]]]\n",
      "[[[0. 0. 0. 0.]]]\n",
      "[[[0. 0. 0. 0.]]]\n",
      "[[[0. 0. 0. 0.]]]\n",
      "[[[0. 0. 0. 0.]]]\n",
      "[[[0. 0. 0. 0.]]]\n",
      "[[[0. 0. 0. 0.]]]\n",
      "[[[0. 0. 0. 0.]]]\n",
      "[[[0. 0. 0. 0.]]]\n",
      "[[[0. 0. 0. 0.]]]\n",
      "[[[0. 0. 0. 0.]]]\n",
      "[[[0. 0. 0. 0.]]]\n",
      "[[[0. 0. 0. 0.]]]\n",
      "[[[0. 0. 0. 0.]]]\n",
      "[[[0. 0. 0. 0.]]]\n",
      "[[[0. 0. 0. 0.]]]\n",
      "[[[0. 0. 0. 0.]]]\n",
      "[[[0. 0. 0. 0.]]]\n",
      "[[[0. 0. 0. 0.]]]\n",
      "[[[0. 0. 0. 0.]]]\n",
      "[[[0. 0. 0. 0.]]]\n",
      "[[[0. 0. 0. 0.]]]\n",
      "[[[0. 0. 0. 0.]]]\n",
      "[[[0. 0. 0. 0.]]]\n",
      "[[[0. 0. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "# Трекинг по видео\n",
    "file = \"0516.mp4\"\n",
    "video = cv2.VideoCapture(file)\n",
    "ok, image = video.read()\n",
    "if not video.isOpened():\n",
    "    print(\"Could not open video\")\n",
    "    sys.exit()\n",
    "    \n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "x, y, w, h = cv2.selectROI( image, fromCenter=False)\n",
    "init_state = [x, y, w, h]\n",
    "def _build_init_info(box):\n",
    "            return {'init_bbox': box}\n",
    "tracker.initialize(image, _build_init_info(init_state))\n",
    "counter = 0\n",
    "while True:\n",
    "    ok, image = video.read()\n",
    "    if not ok:\n",
    "        break\n",
    "\n",
    "    # Конвертация для трекера\n",
    "    tracker_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Трекинг\n",
    "    start_time = time.time()\n",
    "    out = tracker.track(tracker_image)\n",
    "    state = [int(s) for s in out['target_bbox']]\n",
    "    #best_score = out[\"best_score\"]\n",
    "    best_score = 1\n",
    "    fps = 1 / (time.time() - start_time + 1e-6)\n",
    "\n",
    "    # Визуализация\n",
    "    display_image = image.copy()\n",
    "    x, y, w, h = state\n",
    "    \n",
    "    # Динамический цвет рамки в зависимости от уверенности\n",
    "    color = (0, 255, 0) if best_score > 0.7 else (0, 255, 255) if best_score > 0.4 else (0, 0, 255)\n",
    "    thickness = 3 if best_score > 0.7 else 2\n",
    "    \n",
    "    # Рисуем bounding box с увеличенными размерами\n",
    "    cv2.rectangle(display_image, (x, y), (x + w, y + h), color, thickness)\n",
    "    \n",
    "    # Добавляем информационный текст\n",
    "    cv2.putText(display_image, f\"Score: {best_score:.2f}\", (x, y-10), \n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)\n",
    "    cv2.putText(display_image, f\"FPS: {fps:.1f}\", (20, 40), \n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)\n",
    "    \n",
    "    cv2.imshow(\"tracking\", display_image)\n",
    "    \n",
    "    # Обработка клавиш\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    if key == 32:  # SPACE - переинициализация\n",
    "        x, y, w, h = cv2.selectROI(\"Select ROI\", image, fromCenter=False)\n",
    "        if w > 10 and h > 10:  # Минимальный размер ROI\n",
    "            init_state = [x, y, w, h]\n",
    "            print(\"Переинициализация...\")\n",
    "            tracker.initialize(tracker_image, _build_init_info(init_state))\n",
    "    elif key == 27:  # ESC - выход\n",
    "        break\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Метрики\n",
    "import numpy as np\n",
    "\n",
    "def iou(boxA, boxB):\n",
    "    # boxA, boxB: [x, y, w, h]\n",
    "    xA = max(boxA[0], boxB[0])\n",
    "    yA = max(boxA[1], boxB[1])\n",
    "    xB = min(boxA[0] + boxA[2], boxB[0] + boxB[2])\n",
    "    yB = min(boxA[1] + boxA[3], boxB[1] + boxB[3])\n",
    "\n",
    "    interW = max(0, xB - xA)\n",
    "    interH = max(0, yB - yA)\n",
    "    interArea = interW * interH\n",
    "\n",
    "    boxAArea = boxA[2] * boxA[3]\n",
    "    boxBArea = boxB[2] * boxB[3]\n",
    "    unionArea = boxAArea + boxBArea - interArea\n",
    "\n",
    "    if unionArea == 0:\n",
    "        return 0.0\n",
    "    return interArea / unionArea\n",
    "\n",
    "def precision(boxA, boxB):\n",
    "    # центры bbox\n",
    "    centerA = (boxA[0] + boxA[2]/2, boxA[1] + boxA[3]/2)\n",
    "    centerB = (boxB[0] + boxB[2]/2, boxB[1] + boxB[3]/2)\n",
    "    dist = np.sqrt((centerA[0] - centerB[0])**2 + (centerA[1] - centerB[1])**2)\n",
    "    return dist\n",
    "sr_thresh = 0.5\n",
    "prec_thresh = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GOT: val/GOT-10k_Val_000001\n",
      "FPS_TRT: 26.54\n",
      "Success Rate (SR@0.5)_TRT: 1.00\n",
      "Average Overlap (AO)_TRT: 0.93\n",
      "Precision @20px_TRT: 0.98\n"
     ]
    }
   ],
   "source": [
    "#Трекинг got10k с метриками ONNX\n",
    "import glob\n",
    "import time\n",
    "import  os\n",
    "gt_bboxes = []\n",
    "pred_bboxes = []\n",
    "seq_path = \"val/GOT-10k_Val_000001\"\n",
    "txt_files = glob.glob(os.path.join(seq_path, '*.txt'))\n",
    "if not txt_files:\n",
    "    raise FileNotFoundError(f\"No .txt files found in {seq_path}\")\n",
    "\n",
    "img_files = sorted(glob.glob(os.path.join(seq_path, '*.jpg')))\n",
    "with open(txt_files[0], 'r') as f:\n",
    "    gt_bboxes = [list(map(float, line.strip().split(','))) for line in f]\n",
    "\n",
    "# Получаем размер первого изображения\n",
    "sample_img = cv2.imread(img_files[0])\n",
    "if sample_img is None:\n",
    "    raise ValueError(f\"Failed to read sample image: {img_files[0]}\")\n",
    "\n",
    "#height, width = sample_img.shape[:2]\n",
    "#fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "#output_filename = f\"{seq_path.split('/')[-1]}_output.avi\"\n",
    "#video_vriter = cv2.VideoWriter(output_filename, fourcc, 10, (width, height))  \n",
    "\n",
    "assert len(img_files) == len(gt_bboxes), \"Количество кадров и bbox'ов не совпадает\"\n",
    "\n",
    "x, y, w, h = map(int, gt_bboxes[0])\n",
    "init_state = [x, y, w, h]\n",
    "\n",
    "def _build_init_info(box):\n",
    "            return {'init_bbox': box}\n",
    "\n",
    "counter = 0\n",
    "\n",
    "\n",
    "tracker.initialize(sample_img, _build_init_info(init_state))\n",
    "\n",
    "start_time = time.time()  # Начало замера\n",
    "\n",
    "for img_file, bbox in zip(img_files, gt_bboxes):\n",
    "        \n",
    "        # Читаем изображение\n",
    "        img = cv2.imread(img_file)\n",
    "        if img is None:\n",
    "            print(f\"Не удалось загрузить изображение: {img_file}\")\n",
    "            continue\n",
    "        \n",
    "        \n",
    "        out  = tracker.track(img)\n",
    "        state = [int(s) for s in out['target_bbox']]   \n",
    "                           \n",
    "        # Рисуем bounding box        \n",
    "        x, y, w, h = [int(x) for x in state]\n",
    "\n",
    "        cv2.rectangle(img, (x, y), (x + w, y + h), (0, 0, 200), 2)\n",
    "        \n",
    "        x1, y1, w1, h1 = map(int, bbox)\n",
    "        cv2.rectangle(img, (x1, y1), (x1+w1, y1+h1), (0, 200, 0), 2)\n",
    "        bbox_pred = x, y, w, h\n",
    "        \n",
    "        gt_bboxes.append(bbox)\n",
    "        pred_bboxes.append(bbox_pred)\n",
    "\n",
    "        #cv2.imshow(seq_path, img)\n",
    "        #video_vriter.write(img)\n",
    "        counter+=1\n",
    "\n",
    "\n",
    "        # Выход по нажатию 'q' или ESC\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "        if key == ord('q') or key == 27:\n",
    "            break\n",
    "       \n",
    "        \n",
    "                \n",
    "end_time = time.time()    # Конец замера    \n",
    "total_frames = counter       # Общее количество обработанных кадров\n",
    "total_time = end_time - start_time\n",
    "fps = total_frames / total_time\n",
    "ious = [iou(gt, pred) for gt, pred in zip(gt_bboxes, pred_bboxes)]\n",
    "ao = np.mean(ious)\n",
    "sr = np.mean([1 if val >= sr_thresh else 0 for val in ious])\n",
    "precisions = [precision(gt, pred) for gt, pred in zip(gt_bboxes, pred_bboxes)]\n",
    "prec = np.mean([1 if d <= prec_thresh else 0 for d in precisions])\n",
    "\n",
    "print(f\"GOT: {seq_path}\")\n",
    "print(f\"FPS_TRT: {fps:.2f}\")\n",
    "print(f'Success Rate (SR@0.5)_TRT: {sr:.2f}')\n",
    "print(f'Average Overlap (AO)_TRT: {ao:.2f}')\n",
    "print(f'Precision @20px_TRT: {prec:.2f}')\n",
    "\n",
    "#cv2.destroyAllWindows()\n",
    "#video_vriter.release()\n",
    "#print(f\"Video saved as: {output_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Начинаем конвертацию MCITrack1.onnx в TensorRT engine\n",
      "Загрузка ONNX модели из файла MCITrack1.onnx\n",
      "Строим TensorRT engine. Это может занять некоторое время...\n",
      "Сохранение engine в файл MCITrack2.trt\n",
      "Конвертация успешно завершена!\n",
      "TensorRT engine успешно сохранен в MCITrack2.trt\n"
     ]
    }
   ],
   "source": [
    "#Создание TRT-модели\n",
    "import tensorrt as trt\n",
    "import os\n",
    "\n",
    "def build_engine(onnx_file_path, engine_file_path, precision=\"FP32\", max_batch_size=1, max_workspace_size=1<<30):\n",
    "    \"\"\"\n",
    "    Конвертирует ONNX модель в TensorRT engine (для TensorRT 8.0+)\n",
    "    \n",
    "    Параметры:\n",
    "        onnx_file_path: путь к файлу ONNX модели\n",
    "        engine_file_path: путь для сохранения engine\n",
    "        precision: \"FP32\", \"FP16\" или \"INT8\"\n",
    "        max_batch_size: максимальный размер батча\n",
    "        max_workspace_size: максимальный размер рабочей памяти (в байтах)\n",
    "    \"\"\"\n",
    "    TRT_LOGGER = trt.Logger(trt.Logger.WARNING)\n",
    "    explicit_batch = 1 << (int)(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)\n",
    "    \n",
    "    # Инициализация builder, сети и конфига\n",
    "    with trt.Builder(TRT_LOGGER) as builder, \\\n",
    "         builder.create_network(explicit_batch) as network, \\\n",
    "         trt.OnnxParser(network, TRT_LOGGER) as parser, \\\n",
    "         builder.create_builder_config() as config:\n",
    "        \n",
    "        # Установка максимального размера рабочей памяти\n",
    "        config.set_memory_pool_limit(trt.MemoryPoolType.WORKSPACE, max_workspace_size)\n",
    "        \n",
    "        # Установка precision mode\n",
    "        if precision == \"FP16\":\n",
    "            config.set_flag(trt.BuilderFlag.FP16)\n",
    "        elif precision == \"INT8\":\n",
    "            config.set_flag(trt.BuilderFlag.FP16)\n",
    "            config.set_flag(trt.BuilderFlag.INT8)\n",
    "            # Для INT8 требуется калибровочный набор данных\n",
    "            # config.int8_calibrator = ... \n",
    "        \n",
    "        # Проверка поддержки precision\n",
    "        if precision == \"FP16\" and not builder.platform_has_fast_fp16:\n",
    "            print(\"FP16 не поддерживается на этой платформе\")\n",
    "        if precision == \"INT8\" and not builder.platform_has_fast_int8:\n",
    "            print(\"INT8 не поддерживается на этой платформе\")\n",
    "        \n",
    "        # Загрузка ONNX модели\n",
    "        print(f\"Загрузка ONNX модели из файла {onnx_file_path}\")\n",
    "        with open(onnx_file_path, \"rb\") as model:\n",
    "            if not parser.parse(model.read()):\n",
    "                print(\"Ошибка при парсинге ONNX модели\")\n",
    "                for error in range(parser.num_errors):\n",
    "                    print(parser.get_error(error))\n",
    "                return None\n",
    "        \n",
    "        # Оптимизация и создание engine\n",
    "        print(\"Строим TensorRT engine. Это может занять некоторое время...\")\n",
    "        \n",
    "        # Установка максимального размера батча\n",
    "        # В новых версиях это делается через профиль оптимизации\n",
    "        profile = builder.create_optimization_profile()\n",
    "        input_tensor = network.get_input(0)\n",
    "        input_shape = input_tensor.shape\n",
    "        profile.set_shape(input_tensor.name, \n",
    "                        (1, *input_shape[1:]),  # min shape\n",
    "                        (max_batch_size, *input_shape[1:]),  # opt shape\n",
    "                        (max_batch_size, *input_shape[1:]))  # max shape\n",
    "        config.add_optimization_profile(profile)\n",
    "        \n",
    "        # Построение engine\n",
    "        serialized_engine = builder.build_serialized_network(network, config)\n",
    "        \n",
    "        if serialized_engine is None:\n",
    "            print(\"Ошибка при создании engine\")\n",
    "            return None\n",
    "        \n",
    "        # Сохранение engine в файл\n",
    "        print(f\"Сохранение engine в файл {engine_file_path}\")\n",
    "        with open(engine_file_path, \"wb\") as f:\n",
    "            f.write(serialized_engine)\n",
    "        \n",
    "        # Возвращаем десериализованный engine для использования\n",
    "        runtime = trt.Runtime(TRT_LOGGER)\n",
    "        return runtime.deserialize_cuda_engine(serialized_engine)\n",
    "\n",
    "def convert_onnx_to_tensorrt(onnx_path, trt_path, precision=\"FP32\"):\n",
    "    \"\"\"\n",
    "    Основная функция конвертации\n",
    "    \"\"\"\n",
    "    # Проверка существования ONNX файла\n",
    "    if not os.path.exists(onnx_path):\n",
    "        print(f\"Файл {onnx_path} не найден\")\n",
    "        return False\n",
    "    \n",
    "    print(f\"Начинаем конвертацию {onnx_path} в TensorRT engine\")\n",
    "    \n",
    "    # Построение engine\n",
    "    engine = build_engine(onnx_path, trt_path, precision)\n",
    "    \n",
    "    if engine is not None:\n",
    "        print(\"Конвертация успешно завершена!\")\n",
    "        return True\n",
    "    else:\n",
    "        print(\"Конвертация не удалась\")\n",
    "        return False\n",
    "\n",
    "# Пример использования\n",
    "if __name__ == \"__main__\":\n",
    "    onnx_model_path = \"MCITrack1.onnx\"  # Путь к вашей ONNX модели\n",
    "    trt_engine_path = \"MCITrack2.trt\"   # Куда сохранить TensorRT engine\n",
    "    \n",
    "    # Доступные precision: \"FP32\", \"FP16\", \"INT8\"\n",
    "    precision_mode = \"FP16\"  \n",
    "    \n",
    "    success = convert_onnx_to_tensorrt(onnx_model_path, trt_engine_path, precision_mode)\n",
    "    \n",
    "    if success:\n",
    "        print(f\"TensorRT engine успешно сохранен в {trt_engine_path}\")\n",
    "    else:\n",
    "        print(\"Ошибка при конвертации модели\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 2/181 [00:07<11:54,  3.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************** Прервано на 3! ********************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Path</th>\n",
       "      <th>FPS</th>\n",
       "      <th>Success Rate (SR@0.5)</th>\n",
       "      <th>Average Overlap (AO)</th>\n",
       "      <th>Precision @20px</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GOT-10k_Val_000001</td>\n",
       "      <td>27.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.929659</td>\n",
       "      <td>0.983333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GOT-10k_Val_000002</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.325000</td>\n",
       "      <td>0.419739</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GOT-10k_Val_000003</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003690</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Average</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.441667</td>\n",
       "      <td>0.451029</td>\n",
       "      <td>0.327778</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Path   FPS  Success Rate (SR@0.5)  Average Overlap (AO)  \\\n",
       "0  GOT-10k_Val_000001  27.0               1.000000              0.929659   \n",
       "1  GOT-10k_Val_000002  32.0               0.325000              0.419739   \n",
       "2  GOT-10k_Val_000003  31.0               0.000000              0.003690   \n",
       "3             Average  30.0               0.441667              0.451029   \n",
       "\n",
       "   Precision @20px  \n",
       "0         0.983333  \n",
       "1         0.000000  \n",
       "2         0.000000  \n",
       "3         0.327778  "
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Проход по всему got10k\n",
    "import glob\n",
    "import time\n",
    "import  os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "gt_bboxes = []\n",
    "pred_bboxes = []\n",
    "metrics = pd.DataFrame(columns=['Path', 'FPS', 'Success Rate (SR@0.5)', \"Average Overlap (AO)\", \"Precision @20px\"])\n",
    "base_dir = \"val/\"\n",
    "folders = os.listdir(f'{base_dir}')\n",
    "counter_test = 0\n",
    "for folder in tqdm(folders):\n",
    "    if folder == \"val/list.txt\":\n",
    "        print(f\"{'*' * 20} Завершено! {'*' * 20}\")\n",
    "        break\n",
    "    \n",
    "    seq_path = os.path.join(base_dir, folder)\n",
    "    txt_files = glob.glob(os.path.join(seq_path, '*.txt'))\n",
    "    if not txt_files:\n",
    "        raise FileNotFoundError(f\"No .txt files found in {seq_path}\")\n",
    "\n",
    "    img_files = sorted(glob.glob(os.path.join(seq_path, '*.jpg')))\n",
    "    with open(txt_files[0], 'r') as f:\n",
    "        gt_bboxes = [list(map(float, line.strip().split(','))) for line in f]\n",
    "\n",
    "    # Получаем размер первого изображения\n",
    "    sample_img = cv2.imread(img_files[0])\n",
    "    if sample_img is None:\n",
    "        raise ValueError(f\"Failed to read sample image: {img_files[0]}\")  \n",
    "\n",
    "    assert len(img_files) == len(gt_bboxes), \"Количество кадров и bbox'ов не совпадает\"\n",
    "\n",
    "    x, y, w, h = map(int, gt_bboxes[0])\n",
    "    init_state = [x, y, w, h]\n",
    "\n",
    "    def _build_init_info(box):\n",
    "                return {'init_bbox': box}\n",
    "\n",
    "    counter = 0\n",
    "    counter_test += 1\n",
    "\n",
    "\n",
    "    tracker.initialize(sample_img, _build_init_info(init_state))\n",
    "\n",
    "    start_time = time.time()  # Начало замера\n",
    "\n",
    "    for img_file, bbox in zip(img_files, gt_bboxes):\n",
    "            \n",
    "            # Читаем изображение\n",
    "            img = cv2.imread(img_file)\n",
    "            if img is None:\n",
    "                print(f\"Не удалось загрузить изображение: {img_file}\")\n",
    "                continue\n",
    "                        \n",
    "            out  = tracker.track(img)\n",
    "            state = [int(s) for s in out['target_bbox']]   \n",
    "                            \n",
    "            # Рисуем bounding box        \n",
    "            x, y, w, h = [int(x) for x in state]\n",
    "\n",
    "            cv2.rectangle(img, (x, y), (x + w, y + h), (0, 0, 200), 2)\n",
    "            \n",
    "            x1, y1, w1, h1 = map(int, bbox)\n",
    "            cv2.rectangle(img, (x1, y1), (x1+w1, y1+h1), (0, 200, 0), 2)\n",
    "            bbox_pred = x, y, w, h\n",
    "            \n",
    "            gt_bboxes.append(bbox)\n",
    "            pred_bboxes.append(bbox_pred)\n",
    "   \n",
    "            counter+=1\n",
    "\n",
    "            # Выход по нажатию 'q' или ESC\n",
    "            key = cv2.waitKey(1) & 0xFF\n",
    "            if key == ord('q') or key == 27:\n",
    "                break\n",
    "        \n",
    "            \n",
    "                    \n",
    "    end_time = time.time()    # Конец замера    \n",
    "    total_frames = counter       # Общее количество обработанных кадров\n",
    "    total_time = end_time - start_time\n",
    "    fps = round(total_frames / total_time)\n",
    "    ious = [iou(gt, pred) for gt, pred in zip(gt_bboxes, pred_bboxes)]\n",
    "    ao = np.mean(ious)\n",
    "    sr = np.mean([1 if val >= sr_thresh else 0 for val in ious])\n",
    "    precisions = [precision(gt, pred) for gt, pred in zip(gt_bboxes, pred_bboxes)]\n",
    "    prec = np.mean([1 if d <= prec_thresh else 0 for d in precisions])\n",
    "       \n",
    "    if metrics.empty:\n",
    "        metrics = pd.DataFrame(dict(zip(metrics.columns,\n",
    "        [folder, fps, sr, ao, prec])), index=[0])\n",
    "    else:\n",
    "        metrics = metrics._append(pd.Series(dict(zip(metrics.columns,\n",
    "        [folder, fps, sr, ao, prec]))), ignore_index=True)\n",
    "    if counter_test == 3:\n",
    "        print(f\"{'*' * 20} Прервано на 3! {'*' * 20}\")\n",
    "        break\n",
    "\n",
    "metrics = metrics._append(pd.Series(dict(zip(metrics.columns,\n",
    "        [\"Average\", metrics['FPS'].mean(), metrics['Success Rate (SR@0.5)'].mean(), metrics['Average Overlap (AO)'].mean(), metrics['Precision @20px'].mean()]))), ignore_index=True)\n",
    "metrics   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
