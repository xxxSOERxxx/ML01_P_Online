{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' TRT_LOGGER = trt.Logger(trt.Logger.WARNING)\\n\\ndef load_engine(trt_runtime, engine_path):\\n    with open(engine_path, \"rb\") as f:\\n        engine_data = f.read()\\n    return trt_runtime.deserialize_cuda_engine(engine_data)\\n\\ntrt_runtime = trt.Runtime(TRT_LOGGER)\\nengine = load_engine(trt_runtime, \"MCITrack.trt\")\\nengine '"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorrt as trt\n",
    "import pycuda.driver as cuda\n",
    "import pycuda.autoinit  # автоматически инициализирует CUDA контекст\n",
    "import torch\n",
    "import cv2\n",
    "import sys\n",
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "import yaml\n",
    "import torch\n",
    "import numpy as np\n",
    "import onnx\n",
    "import onnxruntime as ort\n",
    "\n",
    "\n",
    "\"\"\" TRT_LOGGER = trt.Logger(trt.Logger.WARNING)\n",
    "\n",
    "def load_engine(trt_runtime, engine_path):\n",
    "    with open(engine_path, \"rb\") as f:\n",
    "        engine_data = f.read()\n",
    "    return trt_runtime.deserialize_cuda_engine(engine_data)\n",
    "\n",
    "trt_runtime = trt.Runtime(TRT_LOGGER)\n",
    "engine = load_engine(trt_runtime, \"MCITrack.trt\")\n",
    "engine \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" def cal_bbox(score_map_ctr, size_map, offset_map, return_score=True):\n",
    "    \n",
    "    # 2. Получаем размеры feature map\n",
    "    feat_h, feat_w = score_map_ctr.shape[-2], score_map_ctr.shape[-1]\n",
    "    \n",
    "    # 3. Находим позицию с максимальным score (современный способ)\n",
    "    max_score, flat_idx = torch.max(score_map_ctr.flatten(1), dim=1)\n",
    "    idx = flat_idx.unsqueeze(1)\n",
    "    idx_y = torch.div(flat_idx, feat_w, rounding_mode='floor')\n",
    "    idx_x = flat_idx % feat_w\n",
    "    \n",
    "    # 4. Подготовка индексов для gather\n",
    "    gather_idx = idx.unsqueeze(1).expand(-1, 2, -1)\n",
    "    \n",
    "    # 5. Обработка size_map (расширяем если 1 канал)\n",
    "    if size_map.size(1) == 1:\n",
    "        size_map = size_map.expand(-1, 2, -1, -1)\n",
    "    \n",
    "    # 6. Получаем размеры и смещения\n",
    "    try:\n",
    "        size = size_map.flatten(2).gather(2, gather_idx)\n",
    "        offset = offset_map.flatten(2).gather(2, gather_idx).squeeze(-1)\n",
    "    except RuntimeError as e:\n",
    "        print(\"Ошибка размерностей:\")\n",
    "        print(f\"score_map_ctr: {score_map_ctr.shape}\")\n",
    "        print(f\"size_map: {size_map.shape}\")\n",
    "        print(f\"offset_map: {offset_map.shape}\")\n",
    "        print(f\"gather_idx: {gather_idx.shape}\")\n",
    "        raise\n",
    "    \n",
    "    # 7. Формируем bbox (cx, cy, w, h)\n",
    "    bbox = torch.cat([\n",
    "        (idx_x.to(torch.float) + offset[:, 0:1]) / feat_w,\n",
    "        (idx_y.to(torch.float) + offset[:, 1:2]) / feat_h,\n",
    "        size.squeeze(-1)\n",
    "    ], dim=1)\n",
    "    \n",
    "    return (bbox, max_score) if return_score else bbox \"\"\"\n",
    "\n",
    "def cal_bbox(score_map_ctr, size_map, offset_map, return_score=True):\n",
    "        feat_sz = 14\n",
    "        max_score, idx = torch.max(score_map_ctr.flatten(1), dim=1, keepdim=True) # score_map_ctr.flatten(1): torch.Size([32, 256]) idx: torch.Size([32, 1]) max_score: torch.Size([32, 1])\n",
    "        idx_y = torch.div(idx, feat_sz, rounding_mode='floor')\n",
    "        idx_x = idx % feat_sz\n",
    "       \n",
    "        \n",
    "\n",
    "        idx = idx.unsqueeze(1).expand(idx.shape[0], 2, 1)\n",
    "        size = size_map.flatten(2).gather(dim=2, index=idx) # size_map: torch.Size([32, 2, 16, 16])  size_map.flatten(2): torch.Size([32, 2, 256])\n",
    "        offset = offset_map.flatten(2).gather(dim=2, index=idx).squeeze(-1)\n",
    "\n",
    "        # bbox = torch.cat([idx_x - size[:, 0] / 2, idx_y - size[:, 1] / 2,\n",
    "        #                   idx_x + size[:, 0] / 2, idx_y + size[:, 1] / 2], dim=1) / self.feat_sz\n",
    "        # cx, cy, w, h\n",
    "        bbox = torch.cat([(idx_x.to(torch.float) + offset[:, :1]) / feat_sz,\n",
    "                          (idx_y.to(torch.float) + offset[:, 1:]) / feat_sz,\n",
    "                          size.squeeze(-1)], dim=1)\n",
    "\n",
    "        if return_score:\n",
    "            return bbox, max_score\n",
    "        return (bbox, max_score) if return_score else bbox\n",
    "        \n",
    "class Preprocessor(object):\n",
    "    def __init__(self):\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.mean = torch.tensor([0.485, 0.456, 0.406]).view((1, 3, 1, 1)).to(self.device)\n",
    "        self.std = torch.tensor([0.229, 0.224, 0.225]).view((1, 3, 1, 1)).to(self.device)\n",
    "        self.mm_mean = torch.tensor([0.485, 0.456, 0.406, 0.485, 0.456, 0.406]).view((1, 6, 1, 1)).to(self.device)\n",
    "        self.mm_std = torch.tensor([0.229, 0.224, 0.225, 0.229, 0.224, 0.225]).view((1, 6, 1, 1)).to(self.device)\n",
    "\n",
    "    def process(self, img_arr: np.ndarray):\n",
    "        if img_arr.shape[-1] == 6:\n",
    "            mean = self.mm_mean\n",
    "            std = self.mm_std\n",
    "        else:\n",
    "            mean = self.mean\n",
    "            std = self.std\n",
    "        # Deal with the image patch\n",
    "        img_tensor = torch.tensor(img_arr).to(self.device).float().permute((2,0,1)).unsqueeze(dim=0)        \n",
    "        img_tensor_norm = ((img_tensor / 255.0) - mean) / std  # (1,3,H,W)\n",
    "        return img_tensor_norm\n",
    "    \n",
    "def hann1d(sz: int, centered = True) -> torch.Tensor:\n",
    "    \"\"\"1D cosine window.\"\"\"\n",
    "    if centered:\n",
    "        return 0.5 * (1 - torch.cos((2 * math.pi / (sz + 1)) * torch.arange(1, sz + 1).float()))\n",
    "    w = 0.5 * (1 + torch.cos((2 * math.pi / (sz + 2)) * torch.arange(0, sz//2 + 1).float()))\n",
    "    return torch.cat([w, w[1:sz-sz//2].flip((0,))])\n",
    "    \n",
    "def hann2d(sz: torch.Tensor, centered = True) -> torch.Tensor:\n",
    "    \"\"\"2D cosine window.\"\"\"\n",
    "    return hann1d(sz[0].item(), centered).reshape(1, 1, -1, 1) * hann1d(sz[1].item(), centered).reshape(1, 1, 1, -1)    \n",
    "\n",
    "def sample_target(im, target_bb, search_area_factor, output_sz=None):\n",
    "   \n",
    "    if not isinstance(target_bb, list):\n",
    "        x, y, w, h = target_bb.tolist()\n",
    "    else:\n",
    "        x, y, w, h = target_bb\n",
    "    # Crop image\n",
    "    crop_sz = math.ceil(math.sqrt(w * h) * search_area_factor)\n",
    "\n",
    "    if crop_sz < 1:\n",
    "        raise Exception('Too small bounding box.')\n",
    "\n",
    "    x1 = round(x + 0.5 * w - crop_sz * 0.5)\n",
    "    x2 = x1 + crop_sz\n",
    "\n",
    "    y1 = round(y + 0.5 * h - crop_sz * 0.5)\n",
    "    y2 = y1 + crop_sz\n",
    "\n",
    "    x1_pad = max(0, -x1)\n",
    "    x2_pad = max(x2 - im.shape[1] + 1, 0)\n",
    "\n",
    "    y1_pad = max(0, -y1)\n",
    "    y2_pad = max(y2 - im.shape[0] + 1, 0)\n",
    "\n",
    "    # Crop target\n",
    "    im_crop = im[y1 + y1_pad:y2 - y2_pad, x1 + x1_pad:x2 - x2_pad, :]\n",
    "\n",
    "    # Pad\n",
    "    im_crop_padded = cv2.copyMakeBorder(im_crop, y1_pad, y2_pad, x1_pad, x2_pad, cv2.BORDER_CONSTANT)\n",
    "    # deal with attention mask\n",
    "    H, W, _ = im_crop_padded.shape\n",
    "\n",
    "    if output_sz is not None:\n",
    "        resize_factor = output_sz / crop_sz\n",
    "        im_crop_padded = cv2.resize(im_crop_padded, (output_sz, output_sz))\n",
    "\n",
    "        return im_crop_padded, resize_factor\n",
    "\n",
    "    else:\n",
    "        return im_crop_padded, 1.0\n",
    "def transform_image_to_crop(box_in: torch.Tensor, box_extract: torch.Tensor, resize_factor: float,\n",
    "                            crop_sz: torch.Tensor, normalize=False) -> torch.Tensor:\n",
    "   \n",
    "    box_extract_center = box_extract[0:2] + 0.5 * box_extract[2:4]\n",
    "\n",
    "    box_in_center = box_in[0:2] + 0.5 * box_in[2:4]\n",
    "\n",
    "    box_out_center = (crop_sz - 1) / 2 + (box_in_center - box_extract_center) * resize_factor\n",
    "    box_out_wh = box_in[2:4] * resize_factor\n",
    "\n",
    "    box_out = torch.cat((box_out_center - 0.5 * box_out_wh, box_out_wh))\n",
    "    if normalize:\n",
    "        return box_out / (crop_sz[0]-1)\n",
    "    else:\n",
    "        return box_out\n",
    "def clip_box(box: list, H, W, margin=0):\n",
    "    x1, y1, w, h = box\n",
    "    x2, y2 = x1 + w, y1 + h\n",
    "    x1 = min(max(0, x1), W-margin)\n",
    "    x2 = min(max(margin, x2), W)\n",
    "    y1 = min(max(0, y1), H-margin)\n",
    "    y2 = min(max(margin, y2), H)\n",
    "    w = max(margin, x2-x1)\n",
    "    h = max(margin, y2-y1)\n",
    "    return [x1, y1, w, h]\n",
    "\n",
    "class BaseTracker():\n",
    "    \"\"\"Base class for all trackers.\"\"\"\n",
    "\n",
    "    def __init__(self, params):\n",
    "        self.params = params\n",
    "        self.visdom = None\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    def predicts_segmentation_mask(self):\n",
    "        return False\n",
    "\n",
    "    def initialize(self, image, info: dict) -> dict:\n",
    "        \"\"\"Overload this function in your tracker. This should initialize the model.\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def track(self, image, info: dict = None) -> dict:\n",
    "        \"\"\"Overload this function in your tracker. This should track in the frame and update the model.\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def visdom_draw_tracking(self, image, box, segmentation=None):\n",
    "        # Упрощенная обработка box без OrderedDict\n",
    "        if isinstance(box, dict):  # Проверяем на обычный dict вместо OrderedDict\n",
    "            box = list(box.values())  # Берем только значения\n",
    "        elif not isinstance(box, (list, tuple)):  # Если не коллекция\n",
    "            box = (box,)  # Превращаем в кортеж\n",
    "        \n",
    "        # Визуализация\n",
    "        if segmentation is None:\n",
    "            self.visdom.register((image, *box), 'Tracking', 1, 'Tracking')\n",
    "        else:\n",
    "            self.visdom.register((image, *box, segmentation), 'Tracking', 1, 'Tracking')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = {}\n",
    "\n",
    "# MODEL\n",
    "cfg[\"MODEL\"] = {}\n",
    "\n",
    "# MODEL.ENCODER\n",
    "cfg[\"MODEL\"][\"ENCODER\"] = {\n",
    "    \"TYPE\": \"dinov2_vitb14\",  # encoder model\n",
    "    \"DROP_PATH\": 0,\n",
    "    \"PRETRAIN_TYPE\": \"mae\",  # mae, default, or scratch. This parameter is not activated for dinov2.\n",
    "    \"USE_CHECKPOINT\": False,  # to save the memory.\n",
    "    \"STRIDE\": 14,\n",
    "    \"POS_TYPE\": 'interpolate',  # type of loading the positional encoding. \"interpolate\" or \"index\".\n",
    "    \"TOKEN_TYPE_INDICATE\": False,  # add a token_type_embedding to indicate the search, template_foreground, template_background\n",
    "    \"INTERACTION_INDEXES\": [[0, 6], [6, 12], [12, 18], [18, 24]],\n",
    "    \"GRAD_CKPT\": False\n",
    "}\n",
    "\n",
    "# MODEL.NECK\n",
    "cfg[\"MODEL\"][\"NECK\"] = {\n",
    "    \"N_LAYERS\": 4,\n",
    "    \"D_MODEL\": 512,\n",
    "    \"D_STATE\": 16  # MAMABA_HIDDEN_STATE\n",
    "}\n",
    "\n",
    "# MODEL.DECODER\n",
    "cfg[\"MODEL\"][\"DECODER\"] = {\n",
    "    \"TYPE\": \"CENTER\",  # MLP, CORNER, CENTER\n",
    "    \"NUM_CHANNELS\": 256\n",
    "}\n",
    "\n",
    "# TRAIN\n",
    "cfg[\"TRAIN\"] = {\n",
    "    \"LR\": 0.0001,\n",
    "    \"WEIGHT_DECAY\": 0.0001,\n",
    "    \"EPOCH\": 500,\n",
    "    \"LR_DROP_EPOCH\": 400,\n",
    "    \"BATCH_SIZE\": 8,\n",
    "    \"NUM_WORKER\": 8,\n",
    "    \"OPTIMIZER\": \"ADAMW\",\n",
    "    \"ENCODER_MULTIPLIER\": 0.1,  # encoder's LR = this factor * LR\n",
    "    \"FREEZE_ENCODER\": False,  # for freezing the parameters of encoder\n",
    "    \"ENCODER_OPEN\": [],  # only for debug, open some layers of encoder when FREEZE_ENCODER is True\n",
    "    \"CE_WEIGHT\": 1.0,  # weight for cross-entropy loss\n",
    "    \"GIOU_WEIGHT\": 2.0,\n",
    "    \"L1_WEIGHT\": 5.0,\n",
    "    \"PRINT_INTERVAL\": 50,  # interval to print the training log\n",
    "    \"GRAD_CLIP_NORM\": 0.1,\n",
    "    \"FIX_BN\": False,\n",
    "    \"ENCODER_W\": \"\",\n",
    "    \"TYPE\": \"normal\",  # normal, peft or fft\n",
    "    \"PRETRAINED_PATH\": None\n",
    "}\n",
    "\n",
    "# TRAIN.SCHEDULER\n",
    "cfg[\"TRAIN\"][\"SCHEDULER\"] = {\n",
    "    \"TYPE\": \"step\",\n",
    "    \"DECAY_RATE\": 0.1\n",
    "}\n",
    "\n",
    "# DATA\n",
    "cfg[\"DATA\"] = {\n",
    "    \"MEAN\": [0.485, 0.456, 0.406],\n",
    "    \"STD\": [0.229, 0.224, 0.225],\n",
    "    \"MAX_SAMPLE_INTERVAL\": 200,\n",
    "    \"SAMPLER_MODE\": \"order\",\n",
    "    \"LOADER\": \"tracking\"\n",
    "}\n",
    "\n",
    "# DATA.TRAIN\n",
    "cfg[\"DATA\"][\"TRAIN\"] = {\n",
    "    \"DATASETS_NAME\": [\"LASOT\", \"GOT10K_vottrain\"],\n",
    "    \"DATASETS_RATIO\": [1, 1],\n",
    "    \"SAMPLE_PER_EPOCH\": 60000\n",
    "}\n",
    "\n",
    "# DATA.SEARCH\n",
    "cfg[\"DATA\"][\"SEARCH\"] = {\n",
    "    \"NUMBER\": 1,  # number of search region, only support 1 for now.\n",
    "    \"SIZE\": 256,\n",
    "    \"FACTOR\": 4.0,\n",
    "    \"CENTER_JITTER\": 3.5,\n",
    "    \"SCALE_JITTER\": 0.5\n",
    "}\n",
    "\n",
    "# DATA.TEMPLATE\n",
    "cfg[\"DATA\"][\"TEMPLATE\"] = {\n",
    "    \"NUMBER\": 1,\n",
    "    \"SIZE\": 128,\n",
    "    \"FACTOR\": 2.0,\n",
    "    \"CENTER_JITTER\": 0,\n",
    "    \"SCALE_JITTER\": 0\n",
    "}\n",
    "\n",
    "# TEST\n",
    "cfg[\"TEST\"] = {\n",
    "    \"TEMPLATE_FACTOR\": 4.0,\n",
    "    \"TEMPLATE_SIZE\": 256,\n",
    "    \"SEARCH_FACTOR\": 2.0,\n",
    "    \"SEARCH_SIZE\": 128,\n",
    "    \"EPOCH\": 500,\n",
    "    \"WINDOW\": False,  # window penalty\n",
    "    \"NUM_TEMPLATES\": 1\n",
    "}\n",
    "\n",
    "# TEST.UPT\n",
    "cfg[\"TEST\"][\"UPT\"] = {\n",
    "    \"DEFAULT\": 1,\n",
    "    \"LASOT\": 0,\n",
    "    \"LASOT_EXTENSION_SUBSET\": 0,\n",
    "    \"TRACKINGNET\": 0,\n",
    "    \"TNL2K\": 0,\n",
    "    \"NFS\": 0,\n",
    "    \"UAV\": 0,\n",
    "    \"VOT20\": 0,\n",
    "    \"GOT10K_TEST\": 0\n",
    "}\n",
    "\n",
    "# TEST.UPH\n",
    "cfg[\"TEST\"][\"UPH\"] = {\n",
    "    \"DEFAULT\": 1,\n",
    "    \"LASOT\": 0,\n",
    "    \"LASOT_EXTENSION_SUBSET\": 0,\n",
    "    \"TRACKINGNET\": 0,\n",
    "    \"TNL2K\": 0,\n",
    "    \"NFS\": 0,\n",
    "    \"UAV\": 0,\n",
    "    \"VOT20\": 0,\n",
    "    \"GOT10K_TEST\": 0\n",
    "}\n",
    "\n",
    "# TEST.INTER\n",
    "cfg[\"TEST\"][\"INTER\"] = {\n",
    "    \"DEFAULT\": 999999,\n",
    "    \"LASOT\": 0,\n",
    "    \"LASOT_EXTENSION_SUBSET\": 0,\n",
    "    \"TRACKINGNET\": 0,\n",
    "    \"TNL2K\": 0,\n",
    "    \"NFS\": 0,\n",
    "    \"UAV\": 0,\n",
    "    \"VOT20\": 0,\n",
    "    \"GOT10K_TEST\": 0\n",
    "}\n",
    "\n",
    "# TEST.MB\n",
    "cfg[\"TEST\"][\"MB\"] = {\n",
    "    \"DEFAULT\": 500,\n",
    "    \"LASOT\": 0,\n",
    "    \"LASOT_EXTENSION_SUBSET\": 0,\n",
    "    \"TRACKINGNET\": 0,\n",
    "    \"TNL2K\": 0,\n",
    "    \"NFS\": 0,\n",
    "    \"UAV\": 0,\n",
    "    \"VOT20\": 0,\n",
    "    \"GOT10K_TEST\": 0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test config:  {'MODEL': {'ENCODER': {'TYPE': 'fastitpnt', 'DROP_PATH': 0.1, 'PRETRAIN_TYPE': './fast_itpn_tiny_1600e_1k.pt', 'USE_CHECKPOINT': False, 'STRIDE': 16, 'POS_TYPE': 'index', 'TOKEN_TYPE_INDICATE': True, 'INTERACTION_INDEXES': [[4, 7], [7, 10], [10, 13], [13, 16]], 'GRAD_CKPT': False}, 'NECK': {'N_LAYERS': 4, 'D_MODEL': 384, 'D_STATE': 16}, 'DECODER': {'TYPE': 'CENTER', 'NUM_CHANNELS': 256}}, 'TRAIN': {'LR': 0.0004, 'WEIGHT_DECAY': 0.0001, 'EPOCH': 300, 'LR_DROP_EPOCH': 240, 'BATCH_SIZE': 64, 'NUM_WORKER': 10, 'OPTIMIZER': 'ADAMW', 'ENCODER_MULTIPLIER': 0.1, 'FREEZE_ENCODER': False, 'ENCODER_OPEN': [], 'CE_WEIGHT': 1.0, 'GIOU_WEIGHT': 2.0, 'L1_WEIGHT': 5.0, 'PRINT_INTERVAL': 50, 'GRAD_CLIP_NORM': 0.1, 'FIX_BN': False, 'ENCODER_W': '', 'TYPE': 'normal', 'PRETRAINED_PATH': None, 'SCHEDULER': {'TYPE': 'step', 'DECAY_RATE': 0.1}}, 'DATA': {'MEAN': [0.485, 0.456, 0.406], 'STD': [0.229, 0.224, 0.225], 'MAX_SAMPLE_INTERVAL': 400, 'SAMPLER_MODE': 'order', 'LOADER': 'tracking', 'TRAIN': {'DATASETS_NAME': ['LASOT', 'GOT10K_vottrain', 'COCO17', 'TRACKINGNET', 'VASTTRACK'], 'DATASETS_RATIO': [1, 1, 1, 1, 1], 'SAMPLE_PER_EPOCH': 60000}, 'SEARCH': {'NUMBER': 2, 'SIZE': 224, 'FACTOR': 4.0, 'CENTER_JITTER': 3.5, 'SCALE_JITTER': 0.5}, 'TEMPLATE': {'NUMBER': 5, 'SIZE': 112, 'FACTOR': 2.0, 'CENTER_JITTER': 0, 'SCALE_JITTER': 0}}, 'TEST': {'TEMPLATE_FACTOR': 2.0, 'TEMPLATE_SIZE': 112, 'SEARCH_FACTOR': 4.0, 'SEARCH_SIZE': 224, 'EPOCH': 300, 'WINDOW': True, 'NUM_TEMPLATES': 5, 'UPT': {'DEFAULT': 1, 'LASOT': 0.8, 'LASOT_EXTENSION_SUBSET': 0.85, 'TRACKINGNET': 0.5, 'TNL2K': 0.5, 'NFS': 0.8, 'UAV': 0.2, 'VOT20': 0.4, 'GOT10K_TEST': 0}, 'UPH': {'DEFAULT': 1, 'LASOT': 0.88, 'LASOT_EXTENSION_SUBSET': 0.97, 'TRACKINGNET': 0.9, 'TNL2K': 0.9, 'NFS': 0.92, 'UAV': 0.91, 'VOT20': 0.94, 'GOT10K_TEST': 0}, 'INTER': {'DEFAULT': 999999, 'LASOT': 70, 'LASOT_EXTENSION_SUBSET': 50, 'TRACKINGNET': 20, 'TNL2K': 20, 'NFS': 90, 'UAV': 1, 'VOT20': 1, 'GOT10K_TEST': 0}, 'MB': {'DEFAULT': 500, 'LASOT': 500, 'LASOT_EXTENSION_SUBSET': 500, 'TRACKINGNET': 200, 'TNL2K': 500, 'NFS': 500, 'UAV': 400, 'VOT20': 500, 'GOT10K_TEST': 0}}}\n"
     ]
    }
   ],
   "source": [
    "#Params\n",
    "class TrackerParams:\n",
    "    \"\"\"Class for tracker parameters.\"\"\"\n",
    "    def set_default_values(self, default_vals: dict):\n",
    "        for name, val in default_vals.items():\n",
    "            if not hasattr(self, name):\n",
    "                setattr(self, name, val)\n",
    "\n",
    "    def get(self, name: str, *default):\n",
    "        \"\"\"Get a parameter value with the given name. If it does not exists, it return the default value given as a\n",
    "        second argument or returns an error if no default value is given.\"\"\"\n",
    "        if len(default) > 1:\n",
    "            raise ValueError('Can only give one default value.')\n",
    "\n",
    "        if not default:\n",
    "            return getattr(self, name)\n",
    "\n",
    "        return getattr(self, name, default[0])\n",
    "\n",
    "    def has(self, name: str):\n",
    "        \"\"\"Check if there exist a parameter with the given name.\"\"\"\n",
    "        return hasattr(self, name)\n",
    "\n",
    "def _update_config(base_cfg, exp_cfg):\n",
    "    if isinstance(base_cfg, dict) and isinstance(exp_cfg, dict):\n",
    "        for k, v in exp_cfg.items():\n",
    "            if k in base_cfg:\n",
    "                if not isinstance(v, dict):\n",
    "                    base_cfg[k] = v\n",
    "                else:\n",
    "                    _update_config(base_cfg[k], v)\n",
    "            else:\n",
    "                raise ValueError(\"{} not exist in config.py\".format(k))\n",
    "    else:\n",
    "        return\n",
    "\n",
    "def update_config_from_file(filename):\n",
    "    exp_config = None\n",
    "    with open(filename) as f:\n",
    "        exp_config = yaml.safe_load(f)\n",
    "        _update_config(cfg, exp_config)\n",
    "    \n",
    "def parameters(yaml_name: str):\n",
    "    params = TrackerParams()\n",
    "\n",
    "    yaml_file = \"mcitrack_t224.yaml\"\n",
    "    update_config_from_file(yaml_file)\n",
    "    params.cfg = cfg\n",
    "    print(\"test config: \", cfg)\n",
    "\n",
    "    params.yaml_name = yaml_name\n",
    "    # template and search region\n",
    "    params.template_factor = cfg[\"TEST\"][\"TEMPLATE_FACTOR\"]\n",
    "    params.template_size = cfg[\"TEST\"][\"TEMPLATE_SIZE\"]\n",
    "    params.search_factor = cfg[\"TEST\"][\"SEARCH_FACTOR\"]\n",
    "    params.search_size = cfg[\"TEST\"][\"SEARCH_SIZE\"]\n",
    "\n",
    "    # Network checkpoint path\n",
    "    params.checkpoint = \"MCITrack.trt\"\n",
    "    # whether to save boxes from all queries\n",
    "    params.save_all_boxes = False\n",
    "\n",
    "    return params\n",
    "\n",
    "params = parameters(\"./mcitrack_t224.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRT_LOGGER = trt.Logger(trt.Logger.WARNING)\n",
    "\n",
    "class MCITRACK(BaseTracker):\n",
    "    def __init__(self, params):\n",
    "        super(MCITRACK, self).__init__(params)\n",
    "        self.cfg = params.cfg\n",
    "        \n",
    "        # Загружаем engine (сохраняем как атрибут)\n",
    "        self.engine = self.load_engine(\"MCITrack.trt\")\n",
    "        if not self.engine:\n",
    "            raise RuntimeError(\"Не удалось загрузить TensorRT engine.\")\n",
    "        \n",
    "        # Сохраняем context\n",
    "        self.context = self.engine.create_execution_context()\n",
    "        \n",
    "        # Сохраняем выделенную память\n",
    "        (self.d_template_list, self.d_search_list, ...) = self.initialize_memory()\n",
    "        \n",
    "        self.num_template = self.cfg[\"TEST\"][\"NUM_TEMPLATES\"]\n",
    "\n",
    "        self.preprocessor = Preprocessor()\n",
    "        self.state = None\n",
    "        self.frame_id = 0\n",
    "        self.save_all_boxes = params.save_all_boxes\n",
    "        self.z_dict1 = {}\n",
    "\n",
    "    def load_engine(self, engine_path):\n",
    "        with open(engine_path, \"rb\") as engine_file, trt.Runtime(TRT_LOGGER) as runtime:\n",
    "            engine = runtime.deserialize_cuda_engine(engine_file.read())\n",
    "        return engine\n",
    "\n",
    "    def initialize_memory(self):\n",
    "        # Входы\n",
    "        d_template_list = cuda.mem_alloc(int(np.prod([1, 3, 112, 112]) * np.dtype(np.float32).itemsize))\n",
    "        d_search_list = cuda.mem_alloc(int(np.prod([1, 3, 112, 112]) * np.dtype(np.float32).itemsize))\n",
    "        d_template_anno_list = cuda.mem_alloc(int(np.prod([1, 3, 112, 112]) * np.dtype(np.float32).itemsize))        \n",
    "        d_unsqueeze_3 = cuda.mem_alloc(int(np.prod([1, 3, 112, 112]) * np.dtype(np.float32).itemsize))\n",
    "        d_unsqueeze_4 = cuda.mem_alloc(int(np.prod([1, 3, 112, 112]) * np.dtype(np.float32).itemsize))\n",
    "        d_unsqueeze_5 = cuda.mem_alloc(int(np.prod([1, 3, 224, 224]) * np.dtype(np.float32).itemsize))\n",
    "        d_unsqueeze_6 = cuda.mem_alloc(int(np.prod([1, 4]) * np.dtype(np.float32).itemsize))\n",
    "        d_unsqueeze_7 = cuda.mem_alloc(int(np.prod([1, 4]) * np.dtype(np.float32).itemsize))\n",
    "        d_unsqueeze_8 = cuda.mem_alloc(int(np.prod([1, 4]) * np.dtype(np.float32).itemsize))\n",
    "        d_unsqueeze_9 = cuda.mem_alloc(int(np.prod([1, 4]) * np.dtype(np.float32).itemsize))\n",
    "        d_unsqueeze_10 = cuda.mem_alloc(int(np.prod([1, 4]) * np.dtype(np.float32).itemsize))\n",
    "        # Выходы\n",
    "        d_pred_boxes = cuda.mem_alloc(int(np.prod([1, 1, 4]) * np.dtype(np.float32).itemsize))\n",
    "        d_score_map = cuda.mem_alloc(int(np.prod([1, 1, 14, 14]) * np.dtype(np.float32).itemsize))\n",
    "        d_size_map = cuda.mem_alloc(int(np.prod([1, 2, 14, 14]) * np.dtype(np.float32).itemsize))\n",
    "        d_offset_map = cuda.mem_alloc(int(np.prod([1, 2, 14, 14]) * np.dtype(np.float32).itemsize))        \n",
    "        \n",
    "        return (\n",
    "        d_template_list,\n",
    "        d_search_list,\n",
    "        d_template_anno_list,\n",
    "        d_unsqueeze_3,\n",
    "        d_unsqueeze_4,\n",
    "        d_unsqueeze_5,\n",
    "        d_unsqueeze_6,\n",
    "        d_unsqueeze_7,\n",
    "        d_unsqueeze_8,\n",
    "        d_unsqueeze_9,\n",
    "        d_unsqueeze_10,\n",
    "        d_pred_boxes,\n",
    "        d_score_map,\n",
    "        d_size_map,\n",
    "        d_offset_map\n",
    "    )\n",
    "\n",
    "\n",
    "    \n",
    "    def predict(\n",
    "        context,\n",
    "        template_list,\n",
    "        search_list,\n",
    "        template_anno_list,\n",
    "        unsqueeze_3,\n",
    "        unsqueeze_4,\n",
    "        unsqueeze_5,\n",
    "        unsqueeze_6,\n",
    "        unsqueeze_7,\n",
    "        unsqueeze_8,\n",
    "        unsqueeze_9,\n",
    "        unsqueeze_10,\n",
    "        d_template_list,\n",
    "        d_search_list,\n",
    "        d_template_anno_list,\n",
    "        d_unsqueeze_3,\n",
    "        d_unsqueeze_4,\n",
    "        d_unsqueeze_5,\n",
    "        d_unsqueeze_6,\n",
    "        d_unsqueeze_7,\n",
    "        d_unsqueeze_8,\n",
    "        d_unsqueeze_9,\n",
    "        d_unsqueeze_10,\n",
    "        d_pred_boxes,\n",
    "        d_score_map,\n",
    "        d_size_map,\n",
    "        d_offset_map\n",
    "    ):\n",
    "                \n",
    "        # Копируем данные на GPU (используем выделенные буферы)\n",
    "        cuda.memcpy_htod(d_template_list, template_list.ravel())\n",
    "        cuda.memcpy_htod(d_search_list, search_list.ravel())\n",
    "        cuda.memcpy_htod(d_template_anno_list, template_anno_list.ravel())\n",
    "        cuda.memcpy_htod(d_unsqueeze_3, unsqueeze_3.ravel())\n",
    "        cuda.memcpy_htod(d_unsqueeze_4, unsqueeze_4.ravel())\n",
    "        cuda.memcpy_htod(d_unsqueeze_5, unsqueeze_5.ravel())\n",
    "        cuda.memcpy_htod(d_unsqueeze_6, unsqueeze_6.ravel())\n",
    "        cuda.memcpy_htod(d_unsqueeze_7, unsqueeze_7.ravel())\n",
    "        cuda.memcpy_htod(d_unsqueeze_8, unsqueeze_8.ravel())\n",
    "        cuda.memcpy_htod(d_unsqueeze_9, unsqueeze_9.ravel())\n",
    "        cuda.memcpy_htod(d_unsqueeze_10, unsqueeze_10.ravel())\n",
    "        \n",
    "        # Указываем правильные bindings\n",
    "        context.execute_v2(bindings=[\n",
    "            int(d_template_list),     # 0\n",
    "            int(d_search_list),       # 1\n",
    "            int(d_template_anno_list),# 2\n",
    "            int(d_unsqueeze_3),       # 3\n",
    "            int(d_unsqueeze_4),       # 4\n",
    "            int(d_unsqueeze_5),       # 5\n",
    "            int(d_unsqueeze_6),       # 6\n",
    "            int(d_unsqueeze_7),       # 7\n",
    "            int(d_unsqueeze_8),       # 8\n",
    "            int(d_unsqueeze_9),       # 9\n",
    "            int(d_unsqueeze_10),      # 10\n",
    "            int(d_pred_boxes),       # 11\n",
    "            int(d_score_map),        # 12\n",
    "            int(d_size_map),         # 13\n",
    "            int(d_offset_map)        # 14\n",
    "        ])\n",
    "        \n",
    "        # Копируем результат с GPU\n",
    "        pred_boxes = np.empty([1, 1, 4], dtype=np.float32)\n",
    "        cuda.memcpy_dtoh(pred_boxes, d_pred_boxes)\n",
    "\n",
    "        score_map = np.empty([1, 1, 14, 14], dtype=np.float32)\n",
    "        cuda.memcpy_dtoh(score_map, d_score_map)\n",
    "\n",
    "        size_map = np.empty([1, 2, 14, 14], dtype=np.float32)\n",
    "        cuda.memcpy_dtoh(size_map, d_size_map)\n",
    "\n",
    "        offset_map = np.empty([1, 2, 14, 14], dtype=np.float32)\n",
    "        cuda.memcpy_dtoh(offset_map, d_offset_map)\n",
    "\n",
    "        return pred_boxes, score_map, size_map, offset_map\n",
    "        \n",
    "    def initialize(self, image, info: dict):\n",
    "        z_patch_arr, resize_factor = sample_target(image, info['init_bbox'], \n",
    "                                self.params.template_factor,\n",
    "                                output_sz=self.params.template_size)\n",
    "        \n",
    "        # Шаблон (хранится как тензор на CPU)\n",
    "        self.template = self.preprocessor.process(z_patch_arr).cpu()\n",
    "        self.template_list = [self.template] * self.num_template  # Список тензоров\n",
    "        \n",
    "        # Аннотации (хранятся как тензоры на CPU)\n",
    "        prev_box_crop = transform_image_to_crop(torch.tensor(info['init_bbox']),\n",
    "                                            torch.tensor(info['init_bbox']),\n",
    "                                            resize_factor,\n",
    "                                            torch.Tensor([self.params.template_size, \n",
    "                                                        self.params.template_size]),\n",
    "                                            normalize=True).cpu()\n",
    "        self.template_anno_list = [prev_box_crop.unsqueeze(0)] * self.num_template\n",
    "        \n",
    "        self.state = info['init_bbox']\n",
    "        self.frame_id = 0\n",
    "\n",
    "\n",
    "    def track(self, image, info: dict = None):      \n",
    "        H, W, _ = image.shape\n",
    "        self.frame_id += 1\n",
    "\n",
    "        x_patch_arr, resize_factor = sample_target(image, self.state, \n",
    "                                                self.params.search_factor,\n",
    "                                                output_sz=self.params.search_size)\n",
    "        \n",
    "        # Обрабатываем поисковое изображение и перемещаем на CPU\n",
    "        search = self.preprocessor.process(x_patch_arr).cpu()\n",
    "        search_list = [search]\n",
    "\n",
    "        # Преобразуем тензоры в numpy массивы\n",
    "        template_list_np = [t.numpy() for t in self.template_list]\n",
    "        search_list_np = [s.numpy() for s in search_list]\n",
    "        template_anno_list_np = [ta.numpy() for ta in self.template_anno_list]\n",
    "\n",
    "        # outputs =self.predictV2(self.context,search.cpu().numpy().astype(np.float32), self.template.cpu().numpy().astype(np.float32),self.d_input_z, self.d_input_x, self.d_output_cls)\n",
    "\n",
    "        outputs = self.predict(context, template_list_np, search_list_np, template_anno_list_np)\n",
    "        \n",
    "        out_dict = {\n",
    "            'pred_boxes': outputs[:, :4],\n",
    "            'score_map': outputs[:, 4:5],\n",
    "            'size_map': outputs[:, 5:6],\n",
    "            'offset_map': outputs[:, 6:]\n",
    "        }\n",
    "\n",
    "        # add hann windows\n",
    "        pred_score_map = out_dict['score_map']\n",
    "        if self.cfg[\"TEST\"][\"WINDOW\"] == True:  # for window penalty\n",
    "            response = self.output_window * pred_score_map\n",
    "        else:\n",
    "            response = pred_score_map\n",
    "\n",
    "        pred_boxes, conf_score = cal_bbox(response, out_dict['size_map'],\n",
    "                                                                   out_dict['offset_map'])\n",
    "\n",
    "        #pred_boxes = torch.from_numpy(outputs).view(-1, 4)\n",
    "        pred_boxes = pred_boxes.view(-1, 4)\n",
    "        pred_box = (pred_boxes.mean(dim=0) * self.params.search_size / resize_factor).tolist()  # (cx, cy, w, h) [0,1]   \n",
    "        # get the final box result     \n",
    "        self.state = clip_box(self.map_box_back(pred_box, resize_factor), H, W, margin=10)\n",
    "\n",
    "        return {\"target_bbox\": self.state,\n",
    "                \"best_score\": conf_score}\n",
    "    \n",
    "    def map_box_back(self, pred_box: list, resize_factor: float):\n",
    "        cx_prev, cy_prev = self.state[0] + 0.5 * self.state[2], self.state[1] + 0.5 * self.state[3]\n",
    "        cx, cy, w, h = pred_box\n",
    "        half_side = 0.5 * self.params.search_size / resize_factor\n",
    "        cx_real = cx + (cx_prev - half_side)\n",
    "        cy_real = cy + (cy_prev - half_side)\n",
    "        return [cx_real - 0.5 * w, cy_real - 0.5 * h, w, h]\n",
    "\n",
    "    def map_box_back_batch(self, pred_box: torch.Tensor, resize_factor: float):\n",
    "        cx_prev, cy_prev = self.state[0] + 0.5 * self.state[2], self.state[1] + 0.5 * self.state[3]\n",
    "        cx, cy, w, h = pred_box.unbind(-1)\n",
    "        half_side = 0.5 * self.params.search_size / resize_factor\n",
    "        cx_real = cx + (cx_prev - half_side)\n",
    "        cy_real = cy + (cy_prev - half_side)\n",
    "        return torch.stack([cx_real - 0.5 * w, cy_real - 0.5 * h, w, h], dim=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Диагностика\n",
    "import numpy as np\n",
    "import torch\n",
    "import cv2\n",
    "\n",
    "def create_test_image():\n",
    "    \"\"\"Создает тестовое изображение размером 640x480 с случайными цветами\"\"\"\n",
    "    image = np.random.randint(0, 255, (480, 640, 3), dtype=np.uint8)\n",
    "    return image\n",
    "\n",
    "def create_test_info():\n",
    "    \"\"\"Создает тестовый bounding box\"\"\"\n",
    "    init_bbox = [200, 150, 100, 100]  # [x, y, width, height]\n",
    "    return {'init_bbox': init_bbox}\n",
    "\n",
    "def diagnose_tracker(tracker):\n",
    "    print(\"Запуск диагностики...\")\n",
    "\n",
    "    # Создание тестовых данных\n",
    "    image = create_test_image()\n",
    "    info = create_test_info()\n",
    "\n",
    "    # Проверка инициализации трекера\n",
    "    print(\"Проверка инициализации трекера...\")\n",
    "    tracker.initialize(image, info)\n",
    "    assert tracker.state is not None, \"Ошибка: tracker.state не инициализирован!\"\n",
    "\n",
    "    # Проверка входных данных\n",
    "    print(\"Проверка входных данных...\")\n",
    "    x_patch_arr, resize_factor = sample_target(image, tracker.state, tracker.params.search_factor,\n",
    "                                               output_sz=tracker.params.search_size)\n",
    "    search = tracker.preprocessor.process(x_patch_arr)\n",
    "\n",
    "    search_np = search.cpu().numpy().astype(np.float32)\n",
    "    template_np = tracker.template.cpu().numpy().astype(np.float32)\n",
    "    template_anno_np = tracker.template_anno.cpu().numpy().astype(np.float32)\n",
    "\n",
    "    assert search_np is not None, \"Ошибка: search_np = None!\"\n",
    "    assert template_np is not None, \"Ошибка: template_np = None!\"\n",
    "    assert template_anno_np is not None, \"Ошибка: template_anno_np = None!\"\n",
    "\n",
    "    print(\"Диапазон входных данных:\")\n",
    "    print(f\"search_np: min={search_np.min()}, max={search_np.max()}\")\n",
    "    print(f\"template_np: min={template_np.min()}, max={template_np.max()}\")\n",
    "    print(f\"template_anno_np: min={template_anno_np.min()}, max={template_anno_np.max()}\")\n",
    "\n",
    "    # Проверка предсказания\n",
    "    print(\"Запуск модели...\")\n",
    "    outputs = tracker.predict(search_np, template_np, template_anno_np)\n",
    "\n",
    "    print(\"Диапазон выходных данных:\")\n",
    "    print(f\"outputs: min={outputs.min()}, max={outputs.max()}\")\n",
    "\n",
    "    assert outputs is not None, \"Ошибка: outputs = None!\"\n",
    "    assert np.any(outputs), \"Ошибка: Все значения outputs равны нулю!\"\n",
    "\n",
    "    print(\"Диагностика завершена успешно!\")\n",
    "\n",
    "# Пример вызова:\n",
    "# tracker = MCITRACK(params)  # Создай объект трекера перед вызовом диагностики\n",
    "# diagnose_tracker(tracker)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "treacker = MCITRACK(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnose_tracker(treacker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "LogicError",
     "evalue": "cuMemcpyHtoD failed: invalid argument",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLogicError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 27\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Трекинг\u001b[39;00m\n\u001b[0;32m     26\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m---> 27\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mtreacker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtracker_image\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m state \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mint\u001b[39m(s) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m out[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget_bbox\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m#best_score = out[\"best_score\"]\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[5], line 105\u001b[0m, in \u001b[0;36mMCITRACK.track\u001b[1;34m(self, image, info)\u001b[0m\n\u001b[0;32m    102\u001b[0m search_list_np \u001b[38;5;241m=\u001b[39m [s\u001b[38;5;241m.\u001b[39mnumpy() \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m search_list]\n\u001b[0;32m    103\u001b[0m template_anno_list_np \u001b[38;5;241m=\u001b[39m [ta\u001b[38;5;241m.\u001b[39mnumpy() \u001b[38;5;28;01mfor\u001b[39;00m ta \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtemplate_anno_list]\n\u001b[1;32m--> 105\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtemplate_list_np\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msearch_list_np\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemplate_anno_list_np\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    107\u001b[0m out_dict \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    108\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpred_boxes\u001b[39m\u001b[38;5;124m'\u001b[39m: outputs[:, :\u001b[38;5;241m4\u001b[39m],\n\u001b[0;32m    109\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscore_map\u001b[39m\u001b[38;5;124m'\u001b[39m: outputs[:, \u001b[38;5;241m4\u001b[39m:\u001b[38;5;241m5\u001b[39m],\n\u001b[0;32m    110\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msize_map\u001b[39m\u001b[38;5;124m'\u001b[39m: outputs[:, \u001b[38;5;241m5\u001b[39m:\u001b[38;5;241m6\u001b[39m],\n\u001b[0;32m    111\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moffset_map\u001b[39m\u001b[38;5;124m'\u001b[39m: outputs[:, \u001b[38;5;241m6\u001b[39m:]\n\u001b[0;32m    112\u001b[0m }\n\u001b[0;32m    114\u001b[0m \u001b[38;5;66;03m# add hann windows\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[5], line 50\u001b[0m, in \u001b[0;36mMCITRACK.predict\u001b[1;34m(self, template_list_np, search_list_np, template_anno_list_np)\u001b[0m\n\u001b[0;32m     47\u001b[0m template_anno_arr \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate(template_anno_list_np)\u001b[38;5;241m.\u001b[39mravel()\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m# Копируем данные на GPU (используем выделенные буферы)\u001b[39;00m\n\u001b[1;32m---> 50\u001b[0m \u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmemcpy_htod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43md_template\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemplate_arr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m cuda\u001b[38;5;241m.\u001b[39mmemcpy_htod(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md_search, search_arr)\n\u001b[0;32m     52\u001b[0m cuda\u001b[38;5;241m.\u001b[39mmemcpy_htod(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md_template_anno, template_anno_arr)\n",
      "\u001b[1;31mLogicError\u001b[0m: cuMemcpyHtoD failed: invalid argument"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Трекинг по видео\n",
    "file = \"0516.mp4\"\n",
    "video = cv2.VideoCapture(file)\n",
    "ok, image = video.read()\n",
    "if not video.isOpened():\n",
    "    print(\"Could not open video\")\n",
    "    sys.exit()\n",
    "    \n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "x, y, w, h = cv2.selectROI( image, fromCenter=False)\n",
    "init_state = [x, y, w, h]\n",
    "def _build_init_info(box):\n",
    "            return {'init_bbox': box}\n",
    "treacker.initialize(image, _build_init_info(init_state))\n",
    "counter = 0\n",
    "while True:\n",
    "    ok, image = video.read()\n",
    "    if not ok:\n",
    "        break\n",
    "\n",
    "    # Конвертация для трекера\n",
    "    tracker_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Трекинг\n",
    "    start_time = time.time()\n",
    "    out = treacker.track(tracker_image)\n",
    "    state = [int(s) for s in out['target_bbox']]\n",
    "    #best_score = out[\"best_score\"]\n",
    "    best_score = 1\n",
    "    fps = 1 / (time.time() - start_time + 1e-6)\n",
    "\n",
    "    # Визуализация\n",
    "    display_image = image.copy()\n",
    "    x, y, w, h = state\n",
    "    \n",
    "    # Динамический цвет рамки в зависимости от уверенности\n",
    "    color = (0, 255, 0) if best_score > 0.7 else (0, 255, 255) if best_score > 0.4 else (0, 0, 255)\n",
    "    thickness = 3 if best_score > 0.7 else 2\n",
    "    \n",
    "    # Рисуем bounding box с увеличенными размерами\n",
    "    cv2.rectangle(display_image, (x, y), (x + w, y + h), color, thickness)\n",
    "    \n",
    "    # Добавляем информационный текст\n",
    "    cv2.putText(display_image, f\"Score: {best_score:.2f}\", (x, y-10), \n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)\n",
    "    cv2.putText(display_image, f\"FPS: {fps:.1f}\", (20, 40), \n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)\n",
    "    \n",
    "    cv2.imshow(\"tracking\", display_image)\n",
    "    \n",
    "    # Обработка клавиш\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    if key == 32:  # SPACE - переинициализация\n",
    "        x, y, w, h = cv2.selectROI(\"Select ROI\", image, fromCenter=False)\n",
    "        if w > 10 and h > 10:  # Минимальный размер ROI\n",
    "            init_state = [x, y, w, h]\n",
    "            print(\"Переинициализация...\")\n",
    "            treacker.initialize(tracker_image, _build_init_info(init_state))\n",
    "    elif key == 27:  # ESC - выход\n",
    "        break\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
