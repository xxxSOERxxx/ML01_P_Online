{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7bb640b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "429c4397",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\piuta_en\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import yaml\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils import checkpoint\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import cv2\n",
    "import sys\n",
    "import numpy as np\n",
    "import fastitpn as fastitpn_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4bde567b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBase(nn.Module):\n",
    "\n",
    "    def __init__(self, encoder: nn.Module, train_encoder: bool, open_layers: list, num_channels: int):\n",
    "        super().__init__()\n",
    "        open_blocks = open_layers[2:]\n",
    "        open_items = open_layers[0:2]\n",
    "        for name, parameter in encoder.named_parameters():\n",
    "\n",
    "            if not train_encoder:\n",
    "                freeze = True\n",
    "                for open_block in open_blocks:\n",
    "                    if open_block in name:\n",
    "                        freeze = False\n",
    "                if name in open_items:\n",
    "                    freeze = False\n",
    "                if freeze == True:\n",
    "                    parameter.requires_grad_(False)  # here should allow users to specify which layers to freeze !\n",
    "\n",
    "        self.body = encoder\n",
    "        self.num_channels = num_channels\n",
    "\n",
    "    def forward(self, template_list, search_list, template_anno_list):\n",
    "        xs = self.body(template_list, search_list, template_anno_list)\n",
    "        return xs\n",
    "\n",
    "\n",
    "#fast_itpn_tiny_1600e_1k\n",
    "\n",
    "class Encoder(EncoderBase):\n",
    "    \"\"\"FastITPN encoder.\"\"\"\n",
    "    def __init__(self, name: str,\n",
    "                 train_encoder: bool,\n",
    "                 pretrain_type: str,\n",
    "                 search_size: int,\n",
    "                 search_number: int,\n",
    "                 template_size: int,\n",
    "                 template_number: int,\n",
    "                 open_layers: list,\n",
    "                 cfg=None):\n",
    "        if \"fastitpn\" in name.lower():\n",
    "            encoder = getattr(fastitpn_module, name)(\n",
    "                pretrained=True,\n",
    "                search_size=search_size,\n",
    "                template_size=template_size,\n",
    "                drop_rate=0.0,\n",
    "                drop_path_rate=0.1,\n",
    "                attn_drop_rate=0.0,\n",
    "                init_values=0.1,\n",
    "                drop_block_rate=None,\n",
    "                use_mean_pooling=True,\n",
    "                grad_ckpt=cfg[\"MODEL\"][\"ENCODER\"][\"GRAD_CKPT\"],\n",
    "                pos_type=cfg[\"MODEL\"][\"ENCODER\"][\"POS_TYPE\"],\n",
    "                token_type_indicate=cfg[\"MODEL\"][\"ENCODER\"][\"TOKEN_TYPE_INDICATE\"],\n",
    "                pretrain_type = cfg[\"MODEL\"][\"ENCODER\"][\"PRETRAIN_TYPE\"],\n",
    "            )\n",
    "            if \"itpnb\" in name:\n",
    "                num_channels = 512\n",
    "            elif \"itpnl\" in name:\n",
    "                num_channels = 768\n",
    "            elif \"itpnt\" in name:\n",
    "                num_channels = 384\n",
    "            elif \"itpns\" in name:\n",
    "                num_channels = 384\n",
    "            else:\n",
    "                num_channels = 512\n",
    "        else:\n",
    "            raise ValueError()\n",
    "        super().__init__(encoder, train_encoder, open_layers, num_channels)\n",
    "\n",
    "def build_encoder(cfg):\n",
    "    train_encoder = (cfg[\"TRAIN\"][\"ENCODER_MULTIPLIER\"] > 0) and (cfg[\"TRAIN\"][\"FREEZE_ENCODER\"] == False)\n",
    "    encoder = Encoder(cfg[\"MODEL\"][\"ENCODER\"][\"TYPE\"], train_encoder,\n",
    "                      cfg[\"MODEL\"][\"ENCODER\"][\"PRETRAIN_TYPE\"],\n",
    "                      cfg[\"DATA\"][\"SEARCH\"][\"SIZE\"], cfg[\"DATA\"][\"SEARCH\"][\"NUMBER\"],\n",
    "                      cfg[\"DATA\"][\"TEMPLATE\"][\"SIZE\"], cfg[\"DATA\"][\"TEMPLATE\"][\"NUMBER\"],\n",
    "                      cfg[\"TRAIN\"][\"ENCODER_OPEN\"], cfg)\n",
    "    return encoder\n",
    "\n",
    "class MambaBlock(nn.Module):\n",
    "    def __init__(self,dt_scale, d_model,d_inner,dt_rank,d_state,bias,d_conv,conv_bias,dt_init,dt_max,dt_min,dt_init_floor):\n",
    "        super().__init__()\n",
    "        #  projects block input from D to 2*ED (two branches)\n",
    "        self.dt_scale = dt_scale\n",
    "        self.d_model = d_model\n",
    "        self.d_inner = d_inner\n",
    "        self.dt_rank = dt_rank\n",
    "        self.d_state = d_state\n",
    "        self.in_proj = nn.Linear(self.d_model, 2 * self.d_inner, bias=bias)\n",
    "\n",
    "        self.conv1d = nn.Conv1d(in_channels=self.d_inner, out_channels=self.d_inner,\n",
    "                                kernel_size=d_conv, bias=conv_bias,\n",
    "                                groups=self.d_inner,\n",
    "                                padding=(d_conv - 1)//2)\n",
    "\n",
    "        #  projects x to input-dependent Δ, B, C\n",
    "        self.x_proj = nn.Linear(self.d_inner, self.dt_rank + 2 * self.d_state, bias=False)\n",
    "\n",
    "        #  projects Δ from dt_rank to d_inner\n",
    "        self.dt_proj = nn.Linear(self.dt_rank, self.d_inner, bias=True)\n",
    "\n",
    "        #  dt initialization\n",
    "        #  dt weights\n",
    "        dt_init_std = self.dt_rank ** -0.5 * self.dt_scale\n",
    "        if dt_init == \"constant\":\n",
    "            nn.init.constant_(self.dt_proj.weight, dt_init_std)\n",
    "        elif dt_init == \"random\":\n",
    "            nn.init.uniform_(self.dt_proj.weight, -dt_init_std, dt_init_std)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        # dt bias\n",
    "        dt = torch.exp(\n",
    "            torch.rand(self.d_inner) * (math.log(dt_max) - math.log(dt_min)) + math.log(dt_min)\n",
    "        ).clamp(min=dt_init_floor)\n",
    "        inv_dt = dt + torch.log(\n",
    "            -torch.expm1(-dt))  #  inverse of softplus: https://github.com/pytorch/pytorch/issues/72759\n",
    "        with torch.no_grad():\n",
    "            self.dt_proj.bias.copy_(inv_dt)\n",
    "        # self.dt_proj.bias._no_reinit = True # initialization would set all Linear.bias to zero, need to mark this one as _no_reinit\n",
    "        #  todo : explain why removed\n",
    "\n",
    "        # S4D real initialization\n",
    "        A = torch.arange(1, self.d_state + 1, dtype=torch.float32).repeat(self.d_inner, 1)\n",
    "        self.A_log = nn.Parameter(\n",
    "            torch.log(A))  # why store A in log ? to keep A < 0 (cf -torch.exp(...)) ? for gradient stability ?\n",
    "        self.D = nn.Parameter(torch.ones(self.d_inner))\n",
    "\n",
    "        #  projects block output from ED back to D\n",
    "        self.out_proj = nn.Linear(self.d_inner, self.d_model, bias=bias)\n",
    "    def forward(self, x, h):\n",
    "        #  x : (B,L, D)\n",
    "        # h : (B,L, ED, N)\n",
    "\n",
    "        #  y : (B, L, D)\n",
    "\n",
    "\n",
    "        xz = self.in_proj(x)  # (B, L,2*ED)\n",
    "        x, z = xz.chunk(2, dim=-1)  #  (B,L, ED), (B,L, ED)\n",
    "        x_cache = x.permute(0,2,1)#(B, ED,L)\n",
    "\n",
    "        #  x branch\n",
    "        x = self.conv1d( x_cache).permute(0,2,1) #  (B,L , ED)\n",
    "\n",
    "        x = F.silu(x)\n",
    "        y, h = self.ssm_step(x, h)\n",
    "        #y->B,L,ED;h->B,L,ED,N\n",
    "\n",
    "        #  z branch\n",
    "        z = F.silu(z)\n",
    "\n",
    "        output = y * z\n",
    "        output = self.out_proj(output)  #  (B, L, D)\n",
    "\n",
    "        return output, h\n",
    "\n",
    "    def ssm_step(self, x, h):\n",
    "        #  x : (B, L, ED)\n",
    "        #  h : (B, L, ED, N)\n",
    "\n",
    "        A = -torch.exp(\n",
    "            self.A_log.float())  # (ED, N) # todo : ne pas le faire tout le temps, puisque c'est indépendant de la timestep\n",
    "        D = self.D.float()\n",
    "        #  TODO remove .float()\n",
    "\n",
    "        deltaBC = self.x_proj(x)  #  (B, L, dt_rank+2*N)\n",
    "\n",
    "        delta, B, C = torch.split(deltaBC, [self.dt_rank, self.d_state, self.d_state],\n",
    "                                  dim=-1)  #  (B, L,dt_rank), (B, L, N), (B, L, N)\n",
    "        delta = F.softplus(self.dt_proj(delta))  #  (B, L, ED)\n",
    "\n",
    "        deltaA = torch.exp(delta.unsqueeze(-1) * A)  #  (B,L, ED, N)\n",
    "        deltaB = delta.unsqueeze(-1) * B.unsqueeze(2)  #  (B,L, ED, N)\n",
    "\n",
    "        BX = deltaB * (x.unsqueeze(-1))  #  (B, L,ED, N)\n",
    "\n",
    "        if h is None:\n",
    "            h = torch.zeros(x.size(0), x.size(1), self.d_inner, self.d_state, device=deltaA.device)  #  (B, L, ED, N)\n",
    "\n",
    "        h = deltaA * h + BX  #  (B, L, ED, N)\n",
    "\n",
    "        y = (h @ C.unsqueeze(-1)).squeeze(3)  #  (B, L, ED, N) @ (B, L, N, 1) -> (B, L, ED, 1)\n",
    "\n",
    "        y = y + D * x#B,L,ED\n",
    "\n",
    "        #  todo : pq h.squeeze(1) ??\n",
    "        return y, h\n",
    "    \n",
    "class DWConv(nn.Module):\n",
    "    def __init__(self, dim=768):\n",
    "        super().__init__()\n",
    "        self.dwconv = nn.Conv2d(dim, dim, 3, 1, 1, bias=True, groups=dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(1,0,2)\n",
    "        B, N, C = x.shape\n",
    "        x = x.transpose(1,2).view(B,C,int(N**0.5),int(N**0.5)).contiguous()\n",
    "        x = self.dwconv(x).flatten(2).transpose(1, 2)#B,N,C\n",
    "        x = x.permute(1,0,2)\n",
    "        return x\n",
    "class ConvFFN(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None,\n",
    "                 act_layer=nn.GELU, drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.dwconv = DWConv(hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.dwconv(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "    \n",
    "class ConvFFN(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None,\n",
    "                 act_layer=nn.GELU, drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.dwconv = DWConv(hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.dwconv(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "def drop_path(x, drop_prob: float = 0., training: bool = False, scale_by_keep: bool = True):\n",
    "    \"\"\"Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).\n",
    "\n",
    "    This is the same as the DropConnect impl I created for EfficientNet, etc networks, however,\n",
    "    the original name is misleading as 'Drop Connect' is a different form of dropout in a separate paper...\n",
    "    See discussion: https://github.com/tensorflow/tpu/issues/494#issuecomment-532968956 ... I've opted for\n",
    "    changing the layer and argument names to 'drop path' rather than mix DropConnect as a layer name and use\n",
    "    'survival rate' as the argument.\n",
    "\n",
    "    \"\"\"\n",
    "    if drop_prob == 0. or not training:\n",
    "        return x\n",
    "    keep_prob = 1 - drop_prob\n",
    "    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n",
    "    random_tensor = x.new_empty(shape).bernoulli_(keep_prob)\n",
    "    if keep_prob > 0.0 and scale_by_keep:\n",
    "        random_tensor.div_(keep_prob)\n",
    "    return x * random_tensor\n",
    "\n",
    "class DropPath(nn.Module):\n",
    "    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\n",
    "    \"\"\"\n",
    "    def __init__(self, drop_prob: float = 0., scale_by_keep: bool = True):\n",
    "        super(DropPath, self).__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.scale_by_keep = scale_by_keep\n",
    "\n",
    "    def forward(self, x):\n",
    "        return drop_path(x, self.drop_prob, self.training, self.scale_by_keep)\n",
    "\n",
    "    def extra_repr(self):\n",
    "        return f'drop_prob={round(self.drop_prob,3):0.3f}'\n",
    "    \n",
    "\"\"\"class partial:\n",
    " \n",
    "\n",
    "    __slots__ = \"func\", \"args\", \"keywords\", \"__dict__\", \"__weakref__\"\n",
    "\n",
    "    def __new__(cls, func, /, *args, **keywords):\n",
    "        if not callable(func):\n",
    "            raise TypeError(\"the first argument must be callable\")\n",
    "\n",
    "        if hasattr(func, \"func\"):\n",
    "            args = func.args + args\n",
    "            keywords = {**func.keywords, **keywords}\n",
    "            func = func.func\n",
    "\n",
    "        self = super(partial, cls).__new__(cls)\n",
    "\n",
    "        self.func = func\n",
    "        self.args = args\n",
    "        self.keywords = keywords\n",
    "        return self\n",
    "\n",
    "class Extractor(nn.Module):\n",
    "    def __init__(self, d_model, num_heads=8, dropout=0.1,drop_path=0.1,\n",
    "                 norm_layer=partial(nn.LayerNorm, eps=1e-6)):\n",
    "        super().__init__()\n",
    "        self.query_norm = norm_layer(d_model)\n",
    "        self.feat_norm = norm_layer(d_model)\n",
    "        self.attn = nn.MultiheadAttention(d_model, num_heads, dropout=dropout)\n",
    "        #convffn\n",
    "        self.ffn = ConvFFN(in_features=d_model, hidden_features=int(d_model * 0.25), drop=0.)\n",
    "        self.ffn_norm = norm_layer(d_model)\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\"\"\"\n",
    "\n",
    "class Extractor(nn.Module):\n",
    "    def __init__(self, d_model, num_heads=8, dropout=0.1, drop_path=0.1,\n",
    "                 norm_layer=lambda x: nn.LayerNorm(x, eps=1e-6)):  # Замена partial на лямбду\n",
    "        super().__init__()\n",
    "        self.query_norm = norm_layer(d_model)\n",
    "        self.feat_norm = norm_layer(d_model)\n",
    "        self.attn = nn.MultiheadAttention(d_model, num_heads, dropout=dropout)\n",
    "        # convffn\n",
    "        self.ffn = ConvFFN(in_features=d_model, hidden_features=int(d_model * 0.25), drop=0.)\n",
    "        self.ffn_norm = norm_layer(d_model)\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "\n",
    "    def forward(self, query, feat):\n",
    "\n",
    "        def _inner_forward(query, feat):\n",
    "            # query:l,b,d;feat:l,b,d\n",
    "            attn = self.attn(self.query_norm(query),\n",
    "                             self.feat_norm(feat), self.feat_norm(feat))[0]\n",
    "            query = query + attn\n",
    "\n",
    "            query = query + self.drop_path(self.ffn(self.ffn_norm(query)))\n",
    "            return query\n",
    "\n",
    "        query = _inner_forward(query, feat)\n",
    "\n",
    "        return query\n",
    " \n",
    "class Injector(nn.Module):\n",
    "    def __init__(self, d_model, n_heads=8, norm_layer=lambda x: nn.LayerNorm(x, eps=1e-6), dropout=0.1, init_values=0.):\n",
    "        super().__init__()\n",
    "        self.query_norm = norm_layer(d_model)\n",
    "        self.feat_norm = norm_layer(d_model)\n",
    "        self.attn = nn.MultiheadAttention(d_model, n_heads, dropout=dropout)\n",
    "        self.gamma = nn.Parameter(init_values * torch.ones((d_model)), requires_grad=True)\n",
    "        \n",
    "    def forward(self, query,feat):\n",
    "            #query:l,b,d;feat:l,b,d\n",
    "        def _inner_forward(query, feat):\n",
    "\n",
    "            attn = self.attn(self.query_norm(query),\n",
    "                             self.feat_norm(feat),self.feat_norm(feat))[0]\n",
    "            return query + self.gamma * attn\n",
    "        query = _inner_forward(query, feat)\n",
    "        return query    \n",
    "    \n",
    "    \"\"\"class Injector(nn.Module):\n",
    "    \n",
    "    \n",
    "    \n",
    "    def __init__(self, d_model, n_heads=8,norm_layer=partial(nn.LayerNorm, eps=1e-6),  dropout=0.1,\n",
    "                 init_values=0.):\n",
    "        super().__init__()\n",
    "        self.query_norm = norm_layer(d_model)\n",
    "        self.feat_norm = norm_layer(d_model)\n",
    "        self.attn = nn.MultiheadAttention(d_model, n_heads,dropout=dropout)\n",
    "        self.gamma = nn.Parameter(init_values * torch.ones((d_model)), requires_grad=True)\n",
    "\n",
    "    def forward(self, query,feat):\n",
    "            #query:l,b,d;feat:l,b,d\n",
    "        def _inner_forward(query, feat):\n",
    "\n",
    "            attn = self.attn(self.query_norm(query),\n",
    "                             self.feat_norm(feat),self.feat_norm(feat))[0]\n",
    "            return query + self.gamma * attn\n",
    "        query = _inner_forward(query, feat)\n",
    "        return query\"\"\"\n",
    "    \n",
    "\n",
    "class InteractionBlock(nn.Module):\n",
    "    def __init__(self, d_model, extra_extractor, grad_ckpt):\n",
    "        super().__init__()\n",
    "        self.grad_ckpt = grad_ckpt\n",
    "        self.injector = Injector(d_model=d_model)\n",
    "        self.extractor = Extractor(d_model=d_model)\n",
    "        if extra_extractor:\n",
    "            self.extra_extractors = nn.Sequential(*[\n",
    "                Extractor(d_model=d_model)\n",
    "                for _ in range(2)])\n",
    "        else:\n",
    "            self.extra_extractors = None\n",
    "\n",
    "    def forward(self,x,xs,blocks):\n",
    "        x = self.injector(x.permute(1,0,2),xs.permute(1,0,2)).permute(1,0,2)\n",
    "        for idx,blk in enumerate(blocks):\n",
    "            x = checkpoint.checkpoint(blk, x, None,use_reentrant=False) if self.grad_ckpt else blk(x,None)\n",
    "        xs = checkpoint.checkpoint(self.extractor, xs.permute(1,0,2),x.permute(1,0,2),use_reentrant=False).permute(1,0,2) \\\n",
    "            if self.grad_ckpt else self.extractor(xs.permute(1, 0, 2), x.permute(1, 0, 2)).permute(1, 0, 2)  # b,n,c\n",
    "        # xs = self.extractor(xs.permute(1,0,2),x.permute(1,0,2)).permute(1,0,2)#b,n,c\n",
    "        if self.extra_extractors is not None:\n",
    "            for extractor in self.extra_extractors:\n",
    "                xs = checkpoint.checkpoint(extractor, xs.permute(1, 0, 2), x.permute(1, 0, 2), use_reentrant=False).permute(1, 0, 2) \\\n",
    "                    if self.grad_ckpt else extractor(xs.permute(1, 0, 2), x.permute(1, 0, 2)).permute(1, 0,2)  # b,n,c\n",
    "                # xs = extractor(xs.permute(1,0,2),x.permute(1,0,2)).permute(1,0,2)\n",
    "        return x,xs\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, d_model: int, eps: float = 1e-5):\n",
    "        super().__init__()\n",
    "\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(d_model))\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps) * self.weight\n",
    "\n",
    "        return output\n",
    "    \n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self,dt_scale, d_model,d_inner,dt_rank,d_state,bias,d_conv,conv_bias,dt_init,dt_max,dt_min,dt_init_floor,grad_ckpt):\n",
    "        super().__init__()\n",
    "\n",
    "        self.grad_ckpt = grad_ckpt\n",
    "        self.mixer = MambaBlock(dt_scale,d_model,d_inner,dt_rank,d_state,bias,d_conv,conv_bias,dt_init,dt_max,dt_min,dt_init_floor)\n",
    "        self.norm = RMSNorm(d_model)\n",
    "\n",
    "    def forward(self, x, h):\n",
    "        #  x : (B, L, D)\n",
    "        # h : (B, L, ED, N)\n",
    "        #  output : (B,L, D)\n",
    "\n",
    "        x = self.norm(x)\n",
    "        output, h = checkpoint.checkpoint(self.mixer,x,h,use_reentrant=False) if self.grad_ckpt else self.mixer(x, h)\n",
    "        output = output + x\n",
    "        return output, h\n",
    "    \n",
    "class Mamba_Neck(nn.Module):\n",
    "    def __init__(self, in_channel=512,d_model=512,d_inner=1024,bias=False,n_layers=4,dt_rank=32,d_state=16,d_conv=3,dt_min=0.001,\n",
    "                 dt_max=0.1,dt_init='random',dt_scale=1.0,conv_bias=True,dt_init_floor=0.0001,grad_ckpt=False):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_inner = d_inner\n",
    "        self.bias = bias\n",
    "        self.dt_rank = dt_rank\n",
    "        self.d_state = d_state\n",
    "        self.dt_scale = dt_scale\n",
    "        self.num_channels = self.d_model\n",
    "        self.layers = nn.ModuleList(\n",
    "            [ResidualBlock(dt_scale,d_model,d_inner,dt_rank,d_state,bias,d_conv,conv_bias,dt_init,dt_max,dt_min,dt_init_floor,grad_ckpt)\n",
    "             for _ in range(n_layers)])\n",
    "        self.interactions = nn.ModuleList([\n",
    "            InteractionBlock(d_model=d_model,extra_extractor=(True if i == n_layers - 1 else False),grad_ckpt=grad_ckpt)\n",
    "            for i in range(n_layers)\n",
    "        ])\n",
    "        # self.norm_f = RMSNorm(config.d_model)\n",
    "\n",
    "    def forward(self, x,xs,h,blocks,interaction_indexes):\n",
    "        #  x : (B, L, D)\n",
    "        #  caches : [cache(layer) for all layers], cache : (h, inputs)\n",
    "\n",
    "        #  y : (B, L, D)\n",
    "        #  caches : [cache(layer) for all layers], cache : (h, inputs)\n",
    "        for i,index in enumerate(interaction_indexes):\n",
    "            xs, h[i] = self.layers[i](xs, h[i])\n",
    "            x,xs = self.interactions[i](x,xs,blocks[index[0]:index[1]])\n",
    "\n",
    "        return x, xs, h\n",
    "def build_neck(cfg,encoder):\n",
    "    in_channel = encoder.num_channels\n",
    "    d_model = cfg[\"MODEL\"][\"NECK\"][\"D_MODEL\"]\n",
    "    n_layers = cfg[\"MODEL\"][\"NECK\"][\"N_LAYERS\"]\n",
    "    d_state = cfg[\"MODEL\"][\"NECK\"][\"D_STATE\"]\n",
    "    grad_ckpt = cfg[\"MODEL\"][\"ENCODER\"][\"GRAD_CKPT\"]\n",
    "    neck = Mamba_Neck(in_channel=in_channel,d_model=d_model,d_inner=2*d_model,n_layers=n_layers,dt_rank=d_model//16,d_state=d_state,grad_ckpt=grad_ckpt)\n",
    "    return neck\n",
    "\n",
    "def box_xyxy_to_cxcywh(x):\n",
    "    x0, y0, x1, y1 = x.unbind(-1)\n",
    "    b = [(x0 + x1) / 2, (y0 + y1) / 2,\n",
    "         (x1 - x0), (y1 - y0)]\n",
    "    return torch.stack(b, dim=-1)\n",
    "\n",
    "class MLPPredictor(nn.Module):\n",
    "    def __init__(self, inplanes=64, channel=256, feat_sz=20, stride=16):\n",
    "        super(MLPPredictor, self).__init__()\n",
    "        self.feat_sz = feat_sz\n",
    "        self.stride = stride\n",
    "        self.img_sz = self.feat_sz * self.stride\n",
    "\n",
    "        self.num_layers = 3\n",
    "        h = [channel] * (self.num_layers - 1)\n",
    "        self.layers_cls = nn.ModuleList(nn.Linear(n, k)\n",
    "                                        for n, k in zip([inplanes] + h, h + [1]))\n",
    "        self.layers_reg = nn.ModuleList(nn.Linear(n, k)\n",
    "                                        for n, k in zip([inplanes] + h, h + [4]))\n",
    "\n",
    "        # for p in self.parameters():\n",
    "        #     if p.dim() > 1:\n",
    "        #         nn.init.xavier_uniform_(p)\n",
    "\n",
    "    def forward(self, x, gt_score_map=None):\n",
    "        \"\"\" Forward pass with input x. \"\"\"\n",
    "        score_map, offset_map = self.get_score_map(x)\n",
    "\n",
    "        # assert gt_score_map is None\n",
    "        if gt_score_map is None:\n",
    "            bbox = self.cal_bbox(score_map, offset_map)\n",
    "        else:\n",
    "            bbox = self.cal_bbox(gt_score_map.unsqueeze(1), offset_map)\n",
    "\n",
    "        return score_map, bbox, offset_map\n",
    "\n",
    "    def cal_bbox(self, score_map, offset_map, return_score=False):\n",
    "        max_score, idx = torch.max(score_map.flatten(1), dim=1, keepdim=True)\n",
    "        idx_y = torch.div(idx, self.feat_sz, rounding_mode='floor')\n",
    "        idx_x = idx % self.feat_sz\n",
    "\n",
    "        idx = idx.unsqueeze(1).expand(idx.shape[0], 4, 1) # torch.Size([32, 4, 1])\n",
    "        offset = offset_map.flatten(2).gather(dim=2, index=idx).squeeze(-1)\n",
    "        # offset: (l,t,r,b)\n",
    "\n",
    "        # x1, y1, x2, y2\n",
    "        bbox = torch.cat([idx_x.to(torch.float) / self.feat_sz - offset[:, :1], # the offset should not divide the self.feat_sz, since I use the sigmoid to limit it in (0,1)\n",
    "                          idx_y.to(torch.float) / self.feat_sz - offset[:, 1:2],\n",
    "                          idx_x.to(torch.float) / self.feat_sz + offset[:, 2:3],\n",
    "                          idx_y.to(torch.float) / self.feat_sz + offset[:, 3:4],\n",
    "                          ], dim=1)\n",
    "        bbox = box_xyxy_to_cxcywh(bbox)\n",
    "        if return_score:\n",
    "            return bbox, max_score\n",
    "        return bbox\n",
    "\n",
    "    def get_score_map(self, x):\n",
    "\n",
    "        def _sigmoid(x):\n",
    "            y = torch.clamp(x.sigmoid_(), min=1e-4, max=1 - 1e-4)\n",
    "            return y\n",
    "\n",
    "        x_cls = x\n",
    "        for i, layer in enumerate(self.layers_cls):\n",
    "            x_cls = F.relu(layer(x_cls)) if i < self.num_layers - 1 else layer(x_cls)\n",
    "        x_cls = x_cls.permute(0,2,1).reshape(-1,1,self.feat_sz,self.feat_sz)\n",
    "\n",
    "        x_reg = x\n",
    "        for i, layer in enumerate(self.layers_reg):\n",
    "            x_reg = F.relu(layer(x_reg)) if i < self.num_layers - 1 else layer(x_reg)\n",
    "        x_reg = x_reg.permute(0, 2, 1).reshape(-1, 4, self.feat_sz, self.feat_sz)\n",
    "\n",
    "        return _sigmoid(x_cls), _sigmoid(x_reg)\n",
    "    \n",
    "class FrozenBatchNorm2d(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    BatchNorm2d where the batch statistics and the affine parameters are fixed.\n",
    "\n",
    "    Copy-paste from torchvision.misc.ops with added eps before rqsrt,\n",
    "    without which any other models than torchvision.models.resnet[18,34,50,101]\n",
    "    produce nans.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n):\n",
    "        super(FrozenBatchNorm2d, self).__init__()\n",
    "        self.register_buffer(\"weight\", torch.ones(n))\n",
    "        self.register_buffer(\"bias\", torch.zeros(n))\n",
    "        self.register_buffer(\"running_mean\", torch.zeros(n))\n",
    "        self.register_buffer(\"running_var\", torch.ones(n))\n",
    "\n",
    "    def _load_from_state_dict(self, state_dict, prefix, local_metadata, strict,\n",
    "                              missing_keys, unexpected_keys, error_msgs):\n",
    "        num_batches_tracked_key = prefix + 'num_batches_tracked'\n",
    "        if num_batches_tracked_key in state_dict:\n",
    "            del state_dict[num_batches_tracked_key]\n",
    "\n",
    "        super(FrozenBatchNorm2d, self)._load_from_state_dict(\n",
    "            state_dict, prefix, local_metadata, strict,\n",
    "            missing_keys, unexpected_keys, error_msgs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # move reshapes to the beginning\n",
    "        # to make it fuser-friendly\n",
    "        w = self.weight.reshape(1, -1, 1, 1)\n",
    "        b = self.bias.reshape(1, -1, 1, 1)\n",
    "        rv = self.running_var.reshape(1, -1, 1, 1)\n",
    "        rm = self.running_mean.reshape(1, -1, 1, 1)\n",
    "        eps = 1e-5\n",
    "        scale = w * (rv + eps).rsqrt()  # rsqrt(x): 1/sqrt(x), r: reciprocal\n",
    "        bias = b - rm * scale\n",
    "        return x * scale + bias\n",
    "\n",
    "def conv(in_planes, out_planes, kernel_size=3, stride=1, padding=1, dilation=1,\n",
    "         freeze_bn=False):\n",
    "    if freeze_bn:\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride,\n",
    "                      padding=padding, dilation=dilation, bias=True),\n",
    "            FrozenBatchNorm2d(out_planes),\n",
    "            nn.ReLU(inplace=True))\n",
    "    else:\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride,\n",
    "                      padding=padding, dilation=dilation, bias=True),\n",
    "            nn.BatchNorm2d(out_planes),\n",
    "            nn.ReLU(inplace=True))\n",
    "    \n",
    "def conv(in_planes, out_planes, kernel_size=3, stride=1, padding=1, dilation=1,\n",
    "         freeze_bn=False):\n",
    "    if freeze_bn:\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride,\n",
    "                      padding=padding, dilation=dilation, bias=True),\n",
    "            FrozenBatchNorm2d(out_planes),\n",
    "            nn.ReLU(inplace=True))\n",
    "    else:\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride,\n",
    "                      padding=padding, dilation=dilation, bias=True),\n",
    "            nn.BatchNorm2d(out_planes),\n",
    "            nn.ReLU(inplace=True))\n",
    "    \n",
    "class CenterPredictor(nn.Module, ):\n",
    "    def __init__(self, inplanes=64, channel=256, feat_sz=20, stride=16, freeze_bn=False):\n",
    "        super(CenterPredictor, self).__init__()\n",
    "        self.feat_sz = feat_sz\n",
    "        self.stride = stride\n",
    "        self.img_sz = self.feat_sz * self.stride\n",
    "\n",
    "        # corner predict\n",
    "        self.conv1_ctr = conv(inplanes, channel, freeze_bn=freeze_bn)\n",
    "        self.conv2_ctr = conv(channel, channel // 2, freeze_bn=freeze_bn)\n",
    "        self.conv3_ctr = conv(channel // 2, channel // 4, freeze_bn=freeze_bn)\n",
    "        self.conv4_ctr = conv(channel // 4, channel // 8, freeze_bn=freeze_bn)\n",
    "        self.conv5_ctr = nn.Conv2d(channel // 8, 1, kernel_size=1)\n",
    "\n",
    "        # size regress\n",
    "        self.conv1_offset = conv(inplanes, channel, freeze_bn=freeze_bn)\n",
    "        self.conv2_offset = conv(channel, channel // 2, freeze_bn=freeze_bn)\n",
    "        self.conv3_offset = conv(channel // 2, channel // 4, freeze_bn=freeze_bn)\n",
    "        self.conv4_offset = conv(channel // 4, channel // 8, freeze_bn=freeze_bn)\n",
    "        self.conv5_offset = nn.Conv2d(channel // 8, 2, kernel_size=1)\n",
    "\n",
    "        # size regress\n",
    "        self.conv1_size = conv(inplanes, channel, freeze_bn=freeze_bn)\n",
    "        self.conv2_size = conv(channel, channel // 2, freeze_bn=freeze_bn)\n",
    "        self.conv3_size = conv(channel // 2, channel // 4, freeze_bn=freeze_bn)\n",
    "        self.conv4_size = conv(channel // 4, channel // 8, freeze_bn=freeze_bn)\n",
    "        self.conv5_size = nn.Conv2d(channel // 8, 2, kernel_size=1)\n",
    "\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "\n",
    "    def forward(self, x, gt_score_map=None):\n",
    "        \"\"\" Forward pass with input x. \"\"\"\n",
    "        score_map_ctr, size_map, offset_map = self.get_score_map(x) # x: torch.Size([b, c, h, w])\n",
    "        # score_map_ctr: torch.Size([32, 1, 16, 16]) size_map: torch.Size([32, 2, 16, 16]) offset_map: torch.Size([32, 2, 16, 16])\n",
    "\n",
    "        # assert gt_score_map is None\n",
    "        if gt_score_map is None:\n",
    "            bbox = self.cal_bbox(score_map_ctr, size_map, offset_map)\n",
    "        else:\n",
    "            bbox = self.cal_bbox(gt_score_map.unsqueeze(1), size_map, offset_map)\n",
    "\n",
    "        return score_map_ctr, bbox, size_map, offset_map\n",
    "\n",
    "    def cal_bbox(self, score_map_ctr, size_map, offset_map, return_score=False):\n",
    "        max_score, idx = torch.max(score_map_ctr.flatten(1), dim=1, keepdim=True) # score_map_ctr.flatten(1): torch.Size([32, 256]) idx: torch.Size([32, 1]) max_score: torch.Size([32, 1])\n",
    "        idx_y = torch.div(idx, self.feat_sz, rounding_mode='floor')\n",
    "        idx_x = idx % self.feat_sz\n",
    "\n",
    "        idx = idx.unsqueeze(1).expand(idx.shape[0], 2, 1)\n",
    "        size = size_map.flatten(2).gather(dim=2, index=idx) # size_map: torch.Size([32, 2, 16, 16])  size_map.flatten(2): torch.Size([32, 2, 256])\n",
    "        offset = offset_map.flatten(2).gather(dim=2, index=idx).squeeze(-1)\n",
    "\n",
    "        # bbox = torch.cat([idx_x - size[:, 0] / 2, idx_y - size[:, 1] / 2,\n",
    "        #                   idx_x + size[:, 0] / 2, idx_y + size[:, 1] / 2], dim=1) / self.feat_sz\n",
    "        # cx, cy, w, h\n",
    "        bbox = torch.cat([(idx_x.to(torch.float) + offset[:, :1]) / self.feat_sz,\n",
    "                          (idx_y.to(torch.float) + offset[:, 1:]) / self.feat_sz,\n",
    "                          size.squeeze(-1)], dim=1)\n",
    "\n",
    "        if return_score:\n",
    "            return bbox, max_score\n",
    "        return bbox\n",
    "\n",
    "    def get_pred(self, score_map_ctr, size_map, offset_map):\n",
    "        max_score, idx = torch.max(score_map_ctr.flatten(1), dim=1, keepdim=True)\n",
    "        idx_y = idx // self.feat_sz\n",
    "        idx_x = idx % self.feat_sz\n",
    "\n",
    "        idx = idx.unsqueeze(1).expand(idx.shape[0], 2, 1)\n",
    "        size = size_map.flatten(2).gather(dim=2, index=idx)\n",
    "        offset = offset_map.flatten(2).gather(dim=2, index=idx).squeeze(-1)\n",
    "\n",
    "        # bbox = torch.cat([idx_x - size[:, 0] / 2, idx_y - size[:, 1] / 2,\n",
    "        #                   idx_x + size[:, 0] / 2, idx_y + size[:, 1] / 2], dim=1) / self.feat_sz\n",
    "        return size * self.feat_sz, offset\n",
    "\n",
    "    def get_score_map(self, x):\n",
    "\n",
    "        def _sigmoid(x):\n",
    "            y = torch.clamp(x.sigmoid_(), min=1e-4, max=1 - 1e-4)\n",
    "            return y\n",
    "\n",
    "        # ctr branch\n",
    "        x_ctr1 = self.conv1_ctr(x)\n",
    "        x_ctr2 = self.conv2_ctr(x_ctr1)\n",
    "        x_ctr3 = self.conv3_ctr(x_ctr2)\n",
    "        x_ctr4 = self.conv4_ctr(x_ctr3)\n",
    "        score_map_ctr = self.conv5_ctr(x_ctr4)\n",
    "\n",
    "        # offset branch\n",
    "        x_offset1 = self.conv1_offset(x)\n",
    "        x_offset2 = self.conv2_offset(x_offset1)\n",
    "        x_offset3 = self.conv3_offset(x_offset2)\n",
    "        x_offset4 = self.conv4_offset(x_offset3)\n",
    "        score_map_offset = self.conv5_offset(x_offset4)\n",
    "\n",
    "        # size branch\n",
    "        x_size1 = self.conv1_size(x)\n",
    "        x_size2 = self.conv2_size(x_size1)\n",
    "        x_size3 = self.conv3_size(x_size2)\n",
    "        x_size4 = self.conv4_size(x_size3)\n",
    "        score_map_size = self.conv5_size(x_size4)\n",
    "        return _sigmoid(score_map_ctr), _sigmoid(score_map_size), score_map_offset\n",
    "    \n",
    "def build_decoder(cfg, encoder):\n",
    "    num_channels_enc = encoder.num_channels\n",
    "    stride = cfg[\"MODEL\"][\"ENCODER\"][\"STRIDE\"]\n",
    "    if cfg[\"MODEL\"][\"DECODER\"][\"TYPE\"] == \"MLP\":\n",
    "        in_channel = num_channels_enc\n",
    "        hidden_dim = cfg[\"MODEL\"][\"DECODER\"][\"NUM_CHANNELS\"]\n",
    "        feat_sz = int(cfg[\"DATA\"][\"SEARCH\"][\"SIZE\"] / stride)\n",
    "        mlp_head = MLPPredictor(inplanes=in_channel, channel=hidden_dim,\n",
    "                                feat_sz=feat_sz, stride=stride)\n",
    "        return mlp_head\n",
    "    elif \"CORNER\" in cfg[\"MODEL\"][\"DECODER\"][\"TYPE\"]:\n",
    "        feat_sz = int(cfg[\"DATA\"][\"SEARCH\"][\"SIZE\"] / stride)\n",
    "        channel = getattr(cfg[\"MODEL\"], \"NUM_CHANNELS\", 256)\n",
    "        print(\"head channel: %d\" % channel)\n",
    "        if cfg[\"MODEL\"][\"HEAD\"][\"TYPE\"] == \"CORNER\":\n",
    "            corner_head = Corner_Predictor(inplanes=cfg[\"MODEL\"][\"HIDDEN_DIM\"], channel=channel,\n",
    "                                           feat_sz=feat_sz, stride=stride)\n",
    "        else:\n",
    "            raise ValueError()\n",
    "        return corner_head\n",
    "    elif cfg[\"MODEL\"][\"DECODER\"][\"TYPE\"] == \"CENTER\":\n",
    "        in_channel = num_channels_enc\n",
    "        out_channel = cfg[\"MODEL\"][\"DECODER\"][\"NUM_CHANNELS\"]\n",
    "        feat_sz = int(cfg[\"DATA\"][\"SEARCH\"][\"SIZE\"] / stride)\n",
    "        center_head = CenterPredictor(inplanes=in_channel, channel=out_channel,\n",
    "                                      feat_sz=feat_sz, stride=stride)\n",
    "        return center_head\n",
    "    else:\n",
    "        raise ValueError(\"HEAD TYPE %s is not supported.\" % cfg[\"MODEL\"][\"HEAD_TYPE\"])\n",
    "    \n",
    "class Corner_Predictor(nn.Module):\n",
    "    \"\"\" Corner Predictor module\"\"\"\n",
    "\n",
    "    def __init__(self, inplanes=64, channel=256, feat_sz=20, stride=16, freeze_bn=False):\n",
    "        super(Corner_Predictor, self).__init__()\n",
    "        self.feat_sz = feat_sz\n",
    "        self.stride = stride\n",
    "        self.img_sz = self.feat_sz * self.stride\n",
    "        '''top-left corner'''\n",
    "        self.conv1_tl = conv(inplanes, channel, freeze_bn=freeze_bn)\n",
    "        self.conv2_tl = conv(channel, channel // 2, freeze_bn=freeze_bn)\n",
    "        self.conv3_tl = conv(channel // 2, channel // 4, freeze_bn=freeze_bn)\n",
    "        self.conv4_tl = conv(channel // 4, channel // 8, freeze_bn=freeze_bn)\n",
    "        self.conv5_tl = nn.Conv2d(channel // 8, 1, kernel_size=1)\n",
    "\n",
    "        '''bottom-right corner'''\n",
    "        self.conv1_br = conv(inplanes, channel, freeze_bn=freeze_bn)\n",
    "        self.conv2_br = conv(channel, channel // 2, freeze_bn=freeze_bn)\n",
    "        self.conv3_br = conv(channel // 2, channel // 4, freeze_bn=freeze_bn)\n",
    "        self.conv4_br = conv(channel // 4, channel // 8, freeze_bn=freeze_bn)\n",
    "        self.conv5_br = nn.Conv2d(channel // 8, 1, kernel_size=1)\n",
    "\n",
    "        '''about coordinates and indexs'''\n",
    "        with torch.no_grad():\n",
    "            self.indice = torch.arange(0, self.feat_sz).view(-1, 1) * self.stride\n",
    "            # generate mesh-grid\n",
    "            self.coord_x = self.indice.repeat((self.feat_sz, 1)) \\\n",
    "                .view((self.feat_sz * self.feat_sz,)).float().cuda()\n",
    "            self.coord_y = self.indice.repeat((1, self.feat_sz)) \\\n",
    "                .view((self.feat_sz * self.feat_sz,)).float().cuda()\n",
    "\n",
    "    def forward(self, x, return_dist=False, softmax=True):\n",
    "        \"\"\" Forward pass with input x. \"\"\"\n",
    "        score_map_tl, score_map_br = self.get_score_map(x)\n",
    "        if return_dist:\n",
    "            coorx_tl, coory_tl, prob_vec_tl = self.soft_argmax(score_map_tl, return_dist=True, softmax=softmax)\n",
    "            coorx_br, coory_br, prob_vec_br = self.soft_argmax(score_map_br, return_dist=True, softmax=softmax)\n",
    "            return torch.stack((coorx_tl, coory_tl, coorx_br, coory_br), dim=1) / self.img_sz, prob_vec_tl, prob_vec_br\n",
    "        else:\n",
    "            coorx_tl, coory_tl = self.soft_argmax(score_map_tl)\n",
    "            coorx_br, coory_br = self.soft_argmax(score_map_br)\n",
    "            return torch.stack((coorx_tl, coory_tl, coorx_br, coory_br), dim=1) / self.img_sz\n",
    "\n",
    "    def get_score_map(self, x):\n",
    "        # top-left branch\n",
    "        x_tl1 = self.conv1_tl(x)\n",
    "        x_tl2 = self.conv2_tl(x_tl1)\n",
    "        x_tl3 = self.conv3_tl(x_tl2)\n",
    "        x_tl4 = self.conv4_tl(x_tl3)\n",
    "        score_map_tl = self.conv5_tl(x_tl4)\n",
    "\n",
    "        # bottom-right branch\n",
    "        x_br1 = self.conv1_br(x)\n",
    "        x_br2 = self.conv2_br(x_br1)\n",
    "        x_br3 = self.conv3_br(x_br2)\n",
    "        x_br4 = self.conv4_br(x_br3)\n",
    "        score_map_br = self.conv5_br(x_br4)\n",
    "        return score_map_tl, score_map_br\n",
    "\n",
    "    def soft_argmax(self, score_map, return_dist=False, softmax=True):\n",
    "        \"\"\" get soft-argmax coordinate for a given heatmap \"\"\"\n",
    "        score_vec = score_map.view((-1, self.feat_sz * self.feat_sz))  # (batch, feat_sz * feat_sz)\n",
    "        prob_vec = nn.functional.softmax(score_vec, dim=1)\n",
    "        exp_x = torch.sum((self.coord_x * prob_vec), dim=1)\n",
    "        exp_y = torch.sum((self.coord_y * prob_vec), dim=1)\n",
    "        if return_dist:\n",
    "            if softmax:\n",
    "                return exp_x, exp_y, prob_vec\n",
    "            else:\n",
    "                return exp_x, exp_y, score_vec\n",
    "        else:\n",
    "            return exp_x, exp_y\n",
    "        \n",
    "class Preprocessor(object):\n",
    "    def __init__(self):\n",
    "        self.mean = torch.tensor([0.485, 0.456, 0.406]).view((1, 3, 1, 1)).cuda()\n",
    "        self.std = torch.tensor([0.229, 0.224, 0.225]).view((1, 3, 1, 1)).cuda()\n",
    "        self.mm_mean = torch.tensor([0.485, 0.456, 0.406, 0.485, 0.456, 0.406]).view((1, 6, 1, 1)).cuda()\n",
    "        self.mm_std = torch.tensor([0.229, 0.224, 0.225, 0.229, 0.224, 0.225]).view((1, 6, 1, 1)).cuda()\n",
    "\n",
    "    def process(self, img_arr: np.ndarray):\n",
    "        if img_arr.shape[-1] == 6:\n",
    "            mean = self.mm_mean\n",
    "            std = self.mm_std\n",
    "        else:\n",
    "            mean = self.mean\n",
    "            std = self.std\n",
    "        # Deal with the image patch\n",
    "        img_tensor = torch.tensor(img_arr).cuda().float().permute((2,0,1)).unsqueeze(dim=0)\n",
    "        # img_tensor = torch.tensor(img_arr).float().permute((2,0,1)).unsqueeze(dim=0)\n",
    "        img_tensor_norm = ((img_tensor / 255.0) - mean) / std  # (1,3,H,W)\n",
    "        return img_tensor_norm\n",
    "    \n",
    "def hann1d(sz: int, centered = True) -> torch.Tensor:\n",
    "    \"\"\"1D cosine window.\"\"\"\n",
    "    if centered:\n",
    "        return 0.5 * (1 - torch.cos((2 * math.pi / (sz + 1)) * torch.arange(1, sz + 1).float()))\n",
    "    w = 0.5 * (1 + torch.cos((2 * math.pi / (sz + 2)) * torch.arange(0, sz//2 + 1).float()))\n",
    "    return torch.cat([w, w[1:sz-sz//2].flip((0,))])\n",
    "    \n",
    "def hann2d(sz: torch.Tensor, centered = True) -> torch.Tensor:\n",
    "    \"\"\"2D cosine window.\"\"\"\n",
    "    return hann1d(sz[0].item(), centered).reshape(1, 1, -1, 1) * hann1d(sz[1].item(), centered).reshape(1, 1, 1, -1)    \n",
    "\n",
    "def sample_target(im, target_bb, search_area_factor, output_sz=None):\n",
    "    \"\"\" Extracts a square crop centered at target_bb box, of area search_area_factor^2 times target_bb area\n",
    "\n",
    "    args:\n",
    "        im - cv image\n",
    "        target_bb - target box [x, y, w, h]\n",
    "        search_area_factor - Ratio of crop size to target size\n",
    "        output_sz - (float) Size to which the extracted crop is resized (always square). If None, no resizing is done.\n",
    "\n",
    "    returns:\n",
    "        cv image - extracted crop\n",
    "        float - the factor by which the crop has been resized to make the crop size equal output_size\n",
    "    \"\"\"\n",
    "    if not isinstance(target_bb, list):\n",
    "        x, y, w, h = target_bb.tolist()\n",
    "    else:\n",
    "        x, y, w, h = target_bb\n",
    "    # Crop image\n",
    "    crop_sz = math.ceil(math.sqrt(w * h) * search_area_factor)\n",
    "\n",
    "    if crop_sz < 1:\n",
    "        raise Exception('Too small bounding box.')\n",
    "\n",
    "    x1 = round(x + 0.5 * w - crop_sz * 0.5)\n",
    "    x2 = x1 + crop_sz\n",
    "\n",
    "    y1 = round(y + 0.5 * h - crop_sz * 0.5)\n",
    "    y2 = y1 + crop_sz\n",
    "\n",
    "    x1_pad = max(0, -x1)\n",
    "    x2_pad = max(x2 - im.shape[1] + 1, 0)\n",
    "\n",
    "    y1_pad = max(0, -y1)\n",
    "    y2_pad = max(y2 - im.shape[0] + 1, 0)\n",
    "\n",
    "    # Crop target\n",
    "    im_crop = im[y1 + y1_pad:y2 - y2_pad, x1 + x1_pad:x2 - x2_pad, :]\n",
    "\n",
    "    # Pad\n",
    "    im_crop_padded = cv2.copyMakeBorder(im_crop, y1_pad, y2_pad, x1_pad, x2_pad, cv2.BORDER_CONSTANT)\n",
    "    # deal with attention mask\n",
    "    H, W, _ = im_crop_padded.shape\n",
    "\n",
    "    if output_sz is not None:\n",
    "        resize_factor = output_sz / crop_sz\n",
    "        im_crop_padded = cv2.resize(im_crop_padded, (output_sz, output_sz))\n",
    "\n",
    "        return im_crop_padded, resize_factor\n",
    "\n",
    "    else:\n",
    "        return im_crop_padded, 1.0\n",
    "def transform_image_to_crop(box_in: torch.Tensor, box_extract: torch.Tensor, resize_factor: float,\n",
    "                            crop_sz: torch.Tensor, normalize=False) -> torch.Tensor:\n",
    "    \"\"\" Transform the box co-ordinates from the original image co-ordinates to the co-ordinates of the cropped image\n",
    "    args:\n",
    "        box_in - the box for which the co-ordinates are to be transformed\n",
    "        box_extract - the box about which the image crop has been extracted.\n",
    "        resize_factor - the ratio between the original image scale and the scale of the image crop\n",
    "        crop_sz - size of the cropped image\n",
    "\n",
    "    returns:\n",
    "        torch.Tensor - transformed co-ordinates of box_in\n",
    "    \"\"\"\n",
    "    box_extract_center = box_extract[0:2] + 0.5 * box_extract[2:4]\n",
    "\n",
    "    box_in_center = box_in[0:2] + 0.5 * box_in[2:4]\n",
    "\n",
    "    box_out_center = (crop_sz - 1) / 2 + (box_in_center - box_extract_center) * resize_factor\n",
    "    box_out_wh = box_in[2:4] * resize_factor\n",
    "\n",
    "    box_out = torch.cat((box_out_center - 0.5 * box_out_wh, box_out_wh))\n",
    "    if normalize:\n",
    "        return box_out / (crop_sz[0]-1)\n",
    "    else:\n",
    "        return box_out\n",
    "def clip_box(box: list, H, W, margin=0):\n",
    "    x1, y1, w, h = box\n",
    "    x2, y2 = x1 + w, y1 + h\n",
    "    x1 = min(max(0, x1), W-margin)\n",
    "    x2 = min(max(margin, x2), W)\n",
    "    y1 = min(max(0, y1), H-margin)\n",
    "    y2 = min(max(margin, y2), H)\n",
    "    w = max(margin, x2-x1)\n",
    "    h = max(margin, y2-y1)\n",
    "    return [x1, y1, w, h]\n",
    "\n",
    "class BaseTracker:\n",
    "    \"\"\"Base class for all trackers.\"\"\"\n",
    "\n",
    "    def __init__(self, params):\n",
    "        self.params = params\n",
    "        self.visdom = None\n",
    "\n",
    "    def predicts_segmentation_mask(self):\n",
    "        return False\n",
    "\n",
    "    def initialize(self, image, info: dict) -> dict:\n",
    "        \"\"\"Overload this function in your tracker. This should initialize the model.\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def track(self, image, info: dict = None) -> dict:\n",
    "        \"\"\"Overload this function in your tracker. This should track in the frame and update the model.\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def visdom_draw_tracking(self, image, box, segmentation=None):\n",
    "        # Упрощенная обработка box без OrderedDict\n",
    "        if isinstance(box, dict):  # Проверяем на обычный dict вместо OrderedDict\n",
    "            box = list(box.values())  # Берем только значения\n",
    "        elif not isinstance(box, (list, tuple)):  # Если не коллекция\n",
    "            box = (box,)  # Превращаем в кортеж\n",
    "        \n",
    "        # Визуализация\n",
    "        if segmentation is None:\n",
    "            self.visdom.register((image, *box), 'Tracking', 1, 'Tracking')\n",
    "        else:\n",
    "            self.visdom.register((image, *box, segmentation), 'Tracking', 1, 'Tracking')\n",
    "\n",
    "\n",
    "class MCITRACK(BaseTracker):\n",
    "    def __init__(self, params):\n",
    "        \n",
    "        super(MCITRACK, self).__init__(params)\n",
    "        network = build_mcitrack(params.cfg)\n",
    "        network.load_state_dict(torch.load(\"MCITRACK_ep0300.pth.tar\", map_location='cpu')['net'], strict=True)\n",
    "        \n",
    "        self.cfg = params.cfg\n",
    "        self.network = network.cuda()\n",
    "        self.network.eval()\n",
    "        self.preprocessor = Preprocessor()\n",
    "        self.state = None\n",
    "\n",
    "        self.fx_sz = self.cfg[\"TEST\"][\"SEARCH_SIZE\"] // self.cfg[\"MODEL\"][\"ENCODER\"][\"STRIDE\"]\n",
    "        if self.cfg[\"TEST\"][\"WINDOW\"] == True:  # for window penalty\n",
    "            self.output_window = hann2d(torch.tensor([self.fx_sz, self.fx_sz]).long(), centered=True).cuda()\n",
    "\n",
    "        self.num_template = self.cfg[\"TEST\"][\"NUM_TEMPLATES\"]\n",
    "\n",
    "   \n",
    "        self.frame_id = 0\n",
    "        # for update\n",
    "        self.h_state = [None] * self.cfg[\"MODEL\"][\"NECK\"][\"N_LAYERS\"]\n",
    "\n",
    "\n",
    "\n",
    "        self.memory_bank = self.cfg[\"TEST\"][\"MB\"][\"DEFAULT\"]\n",
    "        self.update_h_t = self.cfg[\"TEST\"][\"UPH\"][\"DEFAULT\"]\n",
    "        self.update_threshold = self.cfg[\"TEST\"][\"UPT\"][\"DEFAULT\"]\n",
    "        self.update_intervals = self.cfg[\"TEST\"][\"INTER\"][\"DEFAULT\"]\n",
    "        print(\"Update threshold is: \", self.memory_bank)\n",
    "\n",
    "    def initialize(self, image, info: dict):\n",
    "\n",
    "\n",
    "        # get the initial templates\n",
    "        z_patch_arr, resize_factor = sample_target(image, info['init_bbox'], self.params.template_factor,\n",
    "                                                   output_sz=self.params.template_size)\n",
    "        z_patch_arr = z_patch_arr\n",
    "        template = self.preprocessor.process(z_patch_arr)\n",
    "        self.template_list = [template] * self.num_template\n",
    "\n",
    "        self.state = info['init_bbox']\n",
    "        prev_box_crop = transform_image_to_crop(torch.tensor(info['init_bbox']),\n",
    "                                                torch.tensor(info['init_bbox']),\n",
    "                                                resize_factor,\n",
    "                                                torch.Tensor([self.params.template_size, self.params.template_size]),\n",
    "                                                normalize=True)\n",
    "        self.template_anno_list = [prev_box_crop.to(template.device).unsqueeze(0)] * self.num_template\n",
    "        self.frame_id = 0\n",
    "        self.memory_template_list = self.template_list.copy()\n",
    "        self.memory_template_anno_list = self.template_anno_list.copy()\n",
    "\n",
    "\n",
    "    def track(self, image, info: dict = None):\n",
    "        H, W, _ = image.shape\n",
    "        self.frame_id += 1\n",
    "        x_patch_arr, resize_factor = sample_target(image, self.state, self.params.search_factor,\n",
    "                                                   output_sz=self.params.search_size)  # (x1, y1, w, h)\n",
    "        search = self.preprocessor.process(x_patch_arr)\n",
    "        search_list = [search]\n",
    "\n",
    "        # run the encoder\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            enc_opt = self.network.forward_encoder(self.template_list, search_list, self.template_anno_list)\n",
    "\n",
    "        # run the time neck\n",
    "        with torch.no_grad():\n",
    "            hidden_state = self.h_state.copy()\n",
    "            encoder_out,out_neck, h = self.network.forward_neck(enc_opt, hidden_state)\n",
    "        # run the decoder\n",
    "        with torch.no_grad():\n",
    "            out_dict = self.network.forward_decoder(feature=out_neck)\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            out_dict = self.network.forward(\n",
    "                template_list=self.template_list,\n",
    "                search_list=search_list,\n",
    "                template_anno_list=self.template_anno_list,\n",
    "                \n",
    "                gt_score_map=None\n",
    "            )\n",
    "            \n",
    "        \"\"\"print(len(self.template_list))\n",
    "        for i in self.template_list:\n",
    "            print(i.shape)\n",
    "        print(len(search_list))\n",
    "        for i in search_list:\n",
    "            print(i.shape)\n",
    "        print(len(self.template_anno_list))\n",
    "        for i in self.template_anno_list:\n",
    "            print(i.shape)\n",
    "        print(len(self.h_state))\n",
    "        \n",
    "        print(\"-\"*50)\n",
    "        \"\"\"\n",
    "        \n",
    "\n",
    "        # add hann windows\n",
    "        pred_score_map = out_dict['score_map']\n",
    "        if self.cfg[\"TEST\"][\"WINDOW\"] == True:  # for window penalty\n",
    "            response = self.output_window * pred_score_map\n",
    "        else:\n",
    "            response = pred_score_map\n",
    "        if 'size_map' in out_dict.keys():\n",
    "            pred_boxes, conf_score = self.network.decoder.cal_bbox(response, out_dict['size_map'],\n",
    "                                                                   out_dict['offset_map'], return_score=True)\n",
    "        else:\n",
    "            pred_boxes, conf_score = self.network.decoder.cal_bbox(response,\n",
    "                                                                   out_dict['offset_map'],\n",
    "                                                                   return_score=True)\n",
    "        pred_boxes = pred_boxes.view(-1, 4)\n",
    "        # Baseline: Take the mean of all pred boxes as the final result\n",
    "        pred_box = (pred_boxes.mean(dim=0) * self.params.search_size / resize_factor).tolist()  # (cx, cy, w, h) [0,1]\n",
    "        # get the final box result\n",
    "        self.state = clip_box(self.map_box_back(pred_box, resize_factor), H, W, margin=10)\n",
    "        # update hiden state\n",
    "        self.h_state = h\n",
    "        if conf_score.item() < self.update_h_t:\n",
    "            self.h_state = [None] * self.cfg[\"MODEL\"][\"NECK\"][\"N_LAYERS\"]\n",
    "\n",
    "        # update the template\n",
    "        if self.num_template > 1:\n",
    "            if (conf_score > self.update_threshold):\n",
    "                z_patch_arr, resize_factor = sample_target(image, self.state, self.params.template_factor,\n",
    "                                                           output_sz=self.params.template_size)\n",
    "                template = self.preprocessor.process(z_patch_arr)\n",
    "                self.memory_template_list.append(template)\n",
    "                prev_box_crop = transform_image_to_crop(torch.tensor(self.state),\n",
    "                                                        torch.tensor(self.state),\n",
    "                                                        resize_factor,\n",
    "                                                        torch.Tensor(\n",
    "                                                            [self.params.template_size, self.params.template_size]),\n",
    "                                                        normalize=True)\n",
    "                self.memory_template_anno_list.append(prev_box_crop.to(template.device).unsqueeze(0))\n",
    "                if len(self.memory_template_list) > self.memory_bank:\n",
    "                    self.memory_template_list.pop(0)\n",
    "                    self.memory_template_anno_list.pop(0)\n",
    "        if (self.frame_id % self.update_intervals == 0):\n",
    "            assert len(self.memory_template_anno_list) == len(self.memory_template_list)\n",
    "            len_list = len(self.memory_template_anno_list)\n",
    "            interval = len_list // self.num_template\n",
    "            for i in range(1, self.num_template):\n",
    "                idx = interval * i\n",
    "                if idx > len_list:\n",
    "                    idx = len_list\n",
    "                self.template_list.append(self.memory_template_list[idx])\n",
    "                self.template_list.pop(1)\n",
    "                self.template_anno_list.append(self.memory_template_anno_list[idx])\n",
    "                self.template_anno_list.pop(1)\n",
    "        assert len(self.template_list) == self.num_template\n",
    "\n",
    "\n",
    "\n",
    "        return {\"target_bbox\": self.state,\n",
    "                \"best_score\": conf_score}\n",
    "\n",
    "    def map_box_back(self, pred_box: list, resize_factor: float):\n",
    "        cx_prev, cy_prev = self.state[0] + 0.5 * self.state[2], self.state[1] + 0.5 * self.state[3]\n",
    "        cx, cy, w, h = pred_box\n",
    "        half_side = 0.5 * self.params.search_size / resize_factor\n",
    "        cx_real = cx + (cx_prev - half_side)\n",
    "        cy_real = cy + (cy_prev - half_side)\n",
    "        return [cx_real - 0.5 * w, cy_real - 0.5 * h, w, h]\n",
    "\n",
    "    def map_box_back_batch(self, pred_box: torch.Tensor, resize_factor: float):\n",
    "        cx_prev, cy_prev = self.state[0] + 0.5 * self.state[2], self.state[1] + 0.5 * self.state[3]\n",
    "        cx, cy, w, h = pred_box.unbind(-1)  # (N,4) --> (N,)\n",
    "        half_side = 0.5 * self.params.search_size / resize_factor\n",
    "        cx_real = cx + (cx_prev - half_side)\n",
    "        cy_real = cy + (cy_prev - half_side)\n",
    "        return torch.stack([cx_real - 0.5 * w, cy_real - 0.5 * h, w, h], dim=-1)\n",
    "\n",
    "class MCITrack(nn.Module):\n",
    "    \"\"\" This is the base class for MCITrack \"\"\"\n",
    "    def __init__(self, encoder, decoder, neck, cfg,\n",
    "                 num_frames=1, num_template=1, decoder_type=\"CENTER\"):\n",
    "        \"\"\"\n",
    "        Initializes the model.\n",
    "\n",
    "        Parameters:\n",
    "            encoder: torch module of the encoder to be used. See encoder.py\n",
    "            decoder: torch module of the decoder architecture. See decoder.py\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder_type = decoder_type\n",
    "        self.neck = neck\n",
    "\n",
    "        self.num_patch_x = self.encoder.body.num_patches_search\n",
    "        self.num_patch_z = self.encoder.body.num_patches_template\n",
    "        self.fx_sz = int(math.sqrt(self.num_patch_x))\n",
    "        self.fz_sz = int(math.sqrt(self.num_patch_z))\n",
    "\n",
    "        self.decoder = decoder\n",
    "\n",
    "        self.num_frames = num_frames\n",
    "        self.num_template = num_template\n",
    "        self.freeze_en = cfg[\"TRAIN\"][\"FREEZE_ENCODER\"]\n",
    "        self.interaction_indexes = cfg[\"MODEL\"][\"ENCODER\"][\"INTERACTION_INDEXES\"]\n",
    "\n",
    "    def forward(self, template_list, search_list, template_anno_list, gt_score_map=None):\n",
    "        \"\"\"\n",
    "        Forward pass that sequentially executes the encoder, neck, and decoder operations.\n",
    "\n",
    "        Args:\n",
    "            template_list: List of template images.\n",
    "            search_list: List of search images.\n",
    "            template_anno_list: List of template annotations.\n",
    "            neck_h_state: Hidden state for the neck.\n",
    "            gt_score_map: Ground truth score map (optional, used in decoder).\n",
    "\n",
    "        Returns:\n",
    "            dict: Output from the decoder, including predicted bounding boxes, score maps, etc.\n",
    "        \"\"\"\n",
    "        # Step 1: Forward pass through the encoder\n",
    "        \n",
    "        neck_h_state=[None,None,None,None]\n",
    "        \n",
    "        xz = self.encoder(template_list, search_list, template_anno_list)\n",
    "\n",
    "        # Step 2: Forward pass through the neck\n",
    "        xs = xz[:, 0:self.num_patch_x]  # Extract patch embeddings\n",
    "        x, xs, h = self.neck(\n",
    "            xz, xs, neck_h_state, \n",
    "            self.encoder.body.blocks, \n",
    "            self.interaction_indexes\n",
    "        )\n",
    "        x = self.encoder.body.fc_norm(x)\n",
    "        xs = xs + x[:, 0:self.num_patch_x]  # Updated patch embeddings\n",
    "\n",
    "        # Step 3: Forward pass through the decoder\n",
    "        bs, HW, C = xs.size()\n",
    "        if self.decoder_type in ['CORNER', 'CENTER']:\n",
    "            xs = xs.permute((0, 2, 1)).contiguous()\n",
    "            xs = xs.view(bs, C, self.fx_sz, self.fx_sz)\n",
    "\n",
    "        if self.decoder_type == \"CORNER\":\n",
    "            # Run the corner head\n",
    "            pred_box, score_map = self.decoder(xs, True)\n",
    "            outputs_coord = box_xyxy_to_cxcywh(pred_box)\n",
    "            outputs_coord_new = outputs_coord.view(bs, 1, 4)\n",
    "            return {\n",
    "                'pred_boxes': outputs_coord_new,\n",
    "                'score_map': score_map\n",
    "            }\n",
    "\n",
    "        elif self.decoder_type == \"CENTER\":\n",
    "            # Run the center head\n",
    "            score_map_ctr, bbox, size_map, offset_map = self.decoder(xs, gt_score_map)\n",
    "            outputs_coord = bbox\n",
    "            outputs_coord_new = outputs_coord.view(bs, 1, 4)\n",
    "            return {\n",
    "                'pred_boxes': outputs_coord_new,\n",
    "                'score_map': score_map_ctr,\n",
    "                'size_map': size_map,\n",
    "                'offset_map': offset_map\n",
    "            }\n",
    "\n",
    "        elif self.decoder_type == \"MLP\":\n",
    "            # Run the MLP head\n",
    "            score_map, bbox, offset_map = self.decoder(xs, gt_score_map)\n",
    "            outputs_coord = bbox\n",
    "            outputs_coord_new = outputs_coord.view(bs, 1, 4)\n",
    "            return {\n",
    "                'pred_boxes': outputs_coord_new,\n",
    "                'score_map': score_map,\n",
    "                'offset_map': offset_map\n",
    "            }\n",
    "\n",
    "        else:\n",
    "            raise NotImplementedError(f\"Decoder type not supported: {self.decoder_type}\")\n",
    "\n",
    "def build_mcitrack(cfg):\n",
    "    encoder = build_encoder(cfg)\n",
    "    neck = build_neck(cfg,encoder)\n",
    "    decoder = build_decoder(cfg, neck)\n",
    "    model = MCITrack(\n",
    "        encoder,\n",
    "        decoder,\n",
    "        neck,\n",
    "        cfg,\n",
    "        num_frames = cfg[\"DATA\"][\"SEARCH\"][\"NUMBER\"],\n",
    "        num_template = cfg[\"DATA\"][\"TEMPLATE\"][\"NUMBER\"],\n",
    "        decoder_type=cfg[\"MODEL\"][\"DECODER\"][\"TYPE\"],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def get_tracker_class():\n",
    "    return MCITRACK\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "407a71c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = {}\n",
    "\n",
    "# MODEL\n",
    "cfg[\"MODEL\"] = {}\n",
    "\n",
    "# MODEL.ENCODER\n",
    "cfg[\"MODEL\"][\"ENCODER\"] = {\n",
    "    \"TYPE\": \"dinov2_vitb14\",  # encoder model\n",
    "    \"DROP_PATH\": 0,\n",
    "    \"PRETRAIN_TYPE\": \"mae\",  # mae, default, or scratch. This parameter is not activated for dinov2.\n",
    "    \"USE_CHECKPOINT\": False,  # to save the memory.\n",
    "    \"STRIDE\": 14,\n",
    "    \"POS_TYPE\": 'interpolate',  # type of loading the positional encoding. \"interpolate\" or \"index\".\n",
    "    \"TOKEN_TYPE_INDICATE\": False,  # add a token_type_embedding to indicate the search, template_foreground, template_background\n",
    "    \"INTERACTION_INDEXES\": [[0, 6], [6, 12], [12, 18], [18, 24]],\n",
    "    \"GRAD_CKPT\": False\n",
    "}\n",
    "\n",
    "# MODEL.NECK\n",
    "cfg[\"MODEL\"][\"NECK\"] = {\n",
    "    \"N_LAYERS\": 4,\n",
    "    \"D_MODEL\": 512,\n",
    "    \"D_STATE\": 16  # MAMABA_HIDDEN_STATE\n",
    "}\n",
    "\n",
    "# MODEL.DECODER\n",
    "cfg[\"MODEL\"][\"DECODER\"] = {\n",
    "    \"TYPE\": \"CENTER\",  # MLP, CORNER, CENTER\n",
    "    \"NUM_CHANNELS\": 256\n",
    "}\n",
    "\n",
    "# TRAIN\n",
    "cfg[\"TRAIN\"] = {\n",
    "    \"LR\": 0.0001,\n",
    "    \"WEIGHT_DECAY\": 0.0001,\n",
    "    \"EPOCH\": 500,\n",
    "    \"LR_DROP_EPOCH\": 400,\n",
    "    \"BATCH_SIZE\": 8,\n",
    "    \"NUM_WORKER\": 8,\n",
    "    \"OPTIMIZER\": \"ADAMW\",\n",
    "    \"ENCODER_MULTIPLIER\": 0.1,  # encoder's LR = this factor * LR\n",
    "    \"FREEZE_ENCODER\": False,  # for freezing the parameters of encoder\n",
    "    \"ENCODER_OPEN\": [],  # only for debug, open some layers of encoder when FREEZE_ENCODER is True\n",
    "    \"CE_WEIGHT\": 1.0,  # weight for cross-entropy loss\n",
    "    \"GIOU_WEIGHT\": 2.0,\n",
    "    \"L1_WEIGHT\": 5.0,\n",
    "    \"PRINT_INTERVAL\": 50,  # interval to print the training log\n",
    "    \"GRAD_CLIP_NORM\": 0.1,\n",
    "    \"FIX_BN\": False,\n",
    "    \"ENCODER_W\": \"\",\n",
    "    \"TYPE\": \"normal\",  # normal, peft or fft\n",
    "    \"PRETRAINED_PATH\": None\n",
    "}\n",
    "\n",
    "# TRAIN.SCHEDULER\n",
    "cfg[\"TRAIN\"][\"SCHEDULER\"] = {\n",
    "    \"TYPE\": \"step\",\n",
    "    \"DECAY_RATE\": 0.1\n",
    "}\n",
    "\n",
    "# DATA\n",
    "cfg[\"DATA\"] = {\n",
    "    \"MEAN\": [0.485, 0.456, 0.406],\n",
    "    \"STD\": [0.229, 0.224, 0.225],\n",
    "    \"MAX_SAMPLE_INTERVAL\": 200,\n",
    "    \"SAMPLER_MODE\": \"order\",\n",
    "    \"LOADER\": \"tracking\"\n",
    "}\n",
    "\n",
    "# DATA.TRAIN\n",
    "cfg[\"DATA\"][\"TRAIN\"] = {\n",
    "    \"DATASETS_NAME\": [\"LASOT\", \"GOT10K_vottrain\"],\n",
    "    \"DATASETS_RATIO\": [1, 1],\n",
    "    \"SAMPLE_PER_EPOCH\": 60000\n",
    "}\n",
    "\n",
    "# DATA.SEARCH\n",
    "cfg[\"DATA\"][\"SEARCH\"] = {\n",
    "    \"NUMBER\": 1,  # number of search region, only support 1 for now.\n",
    "    \"SIZE\": 256,\n",
    "    \"FACTOR\": 4.0,\n",
    "    \"CENTER_JITTER\": 3.5,\n",
    "    \"SCALE_JITTER\": 0.5\n",
    "}\n",
    "\n",
    "# DATA.TEMPLATE\n",
    "cfg[\"DATA\"][\"TEMPLATE\"] = {\n",
    "    \"NUMBER\": 1,\n",
    "    \"SIZE\": 128,\n",
    "    \"FACTOR\": 2.0,\n",
    "    \"CENTER_JITTER\": 0,\n",
    "    \"SCALE_JITTER\": 0\n",
    "}\n",
    "\n",
    "# TEST\n",
    "cfg[\"TEST\"] = {\n",
    "    \"TEMPLATE_FACTOR\": 4.0,\n",
    "    \"TEMPLATE_SIZE\": 256,\n",
    "    \"SEARCH_FACTOR\": 2.0,\n",
    "    \"SEARCH_SIZE\": 128,\n",
    "    \"EPOCH\": 500,\n",
    "    \"WINDOW\": False,  # window penalty\n",
    "    \"NUM_TEMPLATES\": 1\n",
    "}\n",
    "\n",
    "# TEST.UPT\n",
    "cfg[\"TEST\"][\"UPT\"] = {\n",
    "    \"DEFAULT\": 1,\n",
    "    \"LASOT\": 0,\n",
    "    \"LASOT_EXTENSION_SUBSET\": 0,\n",
    "    \"TRACKINGNET\": 0,\n",
    "    \"TNL2K\": 0,\n",
    "    \"NFS\": 0,\n",
    "    \"UAV\": 0,\n",
    "    \"VOT20\": 0,\n",
    "    \"GOT10K_TEST\": 0\n",
    "}\n",
    "\n",
    "# TEST.UPH\n",
    "cfg[\"TEST\"][\"UPH\"] = {\n",
    "    \"DEFAULT\": 1,\n",
    "    \"LASOT\": 0,\n",
    "    \"LASOT_EXTENSION_SUBSET\": 0,\n",
    "    \"TRACKINGNET\": 0,\n",
    "    \"TNL2K\": 0,\n",
    "    \"NFS\": 0,\n",
    "    \"UAV\": 0,\n",
    "    \"VOT20\": 0,\n",
    "    \"GOT10K_TEST\": 0\n",
    "}\n",
    "\n",
    "# TEST.INTER\n",
    "cfg[\"TEST\"][\"INTER\"] = {\n",
    "    \"DEFAULT\": 999999,\n",
    "    \"LASOT\": 0,\n",
    "    \"LASOT_EXTENSION_SUBSET\": 0,\n",
    "    \"TRACKINGNET\": 0,\n",
    "    \"TNL2K\": 0,\n",
    "    \"NFS\": 0,\n",
    "    \"UAV\": 0,\n",
    "    \"VOT20\": 0,\n",
    "    \"GOT10K_TEST\": 0\n",
    "}\n",
    "\n",
    "# TEST.MB\n",
    "cfg[\"TEST\"][\"MB\"] = {\n",
    "    \"DEFAULT\": 500,\n",
    "    \"LASOT\": 0,\n",
    "    \"LASOT_EXTENSION_SUBSET\": 0,\n",
    "    \"TRACKINGNET\": 0,\n",
    "    \"TNL2K\": 0,\n",
    "    \"NFS\": 0,\n",
    "    \"UAV\": 0,\n",
    "    \"VOT20\": 0,\n",
    "    \"GOT10K_TEST\": 0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f2804bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test config:  {'MODEL': {'ENCODER': {'TYPE': 'fastitpnt', 'DROP_PATH': 0.1, 'PRETRAIN_TYPE': './fast_itpn_tiny_1600e_1k.pt', 'USE_CHECKPOINT': False, 'STRIDE': 16, 'POS_TYPE': 'index', 'TOKEN_TYPE_INDICATE': True, 'INTERACTION_INDEXES': [[4, 7], [7, 10], [10, 13], [13, 16]], 'GRAD_CKPT': False}, 'NECK': {'N_LAYERS': 4, 'D_MODEL': 384, 'D_STATE': 16}, 'DECODER': {'TYPE': 'CENTER', 'NUM_CHANNELS': 256}}, 'TRAIN': {'LR': 0.0004, 'WEIGHT_DECAY': 0.0001, 'EPOCH': 300, 'LR_DROP_EPOCH': 240, 'BATCH_SIZE': 64, 'NUM_WORKER': 10, 'OPTIMIZER': 'ADAMW', 'ENCODER_MULTIPLIER': 0.1, 'FREEZE_ENCODER': False, 'ENCODER_OPEN': [], 'CE_WEIGHT': 1.0, 'GIOU_WEIGHT': 2.0, 'L1_WEIGHT': 5.0, 'PRINT_INTERVAL': 50, 'GRAD_CLIP_NORM': 0.1, 'FIX_BN': False, 'ENCODER_W': '', 'TYPE': 'normal', 'PRETRAINED_PATH': None, 'SCHEDULER': {'TYPE': 'step', 'DECAY_RATE': 0.1}}, 'DATA': {'MEAN': [0.485, 0.456, 0.406], 'STD': [0.229, 0.224, 0.225], 'MAX_SAMPLE_INTERVAL': 400, 'SAMPLER_MODE': 'order', 'LOADER': 'tracking', 'TRAIN': {'DATASETS_NAME': ['LASOT', 'GOT10K_vottrain', 'COCO17', 'TRACKINGNET', 'VASTTRACK'], 'DATASETS_RATIO': [1, 1, 1, 1, 1], 'SAMPLE_PER_EPOCH': 60000}, 'SEARCH': {'NUMBER': 2, 'SIZE': 224, 'FACTOR': 4.0, 'CENTER_JITTER': 3.5, 'SCALE_JITTER': 0.5}, 'TEMPLATE': {'NUMBER': 5, 'SIZE': 112, 'FACTOR': 2.0, 'CENTER_JITTER': 0, 'SCALE_JITTER': 0}}, 'TEST': {'TEMPLATE_FACTOR': 2.0, 'TEMPLATE_SIZE': 112, 'SEARCH_FACTOR': 4.0, 'SEARCH_SIZE': 224, 'EPOCH': 300, 'WINDOW': True, 'NUM_TEMPLATES': 5, 'UPT': {'DEFAULT': 1, 'LASOT': 0.8, 'LASOT_EXTENSION_SUBSET': 0.85, 'TRACKINGNET': 0.5, 'TNL2K': 0.5, 'NFS': 0.8, 'UAV': 0.2, 'VOT20': 0.4, 'GOT10K_TEST': 0}, 'UPH': {'DEFAULT': 1, 'LASOT': 0.88, 'LASOT_EXTENSION_SUBSET': 0.97, 'TRACKINGNET': 0.9, 'TNL2K': 0.9, 'NFS': 0.92, 'UAV': 0.91, 'VOT20': 0.94, 'GOT10K_TEST': 0}, 'INTER': {'DEFAULT': 999999, 'LASOT': 70, 'LASOT_EXTENSION_SUBSET': 50, 'TRACKINGNET': 20, 'TNL2K': 20, 'NFS': 90, 'UAV': 1, 'VOT20': 1, 'GOT10K_TEST': 0}, 'MB': {'DEFAULT': 500, 'LASOT': 500, 'LASOT_EXTENSION_SUBSET': 500, 'TRACKINGNET': 200, 'TNL2K': 500, 'NFS': 500, 'UAV': 400, 'VOT20': 500, 'GOT10K_TEST': 0}}}\n"
     ]
    }
   ],
   "source": [
    "class TrackerParams:\n",
    "    \"\"\"Class for tracker parameters.\"\"\"\n",
    "    def set_default_values(self, default_vals: dict):\n",
    "        for name, val in default_vals.items():\n",
    "            if not hasattr(self, name):\n",
    "                setattr(self, name, val)\n",
    "\n",
    "    def get(self, name: str, *default):\n",
    "        \"\"\"Get a parameter value with the given name. If it does not exists, it return the default value given as a\n",
    "        second argument or returns an error if no default value is given.\"\"\"\n",
    "        if len(default) > 1:\n",
    "            raise ValueError('Can only give one default value.')\n",
    "\n",
    "        if not default:\n",
    "            return getattr(self, name)\n",
    "\n",
    "        return getattr(self, name, default[0])\n",
    "\n",
    "    def has(self, name: str):\n",
    "        \"\"\"Check if there exist a parameter with the given name.\"\"\"\n",
    "        return hasattr(self, name)\n",
    "\n",
    "def _update_config(base_cfg, exp_cfg):\n",
    "    if isinstance(base_cfg, dict) and isinstance(exp_cfg, dict):\n",
    "        for k, v in exp_cfg.items():\n",
    "            if k in base_cfg:\n",
    "                if not isinstance(v, dict):\n",
    "                    base_cfg[k] = v\n",
    "                else:\n",
    "                    _update_config(base_cfg[k], v)\n",
    "            else:\n",
    "                raise ValueError(\"{} not exist in config.py\".format(k))\n",
    "    else:\n",
    "        return\n",
    "\n",
    "def update_config_from_file(filename):\n",
    "    exp_config = None\n",
    "    with open(filename) as f:\n",
    "        exp_config = yaml.safe_load(f)\n",
    "        _update_config(cfg, exp_config)\n",
    "    \n",
    "def parameters(yaml_name: str):\n",
    "    params = TrackerParams()\n",
    "\n",
    "    yaml_file = \"mcitrack_t224.yaml\"\n",
    "    update_config_from_file(yaml_file)\n",
    "    params.cfg = cfg\n",
    "    print(\"test config: \", cfg)\n",
    "\n",
    "    params.yaml_name = yaml_name\n",
    "    # template and search region\n",
    "    params.template_factor = cfg[\"TEST\"][\"TEMPLATE_FACTOR\"]\n",
    "    params.template_size = cfg[\"TEST\"][\"TEMPLATE_SIZE\"]\n",
    "    params.search_factor = cfg[\"TEST\"][\"SEARCH_FACTOR\"]\n",
    "    params.search_size = cfg[\"TEST\"][\"SEARCH_SIZE\"]\n",
    "\n",
    "    # Network checkpoint path\n",
    "    params.checkpoint = \"fast_itpn_tiny_1600e_1k.pt\"\n",
    "    # whether to save boxes from all queries\n",
    "    params.save_all_boxes = False\n",
    "\n",
    "    return params\n",
    "\n",
    "params = parameters(\"./mcitrack_t224.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae331c67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update threshold is:  500\n"
     ]
    }
   ],
   "source": [
    "treacker = MCITRACK(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "097a57d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MCITrack(\n",
       "  (encoder): Encoder(\n",
       "    (body): Fast_iTPN(\n",
       "      (patch_embed): ConvPatchEmbed(\n",
       "        (proj): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))\n",
       "      )\n",
       "      (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "      (blocks): ModuleList(\n",
       "        (0): ConvMlpBlock(\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): ConvMlp(\n",
       "            (fc1): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act): GELU(approximate=none)\n",
       "            (ffn_ln): LayerNorm((288,), eps=1e-06, elementwise_affine=True)\n",
       "            (fc2): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): ConvPatchMerge(\n",
       "          (norm): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
       "          (reduction): Conv2d(96, 192, kernel_size=(2, 2), stride=(2, 2))\n",
       "        )\n",
       "        (2): ConvMlpBlock(\n",
       "          (drop_path): DropPath(p=0.007692307699471712)\n",
       "          (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): ConvMlp(\n",
       "            (fc1): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act): GELU(approximate=none)\n",
       "            (ffn_ln): LayerNorm((576,), eps=1e-06, elementwise_affine=True)\n",
       "            (fc2): Conv2d(576, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): ConvPatchMerge(\n",
       "          (norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "          (reduction): Conv2d(192, 384, kernel_size=(2, 2), stride=(2, 2))\n",
       "        )\n",
       "        (4): Block(\n",
       "          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (q_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "            (k_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "            (v_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path): DropPath(p=0.015384615398943424)\n",
       "          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): SwiGLU(\n",
       "            (w1): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (w2): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (act): SiLU()\n",
       "            (ffn_ln): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "            (w3): Linear(in_features=1152, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): Block(\n",
       "          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (q_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "            (k_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "            (v_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path): DropPath(p=0.023076923564076424)\n",
       "          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): SwiGLU(\n",
       "            (w1): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (w2): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (act): SiLU()\n",
       "            (ffn_ln): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "            (w3): Linear(in_features=1152, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): Block(\n",
       "          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (q_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "            (k_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "            (v_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path): DropPath(p=0.03076923079788685)\n",
       "          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): SwiGLU(\n",
       "            (w1): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (w2): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (act): SiLU()\n",
       "            (ffn_ln): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "            (w3): Linear(in_features=1152, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): Block(\n",
       "          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (q_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "            (k_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "            (v_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path): DropPath(p=0.03846153989434242)\n",
       "          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): SwiGLU(\n",
       "            (w1): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (w2): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (act): SiLU()\n",
       "            (ffn_ln): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "            (w3): Linear(in_features=1152, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): Block(\n",
       "          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (q_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "            (k_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "            (v_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path): DropPath(p=0.04615384712815285)\n",
       "          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): SwiGLU(\n",
       "            (w1): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (w2): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (act): SiLU()\n",
       "            (ffn_ln): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "            (w3): Linear(in_features=1152, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): Block(\n",
       "          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (q_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "            (k_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "            (v_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path): DropPath(p=0.05384615436196327)\n",
       "          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): SwiGLU(\n",
       "            (w1): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (w2): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (act): SiLU()\n",
       "            (ffn_ln): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "            (w3): Linear(in_features=1152, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): Block(\n",
       "          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (q_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "            (k_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "            (v_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path): DropPath(p=0.0615384615957737)\n",
       "          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): SwiGLU(\n",
       "            (w1): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (w2): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (act): SiLU()\n",
       "            (ffn_ln): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "            (w3): Linear(in_features=1152, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): Block(\n",
       "          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (q_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "            (k_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "            (v_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path): DropPath(p=0.06923077255487442)\n",
       "          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): SwiGLU(\n",
       "            (w1): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (w2): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (act): SiLU()\n",
       "            (ffn_ln): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "            (w3): Linear(in_features=1152, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (12): Block(\n",
       "          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (q_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "            (k_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "            (v_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path): DropPath(p=0.07692307978868484)\n",
       "          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): SwiGLU(\n",
       "            (w1): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (w2): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (act): SiLU()\n",
       "            (ffn_ln): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "            (w3): Linear(in_features=1152, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (13): Block(\n",
       "          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (q_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "            (k_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "            (v_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path): DropPath(p=0.08461538702249527)\n",
       "          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): SwiGLU(\n",
       "            (w1): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (w2): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (act): SiLU()\n",
       "            (ffn_ln): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "            (w3): Linear(in_features=1152, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (14): Block(\n",
       "          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (q_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "            (k_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "            (v_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path): DropPath(p=0.0923076942563057)\n",
       "          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): SwiGLU(\n",
       "            (w1): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (w2): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (act): SiLU()\n",
       "            (ffn_ln): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "            (w3): Linear(in_features=1152, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (15): Block(\n",
       "          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (q_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "            (k_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "            (v_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path): DropPath(p=0.10000000149011612)\n",
       "          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): SwiGLU(\n",
       "            (w1): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (w2): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (act): SiLU()\n",
       "            (ffn_ln): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "            (w3): Linear(in_features=1152, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (norm): Identity()\n",
       "      (fc_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (head): Identity()\n",
       "    )\n",
       "  )\n",
       "  (neck): Mamba_Neck(\n",
       "    (layers): ModuleList(\n",
       "      (0): ResidualBlock(\n",
       "        (mixer): MambaBlock(\n",
       "          (in_proj): Linear(in_features=384, out_features=1536, bias=False)\n",
       "          (conv1d): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(1,), groups=768)\n",
       "          (x_proj): Linear(in_features=768, out_features=56, bias=False)\n",
       "          (dt_proj): Linear(in_features=24, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=384, bias=False)\n",
       "        )\n",
       "        (norm): RMSNorm()\n",
       "      )\n",
       "      (1): ResidualBlock(\n",
       "        (mixer): MambaBlock(\n",
       "          (in_proj): Linear(in_features=384, out_features=1536, bias=False)\n",
       "          (conv1d): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(1,), groups=768)\n",
       "          (x_proj): Linear(in_features=768, out_features=56, bias=False)\n",
       "          (dt_proj): Linear(in_features=24, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=384, bias=False)\n",
       "        )\n",
       "        (norm): RMSNorm()\n",
       "      )\n",
       "      (2): ResidualBlock(\n",
       "        (mixer): MambaBlock(\n",
       "          (in_proj): Linear(in_features=384, out_features=1536, bias=False)\n",
       "          (conv1d): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(1,), groups=768)\n",
       "          (x_proj): Linear(in_features=768, out_features=56, bias=False)\n",
       "          (dt_proj): Linear(in_features=24, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=384, bias=False)\n",
       "        )\n",
       "        (norm): RMSNorm()\n",
       "      )\n",
       "      (3): ResidualBlock(\n",
       "        (mixer): MambaBlock(\n",
       "          (in_proj): Linear(in_features=384, out_features=1536, bias=False)\n",
       "          (conv1d): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(1,), groups=768)\n",
       "          (x_proj): Linear(in_features=768, out_features=56, bias=False)\n",
       "          (dt_proj): Linear(in_features=24, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=384, bias=False)\n",
       "        )\n",
       "        (norm): RMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (interactions): ModuleList(\n",
       "      (0): InteractionBlock(\n",
       "        (injector): Injector(\n",
       "          (query_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (feat_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (extractor): Extractor(\n",
       "          (query_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (feat_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
       "          )\n",
       "          (ffn): ConvFFN(\n",
       "            (fc1): Linear(in_features=384, out_features=96, bias=True)\n",
       "            (dwconv): DWConv(\n",
       "              (dwconv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)\n",
       "            )\n",
       "            (act): GELU(approximate=none)\n",
       "            (fc2): Linear(in_features=96, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ffn_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (drop_path): DropPath(drop_prob=0.100)\n",
       "        )\n",
       "      )\n",
       "      (1): InteractionBlock(\n",
       "        (injector): Injector(\n",
       "          (query_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (feat_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (extractor): Extractor(\n",
       "          (query_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (feat_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
       "          )\n",
       "          (ffn): ConvFFN(\n",
       "            (fc1): Linear(in_features=384, out_features=96, bias=True)\n",
       "            (dwconv): DWConv(\n",
       "              (dwconv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)\n",
       "            )\n",
       "            (act): GELU(approximate=none)\n",
       "            (fc2): Linear(in_features=96, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ffn_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (drop_path): DropPath(drop_prob=0.100)\n",
       "        )\n",
       "      )\n",
       "      (2): InteractionBlock(\n",
       "        (injector): Injector(\n",
       "          (query_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (feat_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (extractor): Extractor(\n",
       "          (query_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (feat_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
       "          )\n",
       "          (ffn): ConvFFN(\n",
       "            (fc1): Linear(in_features=384, out_features=96, bias=True)\n",
       "            (dwconv): DWConv(\n",
       "              (dwconv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)\n",
       "            )\n",
       "            (act): GELU(approximate=none)\n",
       "            (fc2): Linear(in_features=96, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ffn_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (drop_path): DropPath(drop_prob=0.100)\n",
       "        )\n",
       "      )\n",
       "      (3): InteractionBlock(\n",
       "        (injector): Injector(\n",
       "          (query_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (feat_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (extractor): Extractor(\n",
       "          (query_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (feat_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
       "          )\n",
       "          (ffn): ConvFFN(\n",
       "            (fc1): Linear(in_features=384, out_features=96, bias=True)\n",
       "            (dwconv): DWConv(\n",
       "              (dwconv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)\n",
       "            )\n",
       "            (act): GELU(approximate=none)\n",
       "            (fc2): Linear(in_features=96, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ffn_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (drop_path): DropPath(drop_prob=0.100)\n",
       "        )\n",
       "        (extra_extractors): Sequential(\n",
       "          (0): Extractor(\n",
       "            (query_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "            (feat_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
       "            )\n",
       "            (ffn): ConvFFN(\n",
       "              (fc1): Linear(in_features=384, out_features=96, bias=True)\n",
       "              (dwconv): DWConv(\n",
       "                (dwconv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)\n",
       "              )\n",
       "              (act): GELU(approximate=none)\n",
       "              (fc2): Linear(in_features=96, out_features=384, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ffn_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "            (drop_path): DropPath(drop_prob=0.100)\n",
       "          )\n",
       "          (1): Extractor(\n",
       "            (query_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "            (feat_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
       "            )\n",
       "            (ffn): ConvFFN(\n",
       "              (fc1): Linear(in_features=384, out_features=96, bias=True)\n",
       "              (dwconv): DWConv(\n",
       "                (dwconv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)\n",
       "              )\n",
       "              (act): GELU(approximate=none)\n",
       "              (fc2): Linear(in_features=96, out_features=384, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ffn_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "            (drop_path): DropPath(drop_prob=0.100)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): CenterPredictor(\n",
       "    (conv1_ctr): Sequential(\n",
       "      (0): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "    (conv2_ctr): Sequential(\n",
       "      (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "    (conv3_ctr): Sequential(\n",
       "      (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "    (conv4_ctr): Sequential(\n",
       "      (0): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "    (conv5_ctr): Conv2d(32, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (conv1_offset): Sequential(\n",
       "      (0): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "    (conv2_offset): Sequential(\n",
       "      (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "    (conv3_offset): Sequential(\n",
       "      (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "    (conv4_offset): Sequential(\n",
       "      (0): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "    (conv5_offset): Conv2d(32, 2, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (conv1_size): Sequential(\n",
       "      (0): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "    (conv2_size): Sequential(\n",
       "      (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "    (conv3_size): Sequential(\n",
       "      (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "    (conv4_size): Sequential(\n",
       "      (0): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "    (conv5_size): Conv2d(32, 2, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network = build_mcitrack(params.cfg)\n",
    "network.load_state_dict(torch.load(\"MCITRACK_ep0300.pth.tar\", map_location='cpu')['net'], strict=True)\n",
    "cfg = params.cfg\n",
    "network = network.cuda()\n",
    "network.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "64d5c940",
   "metadata": {},
   "outputs": [],
   "source": [
    "list1 = [torch.zeros(1, 3, 112, 112).to('cuda') for _ in range(5)]  # 5 тензоров размером [1, 3, 112, 112]\n",
    "list2 = [torch.zeros(1, 3, 224, 224).to('cuda')]                    # 1 тензор размером [1, 3, 224, 224]\n",
    "list3 = [torch.zeros(1, 4).to('cuda') for _ in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fc77a871",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = network.forward(list1,list2,list3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7855da66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred_boxes\n",
      "torch.Size([1, 1, 4])\n",
      "score_map\n",
      "torch.Size([1, 1, 14, 14])\n",
      "size_map\n",
      "torch.Size([1, 2, 14, 14])\n",
      "offset_map\n",
      "torch.Size([1, 2, 14, 14])\n"
     ]
    }
   ],
   "source": [
    "for i in  res:\n",
    "    print(i)\n",
    "    print(res[i].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa6b8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelWrapper(torch.nn.Module):\n",
    "    def __init__(self, original_model):\n",
    "        super(ModelWrapper, self).__init__()\n",
    "        self.original_model = original_model\n",
    "\n",
    "    def forward(self, template_list, search_list,template_anno_list):\n",
    "        \n",
    "        output_dict = self.original_model(template_list, search_list,template_anno_list)\n",
    "        \n",
    "        \n",
    "        return (output_dict['pred_boxes'],\n",
    "                output_dict['score_map'],\n",
    "                output_dict['size_map'],\n",
    "                output_dict['offset_map'])\n",
    "\n",
    "\n",
    "model = network\n",
    "model.eval()\n",
    "\n",
    "\n",
    "wrapped_model = ModelWrapper(model)\n",
    "\n",
    "\n",
    "template_list = [torch.zeros(1, 3, 112, 112).to('cuda') for _ in range(5)]  # 5 тензоров размером [1, 3, 112, 112]\n",
    "search_list = [torch.zeros(1, 3, 224, 224).to('cuda')]                    # 1 тензор размером [1, 3, 224, 224]\n",
    "template_anno_list = [torch.zeros(1, 4).to('cuda') for _ in range(5)]\n",
    "\n",
    "traced_model = torch.jit.trace(wrapped_model, (template_list, search_list,template_anno_list))\n",
    "\n",
    "\n",
    "optimized_model = torch.jit.optimize_for_inference(traced_model)\n",
    "\n",
    "\n",
    "optimized_model.save(\"MCITrack.pt\")\n",
    "\n",
    "\n",
    "loaded_model = torch.jit.load(\"MCITrack.pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = loaded_model(template_list, search_list,template_anno_list)\n",
    "\n",
    "for output in outputs:\n",
    "    print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99787837",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "class ModelWrapper(torch.nn.Module):\n",
    "    def __init__(self, original_model):\n",
    "        super(ModelWrapper, self).__init__()\n",
    "        self.original_model = original_model\n",
    "\n",
    "    def forward(self, template_list, search_list,template_anno_list):\n",
    "        \n",
    "        output_dict = self.original_model(template_list, search_list,template_anno_list)\n",
    "        \n",
    "        \n",
    "        return (output_dict['pred_boxes'],\n",
    "                output_dict['score_map'],\n",
    "                output_dict['size_map'],\n",
    "                output_dict['offset_map'])\n",
    "\n",
    "\n",
    "\n",
    "model = network\n",
    "model.eval()\n",
    "\n",
    "\n",
    "wrapped_model = ModelWrapper(model)\n",
    "\n",
    "template_list = [torch.zeros(1, 3, 112, 112).to('cuda') for _ in range(5)]  # 5 тензоров размером [1, 3, 112, 112]\n",
    "search_list = [torch.zeros(1, 3, 224, 224).to('cuda')]                    # 1 тензор размером [1, 3, 224, 224]\n",
    "template_anno_list = [torch.zeros(1, 4).to('cuda') for _ in range(5)]\n",
    "\n",
    "# Важно: для onnx-модели модель должна быть на cpu или cuda, и входы должны быть на том же устройстве.\n",
    "wrapped_model = wrapped_model.to('cuda')\n",
    "wrapped_model.eval()\n",
    "\n",
    "# Указываем пути для сохранения\n",
    "onnx_path = \"MCITrac.onnx\"\n",
    "\n",
    "# Экспортируем модель в ONNX\n",
    "torch.onnx.export(\n",
    "    wrapped_model,                                   # Модель\n",
    "    (template_list, search_list,template_anno_list),                                # Входные данные (tuple)\n",
    "    onnx_path,                                       # Имя файла\n",
    "    export_params=True,                              # Экспортировать параметры (веса)\n",
    "    opset_version=16,                                # Версия ONNX opset\n",
    "    do_constant_folding=True,                        # Оптимизация констант\n",
    "    input_names = ['template_list', 'search_list','template_anno_list'],                        # Имена входов\n",
    "    output_names = ['pred_boxes','score_map','size_map','offset_map'],                   # Имена выходов\n",
    "    #dynamic_axes={'z': {0: 'batch_size'},            # Динамическая ось для батча\n",
    "    #              'x': {0: 'batch_size'},\n",
    "    #              'pred_boxes': {0: 'batch_size'}},\n",
    "    verbose=True                                     # Показывать подробности\n",
    ")\n",
    "\n",
    "print(f'Model has been exported to {onnx_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1666b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trtexec --onnx=MCITrac.onnx  --saveEngine=MCITrac.trt  --fp16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196c2084",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Throughput: 61.4365 qps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae7ecd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can't read frame\n"
     ]
    }
   ],
   "source": [
    "file = \"12-машина_поле_ТВ.mp4\"\n",
    "video = cv2.VideoCapture(file)\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "fps=video.get(cv2.CAP_PROP_FPS)\n",
    "video_vriter = cv2.VideoWriter(file.split('.')[0]+\"_\"+\".avi\", fourcc, fps, (1920, 1080))\n",
    "\n",
    "\n",
    "ok, image = video.read()\n",
    "if not video.isOpened():\n",
    "    print(\"Could not open video\")\n",
    "    sys.exit()\n",
    "    \n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "x, y, w, h = cv2.selectROI( image, fromCenter=False)\n",
    "init_state = [x, y, w, h]\n",
    "def _build_init_info(box):\n",
    "            return {'init_bbox': box}\n",
    "treacker.initialize(image, _build_init_info(init_state))\n",
    "counter = 0\n",
    "while True:\n",
    "            ok, image = video.read()\n",
    "            if not ok:\n",
    "                print(\"Can't read frame\")\n",
    "                break\n",
    "\n",
    "            \n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            start = time.time() \n",
    "            out  = treacker.track(image)\n",
    "            state = [int(s) for s in out['target_bbox']]\n",
    "            best_score=out[\"best_score\"].cpu().numpy()[0][0]\n",
    "            end_time = (time.time() - start)\n",
    "            \n",
    "            \n",
    "            org = (50, 50)\n",
    "\n",
    "            # fontScale\n",
    "            fontScale = 1\n",
    "            font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "            # Blue color in BGR\n",
    "            color = (255, 0, 0)\n",
    "            # Line thickness of 2 px\n",
    "            thickness = 2              \n",
    "            # Using cv2.putText() method\n",
    "            image = cv2.putText(image, str(best_score), org, font, \n",
    "                            fontScale, color, thickness, cv2.LINE_AA)\n",
    "            image = cv2.putText(image, str(end_time), (50,100), font, \n",
    "                            fontScale, color, thickness, cv2.LINE_AA)\n",
    "\n",
    "            x, y, w, h = [int(x) for x in state]\n",
    "\n",
    "            color = (200, 0, 0)  # Цвет в формате BGR\n",
    "            cv2.rectangle(image, (x, y), (x + w, y + h), color, 2)\n",
    "\n",
    "\n",
    "            cv2.imshow(\"tracking\", image)\n",
    "            video_vriter.write(image)\n",
    "\n",
    "\n",
    "            k = cv2.waitKey(1)            \n",
    "            if k == 32:  # SPACE\n",
    "                ok, image = video.read()                             \n",
    "                x, y, w, h = cv2.selectROI( image, fromCenter=False)\n",
    "                init_state = [x, y, w, h]\n",
    "                treacker.initialize(image, _build_init_info(init_state))\n",
    "            if k == 27:  # ESC\n",
    "                break\n",
    "        \n",
    "                \n",
    "                \n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "video.release()\n",
    "video_vriter.release()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e8df45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1d032c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Метрики\n",
    "import numpy as np\n",
    "\n",
    "def iou(boxA, boxB):\n",
    "    # boxA, boxB: [x, y, w, h]\n",
    "    xA = max(boxA[0], boxB[0])\n",
    "    yA = max(boxA[1], boxB[1])\n",
    "    xB = min(boxA[0] + boxA[2], boxB[0] + boxB[2])\n",
    "    yB = min(boxA[1] + boxA[3], boxB[1] + boxB[3])\n",
    "\n",
    "    interW = max(0, xB - xA)\n",
    "    interH = max(0, yB - yA)\n",
    "    interArea = interW * interH\n",
    "\n",
    "    boxAArea = boxA[2] * boxA[3]\n",
    "    boxBArea = boxB[2] * boxB[3]\n",
    "    unionArea = boxAArea + boxBArea - interArea\n",
    "\n",
    "    if unionArea == 0:\n",
    "        return 0.0\n",
    "    return interArea / unionArea\n",
    "\n",
    "def precision(boxA, boxB):\n",
    "    # центры bbox\n",
    "    centerA = (boxA[0] + boxA[2]/2, boxA[1] + boxA[3]/2)\n",
    "    centerB = (boxB[0] + boxB[2]/2, boxB[1] + boxB[3]/2)\n",
    "    dist = np.sqrt((centerA[0] - centerB[0])**2 + (centerA[1] - centerB[1])**2)\n",
    "    return dist\n",
    "\"\"\"\n",
    "# Пример для одного кадра\n",
    "gt_bbox = [50, 50, 100, 100]      # ground truth bbox\n",
    "pred_bbox = [60, 60, 80, 80]      # predicted bbox\n",
    "\n",
    "iou_val = iou(gt_bbox, pred_bbox)\n",
    "print(f'IoU: {iou_val:.2f}')\n",
    "\n",
    "# Success Rate при пороге 0.5\n",
    "sr_thresh = 0.5\n",
    "sr = 1 if iou_val >= sr_thresh else 0\n",
    "print(f'Success Rate @0.5: {sr}')\n",
    "\n",
    "# Precision при пороге 20 пикселей\n",
    "prec_thresh = 20\n",
    "prec_dist = precision(gt_bbox, pred_bbox)\n",
    "prec = 1 if prec_dist <= prec_thresh else 0\n",
    "print(f'Precision @20px: {prec}')\n",
    "\n",
    "# --- Для серии кадров (пример) ---\n",
    "\n",
    "gt_bboxes = [\n",
    "    [50, 50, 100, 100],\n",
    "    [60, 60, 100, 100]\n",
    "]\n",
    "pred_bboxes = [\n",
    "    [60, 60, 80, 80],\n",
    "    [65, 65, 90, 90]\n",
    "]\n",
    "\n",
    "ious = [iou(gt, pred) for gt, pred in zip(gt_bboxes, pred_bboxes)]\n",
    "ao = np.mean(ious)\n",
    "sr = np.mean([1 if val >= sr_thresh else 0 for val in ious])\n",
    "precisions = [precision(gt, pred) for gt, pred in zip(gt_bboxes, pred_bboxes)]\n",
    "prec = np.mean([1 if d <= prec_thresh else 0 for d in precisions])\n",
    "\n",
    "print(f'Average Overlap (AO): {ao:.2f}')\n",
    "print(f'Success Rate (SR@0.5): {sr:.2f}')\n",
    "print(f'Precision @20px: {prec:.2f}')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fab2287",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b56fbba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5201b2e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0c4216",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b0fbe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video saved as: GOT-10k_Val_000001_output.avi\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import  os\n",
    "gt_bboxes = []\n",
    "pred_bboxes = []\n",
    "seq_path = \"val/GOT-10k_Val_000001\"\n",
    "txt_files = glob.glob(os.path.join(seq_path, '*.txt'))\n",
    "if not txt_files:\n",
    "    raise FileNotFoundError(f\"No .txt files found in {seq_path}\")\n",
    "\n",
    "img_files = sorted(glob.glob(os.path.join(seq_path, '*.jpg')))\n",
    "with open(txt_files[0], 'r') as f:\n",
    "    gt_bboxes = [list(map(float, line.strip().split(','))) for line in f]\n",
    "\n",
    "# Получаем размер первого изображения\n",
    "sample_img = cv2.imread(img_files[0])\n",
    "if sample_img is None:\n",
    "    raise ValueError(f\"Failed to read sample image: {img_files[0]}\")\n",
    "height, width = sample_img.shape[:2]\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "output_filename = f\"{seq_path.split('/')[-1]}_output.avi\"\n",
    "video_vriter = cv2.VideoWriter(output_filename, fourcc, 10, (width, height))  \n",
    "\n",
    "assert len(img_files) == len(gt_bboxes), \"Количество кадров и bbox'ов не совпадает\"\n",
    "\n",
    "x, y, w, h = map(int, gt_bboxes[0])\n",
    "init_state = [x, y, w, h]\n",
    "\n",
    "def _build_init_info(box):\n",
    "            return {'init_bbox': box}\n",
    "\n",
    "counter = 0\n",
    "\n",
    "for img_file, bbox in zip(img_files, gt_bboxes):\n",
    "        \n",
    "        # Читаем изображение\n",
    "        img = cv2.imread(img_file)\n",
    "        if img is None:\n",
    "            print(f\"Не удалось загрузить изображение: {img_file}\")\n",
    "            continue\n",
    "        \n",
    "        treacker.initialize(img, _build_init_info(init_state))\n",
    "        out  = treacker.track(img)\n",
    "        state = [int(s) for s in out['target_bbox']]   \n",
    "        org = (50, 50)\n",
    "\n",
    "        # fontScale\n",
    "        fontScale = 1\n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "        #color = (0, 0, 255)\n",
    "        # Line thickness of 2 px\n",
    "        thickness = 2              \n",
    "                \n",
    "        x, y, w, h = [int(x) for x in state]\n",
    "\n",
    "        color = (0, 0, 200)  # Цвет в формате BGR\n",
    "        cv2.rectangle(img, (x, y), (x + w, y + h), color, 2)\n",
    "        # Рисуем bounding box\n",
    "        x1, y1, w1, h1 = map(int, bbox)\n",
    "        cv2.rectangle(img, (x1, y1), (x1+w1, y1+h1), (0, 255, 0), 2)\n",
    "        bbox_pred = x, y, w, h\n",
    "        \n",
    "        gt_bboxes.append(bbox)\n",
    "        pred_bboxes.append(bbox_pred)\n",
    "\n",
    "        cv2.imshow(seq_path, img)\n",
    "        video_vriter.write(img)\n",
    "\n",
    "\n",
    "        # Выход по нажатию 'q' или ESC\n",
    "        key = cv2.waitKey(100) & 0xFF\n",
    "        if key == ord('q') or key == 27:\n",
    "            break\n",
    "        \n",
    "                \n",
    "                \n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "video.release()\n",
    "video_vriter.release()\n",
    "print(f\"Video saved as: {output_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0f852e52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Overlap (AO): 0.46\n",
      "Success Rate (SR@0.5): 0.33\n",
      "Precision @20px: 0.05\n"
     ]
    }
   ],
   "source": [
    "ious = [iou(gt, pred) for gt, pred in zip(gt_bboxes, pred_bboxes)]\n",
    "ao = np.mean(ious)\n",
    "sr = np.mean([1 if val >= sr_thresh else 0 for val in ious])\n",
    "precisions = [precision(gt, pred) for gt, pred in zip(gt_bboxes, pred_bboxes)]\n",
    "prec = np.mean([1 if d <= prec_thresh else 0 for d in precisions])\n",
    "\n",
    "print(f'Average Overlap (AO): {ao:.2f}')\n",
    "print(f'Success Rate (SR@0.5): {sr:.2f}')\n",
    "print(f'Precision @20px: {prec:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1455645",
   "metadata": {},
   "outputs": [],
   "source": [
    "GOT-10k_Val_000001\n",
    "Average Overlap (AO): 0.46\n",
    "Success Rate (SR@0.5): 0.33\n",
    "Precision @20px: 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e49827",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install onnxruntime opencv-python numpy\n",
    "#pip install onnxruntime-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7941bdb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import onnxruntime as ort\n",
    "\n",
    "# Инициализация ONNX Runtime\n",
    "onnx_model_path = \"mcitrack.onnx\"  # путь к вашей модели\n",
    "session = ort.InferenceSession(onnx_model_path, providers=['CUDAExecutionProvider', 'CPUExecutionProvider'])\n",
    "\n",
    "# Получение входных и выходных имен модели\n",
    "input_name = session.get_inputs()[0].name\n",
    "output_name = session.get_outputs()[0].name\n",
    "\n",
    "# Функция для предобработки изображения (зависит от требований модели)\n",
    "def preprocess_image(image):\n",
    "    # Пример: изменение размера, нормализация и т. д.\n",
    "    image = cv2.resize(image, (256, 256))  # предположим, что модель ожидает 256x256\n",
    "    image = image.astype(np.float32) / 255.0\n",
    "    image = np.transpose(image, (2, 0, 1))  # HWC -> CHW\n",
    "    image = np.expand_dims(image, axis=0)    # добавляем batch-размер\n",
    "    return image\n",
    "\n",
    "# Пример работы трекера на кадре\n",
    "def track(frame):\n",
    "    # Предобработка кадра\n",
    "    input_tensor = preprocess_image(frame)\n",
    "    \n",
    "    # Запуск модели\n",
    "    outputs = session.run([output_name], {input_name: input_tensor})\n",
    "    \n",
    "    # Обработка выходных данных (зависит от формата вывода модели)\n",
    "    # Например, если модель возвращает bounding box [x, y, w, h]\n",
    "    bbox = outputs[0][0]\n",
    "    return bbox\n",
    "\n",
    "\n",
    "import glob\n",
    "import  os\n",
    "gt_bboxes = []\n",
    "pred_bboxes = []\n",
    "seq_path = \"val/GOT-10k_Val_000001\"\n",
    "txt_files = glob.glob(os.path.join(seq_path, '*.txt'))\n",
    "if not txt_files:\n",
    "    raise FileNotFoundError(f\"No .txt files found in {seq_path}\")\n",
    "\n",
    "img_files = sorted(glob.glob(os.path.join(seq_path, '*.jpg')))\n",
    "with open(txt_files[0], 'r') as f:\n",
    "    gt_bboxes = [list(map(float, line.strip().split(','))) for line in f]\n",
    "\n",
    "# Получаем размер первого изображения\n",
    "sample_img = cv2.imread(img_files[0])\n",
    "if sample_img is None:\n",
    "    raise ValueError(f\"Failed to read sample image: {img_files[0]}\")\n",
    "#height, width = sample_img.shape[:2]\n",
    "\n",
    "#fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "#output_filename = f\"{seq_path.split('/')[-1]}_output.avi\"\n",
    "#video_vriter = cv2.VideoWriter(output_filename, fourcc, 10, (width, height))  \n",
    "\n",
    "assert len(img_files) == len(gt_bboxes), \"Количество кадров и bbox'ов не совпадает\"\n",
    "\n",
    "#x, y, w, h = map(int, gt_bboxes[0])\n",
    "#init_state = [x, y, w, h]\n",
    "\n",
    "counter = 0\n",
    "\n",
    "for img_file, bbox in zip(img_files, gt_bboxes):\n",
    "        \n",
    "        # Читаем изображение\n",
    "        img = cv2.imread(img_file)\n",
    "        if img is None:\n",
    "            print(f\"Не удалось загрузить изображение: {img_file}\")\n",
    "            continue\n",
    "        \n",
    "        # Получаем предсказанный bbox\n",
    "        pred_bbox = track(img)\n",
    "        pred_bboxes.append(pred_bbox)\n",
    "        \n",
    "        x, y, w, h = map(int, pred_bbox)\n",
    "\n",
    "        color = (0, 0, 200)  # Цвет в формате BGR\n",
    "        cv2.rectangle(img, (x, y), (x + w, y + h), color, 2)\n",
    "        \n",
    "        # Рисуем bounding box\n",
    "        x1, y1, w1, h1 = map(int, bbox)\n",
    "        cv2.rectangle(img, (x1, y1), (x1+w1, y1+h1), (0, 255, 0), 2)\n",
    "       \n",
    "\n",
    "        cv2.imshow(seq_path, img)\n",
    "        \n",
    "        # video_writer.write(img)  \n",
    "\n",
    "\n",
    "        # Выход по нажатию 'q' или ESC\n",
    "        key = cv2.waitKey(100) & 0xFF\n",
    "        if key == ord('q') or key == 27:\n",
    "            break\n",
    "        \n",
    "                \n",
    "                \n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "# video_writer.release() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e91d5a07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input 0: Name=template_list, Shape=[1, 3, 112, 112], Type=tensor(float)\n",
      "Input 1: Name=search_list, Shape=[1, 3, 112, 112], Type=tensor(float)\n",
      "Input 2: Name=template_anno_list, Shape=[1, 3, 112, 112], Type=tensor(float)\n",
      "Input 3: Name=onnx::Unsqueeze_3, Shape=[1, 3, 112, 112], Type=tensor(float)\n",
      "Input 4: Name=onnx::Unsqueeze_4, Shape=[1, 3, 112, 112], Type=tensor(float)\n",
      "Input 5: Name=onnx::Unsqueeze_5, Shape=[1, 3, 224, 224], Type=tensor(float)\n",
      "Input 6: Name=onnx::Unsqueeze_6, Shape=[1, 4], Type=tensor(float)\n",
      "Input 7: Name=onnx::Unsqueeze_7, Shape=[1, 4], Type=tensor(float)\n",
      "Input 8: Name=onnx::Unsqueeze_8, Shape=[1, 4], Type=tensor(float)\n",
      "Input 9: Name=onnx::Unsqueeze_9, Shape=[1, 4], Type=tensor(float)\n",
      "Input 10: Name=onnx::Unsqueeze_10, Shape=[1, 4], Type=tensor(float)\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime as ort\n",
    "\n",
    "session = ort.InferenceSession(\"mcitrack.onnx\")\n",
    "for i, input in enumerate(session.get_inputs()):\n",
    "    print(f\"Input {i}: Name={input.name}, Shape={input.shape}, Type={input.type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cffe95f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime as ort\n",
    "print(ort.get_device())  # Должно вывести \"CPU\" или \"GPU\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e45f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pycuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eb00ee5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: onnxruntime 1.22.0\n",
      "Uninstalling onnxruntime-1.22.0:\n",
      "  Successfully uninstalled onnxruntime-1.22.0\n",
      "Found existing installation: onnxruntime-gpu 1.22.0\n",
      "Uninstalling onnxruntime-gpu-1.22.0:\n",
      "  Successfully uninstalled onnxruntime-gpu-1.22.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip uninstall onnxruntime onnxruntime-gpu -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d57702e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install onnxruntime==1.12.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b299076",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorrt as trt\n",
    "import pycuda.autoinit\n",
    "TRT_LOGGER = trt.Logger(trt.Logger.WARNING)\n",
    "\n",
    "def load_engine(engine_path):\n",
    "        with open(engine_path, \"rb\") as engine_file, trt.Runtime(TRT_LOGGER) as runtime:\n",
    "            engine = runtime.deserialize_cuda_engine(engine_file.read())\n",
    "        return engine\n",
    "bacbone = load_engine(\"resnet18_explicit.trt\")\n",
    "import numpy as np\n",
    "import pycuda.driver as cuda\n",
    "\n",
    "def initialize_trt_bb(engine):\n",
    "    context = engine.create_execution_context()\n",
    "\n",
    "    # Предположим, что размеры входных данных известны заранее\n",
    "    input_size_x = np.prod([1, 3, 224, 224]) * np.dtype(np.float32).itemsize\n",
    "    d_input_x = cuda.mem_alloc(int(input_size_x))\n",
    "\n",
    "    output_size_cls = np.prod([1, 1000]) * np.dtype(np.float32).itemsize\n",
    "    d_output_cls = cuda.mem_alloc(int(output_size_cls))\n",
    "\n",
    "    return context, d_input_x, d_output_cls\n",
    "\n",
    "# Предположим, что backbone уже определён и engine создан\n",
    "context_backbone, d_input_x, d_output_cls  = initialize_trt_bb(bacbone)\n",
    "\n",
    "def predictV2_bb(context, input_data_x, d_input_x, d_output_cls):\n",
    "    # Prepare input\n",
    "    input_shape_x = input_data_x.shape\n",
    "    input_size_x = np.prod(input_shape_x) * input_data_x.itemsize\n",
    "    cuda.memcpy_htod(d_input_x, input_data_x.ravel())\n",
    "\n",
    "    # Execute model\n",
    "    context.execute_v2(bindings=[\n",
    "        int(d_input_x),\n",
    "        int(d_output_cls)\n",
    "    ])\n",
    "\n",
    "    # Fetch output data\n",
    "    output_data_cls = np.empty([1, 1000], dtype=np.float32)\n",
    "    cuda.memcpy_dtoh(output_data_cls, d_output_cls)\n",
    "\n",
    "    return output_data_cls\n",
    "\n",
    "# Пример использования:\n",
    "input_data1 = np.random.rand(1, 3, 224, 224)\n",
    "output = predictV2_bb(context_backbone, input_data1.astype(np.float32), d_input_x, d_output_cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5e01ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4179a41b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\piuta_en\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import onnxruntime as ort\n",
    "import torch\n",
    "from torch.nn.functional import interpolate\n",
    "\n",
    "class MCITRACK_ONNX:\n",
    "    def __init__(self, params, onnx_path=\"MCITRACK.onnx\"):\n",
    "        self.params = params\n",
    "        self.session = ort.InferenceSession(onnx_path, providers=['CUDAExecutionProvider', 'CPUExecutionProvider'])\n",
    "        \n",
    "        # Конфигурация из оригинального трекера\n",
    "        self.fx_sz = params.cfg[\"TEST\"][\"SEARCH_SIZE\"] // params.cfg[\"MODEL\"][\"ENCODER\"][\"STRIDE\"]\n",
    "        self.num_template = params.cfg[\"TEST\"][\"NUM_TEMPLATES\"]\n",
    "        self.memory_bank = params.cfg[\"TEST\"][\"MB\"][\"DEFAULT\"]\n",
    "        self.update_threshold = params.cfg[\"TEST\"][\"UPT\"][\"DEFAULT\"]\n",
    "        self.update_intervals = params.cfg[\"TEST\"][\"INTER\"][\"DEFAULT\"]\n",
    "        \n",
    "        self.state = None\n",
    "        self.frame_id = 0\n",
    "        self.template_list = []\n",
    "        self.template_anno_list = []\n",
    "        self.memory_template_list = []\n",
    "        self.memory_template_anno_list = []\n",
    "\n",
    "    def _preprocess(self, img, bbox, target_size):\n",
    "        \"\"\"Аналог sample_target и preprocessor.process из оригинального кода\"\"\"\n",
    "        # Упрощенная версия предобработки\n",
    "        x, y, w, h = bbox\n",
    "        cx, cy = x + w/2, y + h/2\n",
    "        \n",
    "        # Вырезаем и ресайзим патч\n",
    "        patch = cv2.getRectSubPix(img, (int(w*1.5), int(h*1.5)), (cx, cy))\n",
    "        patch = cv2.resize(patch, target_size)\n",
    "        \n",
    "        # Нормализация\n",
    "        patch = patch.astype(np.float32) / 255.0\n",
    "        patch = np.transpose(patch, (2, 0, 1))  # HWC -> CHW\n",
    "        return np.expand_dims(patch, axis=0)    # Добавляем batch-размер\n",
    "\n",
    "    def initialize(self, image, info: dict):\n",
    "        \"\"\"Инициализация трекера по первому кадру\"\"\"\n",
    "        self.state = info['init_bbox']\n",
    "        self.frame_id = 0\n",
    "        \n",
    "        # Подготовка шаблонов\n",
    "        template = self._preprocess(image, self.state, (112, 112))\n",
    "        self.template_list = [template] * self.num_template\n",
    "        \n",
    "        # Подготовка аннотаций (упрощенная версия)\n",
    "        prev_box_crop = np.array([0.5, 0.5, 1.0, 1.0], dtype=np.float32)  # Центрированная аннотация\n",
    "        self.template_anno_list = [prev_box_crop] * self.num_template\n",
    "        \n",
    "        # Инициализация памяти\n",
    "        self.memory_template_list = self.template_list.copy()\n",
    "        self.memory_template_anno_list = self.template_anno_list.copy()\n",
    "\n",
    "    def track(self, image, info: dict = None):\n",
    "        \"\"\"Основная функция трекинга\"\"\"\n",
    "        H, W, _ = image.shape\n",
    "        self.frame_id += 1\n",
    "        \n",
    "        # Подготовка search-изображения\n",
    "        search = self._preprocess(image, self.state, (112, 112))\n",
    "        \n",
    "        # Подготовка входных данных для ONNX модели\n",
    "        input_feed = {\n",
    "            'template_list': np.stack(self.template_list).squeeze(1),\n",
    "            'search_list': search,\n",
    "            'template_anno_list': np.stack(self.template_anno_list),\n",
    "            'onnx::Unsqueeze_3': np.zeros((self.num_template, 3, 112, 112), dtype=np.float32),\n",
    "            'onnx::Unsqueeze_4': np.zeros((self.num_template, 3, 112, 112), dtype=np.float32),\n",
    "            'onnx::Unsqueeze_5': np.zeros((1, 3, 224, 224), dtype=np.float32),\n",
    "            'onnx::Unsqueeze_6': np.stack(self.template_anno_list),\n",
    "            'onnx::Unsqueeze_7': np.zeros((self.num_template, 4), dtype=np.float32),\n",
    "            'onnx::Unsqueeze_8': np.zeros((self.num_template, 4), dtype=np.float32),\n",
    "            'onnx::Unsqueeze_9': np.zeros((self.num_template, 4), dtype=np.float32),\n",
    "            'onnx::Unsqueeze_10': np.zeros((self.num_template, 4), dtype=np.float32)\n",
    "        }\n",
    "        \n",
    "        # Запуск модели\n",
    "        outputs = self.session.run(None, input_feed)\n",
    "        \n",
    "        # Получение и обработка результатов\n",
    "        pred_bbox = outputs[0][0]  # [x,y,w,h]\n",
    "        \n",
    "        # Обновление состояния\n",
    "        self.state = pred_bbox\n",
    "        \n",
    "        # Логика обновления шаблонов (упрощенная версия)\n",
    "        if self.frame_id % self.update_intervals == 0:\n",
    "            new_template = self._preprocess(image, self.state, (112, 112))\n",
    "            self.template_list.pop(0)\n",
    "            self.template_list.append(new_template)\n",
    "            \n",
    "            new_anno = np.array([0.5, 0.5, 1.0, 1.0], dtype=np.float32)\n",
    "            self.template_anno_list.pop(0)\n",
    "            self.template_anno_list.append(new_anno)\n",
    "        \n",
    "        return {'target_bbox': pred_bbox}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8457be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import onnxruntime as ort\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Инициализация ONNX Runtime\n",
    "onnx_model_path = \"mcitrack.onnx\"\n",
    "session = ort.InferenceSession(onnx_model_path, providers=['CUDAExecutionProvider', 'CPUExecutionProvider'])\n",
    "\n",
    "def preprocess_image(image, target_size=(112, 112)):\n",
    "    \"\"\"Подготовка изображения для модели\"\"\"\n",
    "    image = cv2.resize(image, target_size)\n",
    "    image = image.astype(np.float32) / 255.0\n",
    "    image = np.transpose(image, (2, 0, 1))  # HWC -> CHW\n",
    "    image = np.expand_dims(image, axis=0)    # добавляем batch-размер\n",
    "    return image\n",
    "\n",
    "def track(frame, init_state=None):\n",
    "    \"\"\"Выполнение трекинга на кадре\"\"\"\n",
    "    # Подготовка входных данных\n",
    "    input_feed = {\n",
    "        'template_list': preprocess_image(frame, (112, 112)) if init_state else np.zeros((1, 3, 112, 112), dtype=np.float32),\n",
    "        'search_list': preprocess_image(frame, (112, 112)),\n",
    "        'template_anno_list': np.zeros((1, 3, 112, 112), dtype=np.float32),\n",
    "        'onnx::Unsqueeze_3': np.zeros((1, 3, 112, 112), dtype=np.float32),\n",
    "        'onnx::Unsqueeze_4': np.zeros((1, 3, 112, 112), dtype=np.float32),\n",
    "        'onnx::Unsqueeze_5': np.zeros((1, 3, 224, 224), dtype=np.float32),\n",
    "        'onnx::Unsqueeze_6': np.array([init_state], dtype=np.float32) if init_state else np.zeros((1, 4), dtype=np.float32),\n",
    "        'onnx::Unsqueeze_7': np.zeros((1, 4), dtype=np.float32),\n",
    "        'onnx::Unsqueeze_8': np.zeros((1, 4), dtype=np.float32),\n",
    "        'onnx::Unsqueeze_9': np.zeros((1, 4), dtype=np.float32),\n",
    "        'onnx::Unsqueeze_10': np.zeros((1, 4), dtype=np.float32)\n",
    "    }\n",
    "    \n",
    "    # Запуск модели\n",
    "    outputs = session.run(None, input_feed)\n",
    "    \n",
    "    # Получаем bounding box [x,y,w,h]\n",
    "    bbox = outputs[0][0].astype(int)\n",
    "    return {'target_bbox': bbox}\n",
    "\n",
    "# Загрузка последовательности\n",
    "gt_bboxes = []\n",
    "pred_bboxes = []\n",
    "seq_path = \"val/GOT-10k_Val_000001\"\n",
    "txt_files = glob.glob(os.path.join(seq_path, '*.txt'))\n",
    "if not txt_files:\n",
    "    raise FileNotFoundError(f\"No .txt files found in {seq_path}\")\n",
    "\n",
    "img_files = sorted(glob.glob(os.path.join(seq_path, '*.jpg')))\n",
    "with open(txt_files[0], 'r') as f:\n",
    "    gt_bboxes = [list(map(float, line.strip().split(','))) for line in f]\n",
    "\n",
    "# Получаем размер первого изображения\n",
    "sample_img = cv2.imread(img_files[0])\n",
    "if sample_img is None:\n",
    "    raise ValueError(f\"Failed to read sample image: {img_files[0]}\")\n",
    "height, width = sample_img.shape[:2]\n",
    "\n",
    "# Инициализация видео\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "output_filename = f\"{seq_path.split('/')[-1]}_output.avi\"\n",
    "video_writer = cv2.VideoWriter(output_filename, fourcc, 10, (width, height))  \n",
    "\n",
    "assert len(img_files) == len(gt_bboxes), \"Количество кадров и bbox'ов не совпадает\"\n",
    "\n",
    "# Инициализация трекера по первому кадру\n",
    "x, y, w, h = map(int, gt_bboxes[0])\n",
    "init_state = [x, y, w, h]\n",
    "init_img = cv2.imread(img_files[0])\n",
    "if init_img is None:\n",
    "    raise ValueError(f\"Failed to read initial image: {img_files[0]}\")\n",
    "\n",
    "# Первый вызов для инициализации\n",
    "track(init_img, init_state)\n",
    "\n",
    "counter = 0\n",
    "\n",
    "for img_file, bbox in zip(img_files, gt_bboxes):\n",
    "    # Читаем изображение\n",
    "    img = cv2.imread(img_file)\n",
    "    if img is None:\n",
    "        print(f\"Не удалось загрузить изображение: {img_file}\")\n",
    "        continue\n",
    "    \n",
    "    # Выполняем трекинг\n",
    "    out = track(img)\n",
    "    state = out['target_bbox']\n",
    "    \n",
    "    # Отрисовка\n",
    "    x, y, w, h = state\n",
    "    cv2.rectangle(img, (x, y), (x + w, y + h), (0, 0, 200), 2)  # Pred - красный\n",
    "    \n",
    "    # Отрисовка GT\n",
    "    x1, y1, w1, h1 = map(int, bbox)\n",
    "    cv2.rectangle(img, (x1, y1), (x1 + w1, y1 + h1), (0, 255, 0), 2)  # GT - зеленый\n",
    "    \n",
    "    # Сохраняем результаты\n",
    "    pred_bboxes.append([x, y, w, h])\n",
    "    gt_bboxes.append(bbox)\n",
    "    \n",
    "    # Вывод информации на кадр\n",
    "    cv2.putText(img, f\"Frame: {counter}\", (50, 50), \n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "    counter += 1\n",
    "    \n",
    "    # Запись в видео и отображение\n",
    "    video_writer.write(img)\n",
    "    cv2.imshow(seq_path, img)\n",
    "    \n",
    "    # Выход по нажатию 'q' или ESC\n",
    "    key = cv2.waitKey(100) & 0xFF\n",
    "    if key in (ord('q'), 27):\n",
    "        break\n",
    "\n",
    "# Завершение работы\n",
    "cv2.destroyAllWindows()\n",
    "video_writer.release()\n",
    "print(f\"Video saved as: {output_filename}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
