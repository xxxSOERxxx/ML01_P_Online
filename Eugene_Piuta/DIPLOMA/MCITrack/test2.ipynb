{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7bb640b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "429c4397",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\python3_10\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import yaml\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils import checkpoint\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import cv2\n",
    "import sys\n",
    "import numpy as np\n",
    "import fastitpn as fastitpn_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4bde567b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBase(nn.Module):\n",
    "\n",
    "    def __init__(self, encoder: nn.Module, train_encoder: bool, open_layers: list, num_channels: int):\n",
    "        super().__init__()\n",
    "        open_blocks = open_layers[2:]\n",
    "        open_items = open_layers[0:2]\n",
    "        for name, parameter in encoder.named_parameters():\n",
    "\n",
    "            if not train_encoder:\n",
    "                freeze = True\n",
    "                for open_block in open_blocks:\n",
    "                    if open_block in name:\n",
    "                        freeze = False\n",
    "                if name in open_items:\n",
    "                    freeze = False\n",
    "                if freeze == True:\n",
    "                    parameter.requires_grad_(False)  # here should allow users to specify which layers to freeze !\n",
    "\n",
    "        self.body = encoder\n",
    "        self.num_channels = num_channels\n",
    "\n",
    "    def forward(self, template_list, search_list, template_anno_list):\n",
    "        xs = self.body(template_list, search_list, template_anno_list)\n",
    "        return xs\n",
    "\n",
    "\n",
    "#fast_itpn_tiny_1600e_1k\n",
    "\n",
    "class Encoder(EncoderBase):\n",
    "    \"\"\"FastITPN encoder.\"\"\"\n",
    "    def __init__(self, name: str,\n",
    "                 train_encoder: bool,\n",
    "                 pretrain_type: str,\n",
    "                 search_size: int,\n",
    "                 search_number: int,\n",
    "                 template_size: int,\n",
    "                 template_number: int,\n",
    "                 open_layers: list,\n",
    "                 cfg=None):\n",
    "        if \"fastitpn\" in name.lower():\n",
    "            encoder = getattr(fastitpn_module, name)(\n",
    "                pretrained=True,\n",
    "                search_size=search_size,\n",
    "                template_size=template_size,\n",
    "                drop_rate=0.0,\n",
    "                drop_path_rate=0.1,\n",
    "                attn_drop_rate=0.0,\n",
    "                init_values=0.1,\n",
    "                drop_block_rate=None,\n",
    "                use_mean_pooling=True,\n",
    "                grad_ckpt=cfg[\"MODEL\"][\"ENCODER\"][\"GRAD_CKPT\"],\n",
    "                pos_type=cfg[\"MODEL\"][\"ENCODER\"][\"POS_TYPE\"],\n",
    "                token_type_indicate=cfg[\"MODEL\"][\"ENCODER\"][\"TOKEN_TYPE_INDICATE\"],\n",
    "                pretrain_type = cfg[\"MODEL\"][\"ENCODER\"][\"PRETRAIN_TYPE\"],\n",
    "            )\n",
    "            if \"itpnb\" in name:\n",
    "                num_channels = 512\n",
    "            elif \"itpnl\" in name:\n",
    "                num_channels = 768\n",
    "            elif \"itpnt\" in name:\n",
    "                num_channels = 384\n",
    "            elif \"itpns\" in name:\n",
    "                num_channels = 384\n",
    "            else:\n",
    "                num_channels = 512\n",
    "        else:\n",
    "            raise ValueError()\n",
    "        super().__init__(encoder, train_encoder, open_layers, num_channels)\n",
    "\n",
    "def build_encoder(cfg):\n",
    "    train_encoder = (cfg[\"TRAIN\"][\"ENCODER_MULTIPLIER\"] > 0) and (cfg[\"TRAIN\"][\"FREEZE_ENCODER\"] == False)\n",
    "    encoder = Encoder(cfg[\"MODEL\"][\"ENCODER\"][\"TYPE\"], train_encoder,\n",
    "                      cfg[\"MODEL\"][\"ENCODER\"][\"PRETRAIN_TYPE\"],\n",
    "                      cfg[\"DATA\"][\"SEARCH\"][\"SIZE\"], cfg[\"DATA\"][\"SEARCH\"][\"NUMBER\"],\n",
    "                      cfg[\"DATA\"][\"TEMPLATE\"][\"SIZE\"], cfg[\"DATA\"][\"TEMPLATE\"][\"NUMBER\"],\n",
    "                      cfg[\"TRAIN\"][\"ENCODER_OPEN\"], cfg)\n",
    "    return encoder\n",
    "\n",
    "class MambaBlock(nn.Module):\n",
    "    def __init__(self,dt_scale, d_model,d_inner,dt_rank,d_state,bias,d_conv,conv_bias,dt_init,dt_max,dt_min,dt_init_floor):\n",
    "        super().__init__()\n",
    "        #  projects block input from D to 2*ED (two branches)\n",
    "        self.dt_scale = dt_scale\n",
    "        self.d_model = d_model\n",
    "        self.d_inner = d_inner\n",
    "        self.dt_rank = dt_rank\n",
    "        self.d_state = d_state\n",
    "        self.in_proj = nn.Linear(self.d_model, 2 * self.d_inner, bias=bias)\n",
    "\n",
    "        self.conv1d = nn.Conv1d(in_channels=self.d_inner, out_channels=self.d_inner,\n",
    "                                kernel_size=d_conv, bias=conv_bias,\n",
    "                                groups=self.d_inner,\n",
    "                                padding=(d_conv - 1)//2)\n",
    "\n",
    "        #  projects x to input-dependent Δ, B, C\n",
    "        self.x_proj = nn.Linear(self.d_inner, self.dt_rank + 2 * self.d_state, bias=False)\n",
    "\n",
    "        #  projects Δ from dt_rank to d_inner\n",
    "        self.dt_proj = nn.Linear(self.dt_rank, self.d_inner, bias=True)\n",
    "\n",
    "        #  dt initialization\n",
    "        #  dt weights\n",
    "        dt_init_std = self.dt_rank ** -0.5 * self.dt_scale\n",
    "        if dt_init == \"constant\":\n",
    "            nn.init.constant_(self.dt_proj.weight, dt_init_std)\n",
    "        elif dt_init == \"random\":\n",
    "            nn.init.uniform_(self.dt_proj.weight, -dt_init_std, dt_init_std)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        # dt bias\n",
    "        dt = torch.exp(\n",
    "            torch.rand(self.d_inner) * (math.log(dt_max) - math.log(dt_min)) + math.log(dt_min)\n",
    "        ).clamp(min=dt_init_floor)\n",
    "        inv_dt = dt + torch.log(\n",
    "            -torch.expm1(-dt))  #  inverse of softplus: https://github.com/pytorch/pytorch/issues/72759\n",
    "        with torch.no_grad():\n",
    "            self.dt_proj.bias.copy_(inv_dt)\n",
    "        # self.dt_proj.bias._no_reinit = True # initialization would set all Linear.bias to zero, need to mark this one as _no_reinit\n",
    "        #  todo : explain why removed\n",
    "\n",
    "        # S4D real initialization\n",
    "        A = torch.arange(1, self.d_state + 1, dtype=torch.float32).repeat(self.d_inner, 1)\n",
    "        self.A_log = nn.Parameter(\n",
    "            torch.log(A))  # why store A in log ? to keep A < 0 (cf -torch.exp(...)) ? for gradient stability ?\n",
    "        self.D = nn.Parameter(torch.ones(self.d_inner))\n",
    "\n",
    "        #  projects block output from ED back to D\n",
    "        self.out_proj = nn.Linear(self.d_inner, self.d_model, bias=bias)\n",
    "    def forward(self, x, h):\n",
    "        #  x : (B,L, D)\n",
    "        # h : (B,L, ED, N)\n",
    "\n",
    "        #  y : (B, L, D)\n",
    "\n",
    "\n",
    "        xz = self.in_proj(x)  # (B, L,2*ED)\n",
    "        x, z = xz.chunk(2, dim=-1)  #  (B,L, ED), (B,L, ED)\n",
    "        x_cache = x.permute(0,2,1)#(B, ED,L)\n",
    "\n",
    "        #  x branch\n",
    "        x = self.conv1d( x_cache).permute(0,2,1) #  (B,L , ED)\n",
    "\n",
    "        x = F.silu(x)\n",
    "        y, h = self.ssm_step(x, h)\n",
    "        #y->B,L,ED;h->B,L,ED,N\n",
    "\n",
    "        #  z branch\n",
    "        z = F.silu(z)\n",
    "\n",
    "        output = y * z\n",
    "        output = self.out_proj(output)  #  (B, L, D)\n",
    "\n",
    "        return output, h\n",
    "\n",
    "    def ssm_step(self, x, h):\n",
    "        #  x : (B, L, ED)\n",
    "        #  h : (B, L, ED, N)\n",
    "\n",
    "        A = -torch.exp(\n",
    "            self.A_log.float())  # (ED, N) # todo : ne pas le faire tout le temps, puisque c'est indépendant de la timestep\n",
    "        D = self.D.float()\n",
    "        #  TODO remove .float()\n",
    "\n",
    "        deltaBC = self.x_proj(x)  #  (B, L, dt_rank+2*N)\n",
    "\n",
    "        delta, B, C = torch.split(deltaBC, [self.dt_rank, self.d_state, self.d_state],\n",
    "                                  dim=-1)  #  (B, L,dt_rank), (B, L, N), (B, L, N)\n",
    "        delta = F.softplus(self.dt_proj(delta))  #  (B, L, ED)\n",
    "\n",
    "        deltaA = torch.exp(delta.unsqueeze(-1) * A)  #  (B,L, ED, N)\n",
    "        deltaB = delta.unsqueeze(-1) * B.unsqueeze(2)  #  (B,L, ED, N)\n",
    "\n",
    "        BX = deltaB * (x.unsqueeze(-1))  #  (B, L,ED, N)\n",
    "\n",
    "        if h is None:\n",
    "            h = torch.zeros(x.size(0), x.size(1), self.d_inner, self.d_state, device=deltaA.device)  #  (B, L, ED, N)\n",
    "\n",
    "        h = deltaA * h + BX  #  (B, L, ED, N)\n",
    "\n",
    "        y = (h @ C.unsqueeze(-1)).squeeze(3)  #  (B, L, ED, N) @ (B, L, N, 1) -> (B, L, ED, 1)\n",
    "\n",
    "        y = y + D * x#B,L,ED\n",
    "\n",
    "        #  todo : pq h.squeeze(1) ??\n",
    "        return y, h\n",
    "    \n",
    "class DWConv(nn.Module):\n",
    "    def __init__(self, dim=768):\n",
    "        super().__init__()\n",
    "        self.dwconv = nn.Conv2d(dim, dim, 3, 1, 1, bias=True, groups=dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(1,0,2)\n",
    "        B, N, C = x.shape\n",
    "        x = x.transpose(1,2).view(B,C,int(N**0.5),int(N**0.5)).contiguous()\n",
    "        x = self.dwconv(x).flatten(2).transpose(1, 2)#B,N,C\n",
    "        x = x.permute(1,0,2)\n",
    "        return x\n",
    "class ConvFFN(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None,\n",
    "                 act_layer=nn.GELU, drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.dwconv = DWConv(hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.dwconv(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "    \n",
    "class ConvFFN(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None,\n",
    "                 act_layer=nn.GELU, drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.dwconv = DWConv(hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.dwconv(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "def drop_path(x, drop_prob: float = 0., training: bool = False, scale_by_keep: bool = True):\n",
    "    \"\"\"Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).\n",
    "\n",
    "    This is the same as the DropConnect impl I created for EfficientNet, etc networks, however,\n",
    "    the original name is misleading as 'Drop Connect' is a different form of dropout in a separate paper...\n",
    "    See discussion: https://github.com/tensorflow/tpu/issues/494#issuecomment-532968956 ... I've opted for\n",
    "    changing the layer and argument names to 'drop path' rather than mix DropConnect as a layer name and use\n",
    "    'survival rate' as the argument.\n",
    "\n",
    "    \"\"\"\n",
    "    if drop_prob == 0. or not training:\n",
    "        return x\n",
    "    keep_prob = 1 - drop_prob\n",
    "    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n",
    "    random_tensor = x.new_empty(shape).bernoulli_(keep_prob)\n",
    "    if keep_prob > 0.0 and scale_by_keep:\n",
    "        random_tensor.div_(keep_prob)\n",
    "    return x * random_tensor\n",
    "\n",
    "class DropPath(nn.Module):\n",
    "    \"\"\"Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\n",
    "    \"\"\"\n",
    "    def __init__(self, drop_prob: float = 0., scale_by_keep: bool = True):\n",
    "        super(DropPath, self).__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.scale_by_keep = scale_by_keep\n",
    "\n",
    "    def forward(self, x):\n",
    "        return drop_path(x, self.drop_prob, self.training, self.scale_by_keep)\n",
    "\n",
    "    def extra_repr(self):\n",
    "        return f'drop_prob={round(self.drop_prob,3):0.3f}'\n",
    "    \n",
    "\"\"\"class partial:\n",
    " \n",
    "\n",
    "    __slots__ = \"func\", \"args\", \"keywords\", \"__dict__\", \"__weakref__\"\n",
    "\n",
    "    def __new__(cls, func, /, *args, **keywords):\n",
    "        if not callable(func):\n",
    "            raise TypeError(\"the first argument must be callable\")\n",
    "\n",
    "        if hasattr(func, \"func\"):\n",
    "            args = func.args + args\n",
    "            keywords = {**func.keywords, **keywords}\n",
    "            func = func.func\n",
    "\n",
    "        self = super(partial, cls).__new__(cls)\n",
    "\n",
    "        self.func = func\n",
    "        self.args = args\n",
    "        self.keywords = keywords\n",
    "        return self\n",
    "\n",
    "class Extractor(nn.Module):\n",
    "    def __init__(self, d_model, num_heads=8, dropout=0.1,drop_path=0.1,\n",
    "                 norm_layer=partial(nn.LayerNorm, eps=1e-6)):\n",
    "        super().__init__()\n",
    "        self.query_norm = norm_layer(d_model)\n",
    "        self.feat_norm = norm_layer(d_model)\n",
    "        self.attn = nn.MultiheadAttention(d_model, num_heads, dropout=dropout)\n",
    "        #convffn\n",
    "        self.ffn = ConvFFN(in_features=d_model, hidden_features=int(d_model * 0.25), drop=0.)\n",
    "        self.ffn_norm = norm_layer(d_model)\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\"\"\"\n",
    "\n",
    "class Extractor(nn.Module):\n",
    "    def __init__(self, d_model, num_heads=8, dropout=0.1, drop_path=0.1,\n",
    "                 norm_layer=lambda x: nn.LayerNorm(x, eps=1e-6)):  # Замена partial на лямбду\n",
    "        super().__init__()\n",
    "        self.query_norm = norm_layer(d_model)\n",
    "        self.feat_norm = norm_layer(d_model)\n",
    "        self.attn = nn.MultiheadAttention(d_model, num_heads, dropout=dropout)\n",
    "        # convffn\n",
    "        self.ffn = ConvFFN(in_features=d_model, hidden_features=int(d_model * 0.25), drop=0.)\n",
    "        self.ffn_norm = norm_layer(d_model)\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "\n",
    "    def forward(self, query, feat):\n",
    "\n",
    "        def _inner_forward(query, feat):\n",
    "            # query:l,b,d;feat:l,b,d\n",
    "            attn = self.attn(self.query_norm(query),\n",
    "                             self.feat_norm(feat), self.feat_norm(feat))[0]\n",
    "            query = query + attn\n",
    "\n",
    "            query = query + self.drop_path(self.ffn(self.ffn_norm(query)))\n",
    "            return query\n",
    "\n",
    "        query = _inner_forward(query, feat)\n",
    "\n",
    "        return query\n",
    " \n",
    "class Injector(nn.Module):\n",
    "    def __init__(self, d_model, n_heads=8, norm_layer=lambda x: nn.LayerNorm(x, eps=1e-6), dropout=0.1, init_values=0.):\n",
    "        super().__init__()\n",
    "        self.query_norm = norm_layer(d_model)\n",
    "        self.feat_norm = norm_layer(d_model)\n",
    "        self.attn = nn.MultiheadAttention(d_model, n_heads, dropout=dropout)\n",
    "        self.gamma = nn.Parameter(init_values * torch.ones((d_model)), requires_grad=True)\n",
    "        \n",
    "    def forward(self, query,feat):\n",
    "            #query:l,b,d;feat:l,b,d\n",
    "        def _inner_forward(query, feat):\n",
    "\n",
    "            attn = self.attn(self.query_norm(query),\n",
    "                             self.feat_norm(feat),self.feat_norm(feat))[0]\n",
    "            return query + self.gamma * attn\n",
    "        query = _inner_forward(query, feat)\n",
    "        return query    \n",
    "    \n",
    "    \"\"\"class Injector(nn.Module):\n",
    "    \n",
    "    \n",
    "    \n",
    "    def __init__(self, d_model, n_heads=8,norm_layer=partial(nn.LayerNorm, eps=1e-6),  dropout=0.1,\n",
    "                 init_values=0.):\n",
    "        super().__init__()\n",
    "        self.query_norm = norm_layer(d_model)\n",
    "        self.feat_norm = norm_layer(d_model)\n",
    "        self.attn = nn.MultiheadAttention(d_model, n_heads,dropout=dropout)\n",
    "        self.gamma = nn.Parameter(init_values * torch.ones((d_model)), requires_grad=True)\n",
    "\n",
    "    def forward(self, query,feat):\n",
    "            #query:l,b,d;feat:l,b,d\n",
    "        def _inner_forward(query, feat):\n",
    "\n",
    "            attn = self.attn(self.query_norm(query),\n",
    "                             self.feat_norm(feat),self.feat_norm(feat))[0]\n",
    "            return query + self.gamma * attn\n",
    "        query = _inner_forward(query, feat)\n",
    "        return query\"\"\"\n",
    "    \n",
    "\n",
    "class InteractionBlock(nn.Module):\n",
    "    def __init__(self, d_model, extra_extractor, grad_ckpt):\n",
    "        super().__init__()\n",
    "        self.grad_ckpt = grad_ckpt\n",
    "        self.injector = Injector(d_model=d_model)\n",
    "        self.extractor = Extractor(d_model=d_model)\n",
    "        if extra_extractor:\n",
    "            self.extra_extractors = nn.Sequential(*[\n",
    "                Extractor(d_model=d_model)\n",
    "                for _ in range(2)])\n",
    "        else:\n",
    "            self.extra_extractors = None\n",
    "\n",
    "    def forward(self,x,xs,blocks):\n",
    "        x = self.injector(x.permute(1,0,2),xs.permute(1,0,2)).permute(1,0,2)\n",
    "        for idx,blk in enumerate(blocks):\n",
    "            x = checkpoint.checkpoint(blk, x, None,use_reentrant=False) if self.grad_ckpt else blk(x,None)\n",
    "        xs = checkpoint.checkpoint(self.extractor, xs.permute(1,0,2),x.permute(1,0,2),use_reentrant=False).permute(1,0,2) \\\n",
    "            if self.grad_ckpt else self.extractor(xs.permute(1, 0, 2), x.permute(1, 0, 2)).permute(1, 0, 2)  # b,n,c\n",
    "        # xs = self.extractor(xs.permute(1,0,2),x.permute(1,0,2)).permute(1,0,2)#b,n,c\n",
    "        if self.extra_extractors is not None:\n",
    "            for extractor in self.extra_extractors:\n",
    "                xs = checkpoint.checkpoint(extractor, xs.permute(1, 0, 2), x.permute(1, 0, 2), use_reentrant=False).permute(1, 0, 2) \\\n",
    "                    if self.grad_ckpt else extractor(xs.permute(1, 0, 2), x.permute(1, 0, 2)).permute(1, 0,2)  # b,n,c\n",
    "                # xs = extractor(xs.permute(1,0,2),x.permute(1,0,2)).permute(1,0,2)\n",
    "        return x,xs\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, d_model: int, eps: float = 1e-5):\n",
    "        super().__init__()\n",
    "\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(d_model))\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps) * self.weight\n",
    "\n",
    "        return output\n",
    "    \n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self,dt_scale, d_model,d_inner,dt_rank,d_state,bias,d_conv,conv_bias,dt_init,dt_max,dt_min,dt_init_floor,grad_ckpt):\n",
    "        super().__init__()\n",
    "\n",
    "        self.grad_ckpt = grad_ckpt\n",
    "        self.mixer = MambaBlock(dt_scale,d_model,d_inner,dt_rank,d_state,bias,d_conv,conv_bias,dt_init,dt_max,dt_min,dt_init_floor)\n",
    "        self.norm = RMSNorm(d_model)\n",
    "\n",
    "    def forward(self, x, h):\n",
    "        #  x : (B, L, D)\n",
    "        # h : (B, L, ED, N)\n",
    "        #  output : (B,L, D)\n",
    "\n",
    "        x = self.norm(x)\n",
    "        output, h = checkpoint.checkpoint(self.mixer,x,h,use_reentrant=False) if self.grad_ckpt else self.mixer(x, h)\n",
    "        output = output + x\n",
    "        return output, h\n",
    "    \n",
    "class Mamba_Neck(nn.Module):\n",
    "    def __init__(self, in_channel=512,d_model=512,d_inner=1024,bias=False,n_layers=4,dt_rank=32,d_state=16,d_conv=3,dt_min=0.001,\n",
    "                 dt_max=0.1,dt_init='random',dt_scale=1.0,conv_bias=True,dt_init_floor=0.0001,grad_ckpt=False):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_inner = d_inner\n",
    "        self.bias = bias\n",
    "        self.dt_rank = dt_rank\n",
    "        self.d_state = d_state\n",
    "        self.dt_scale = dt_scale\n",
    "        self.num_channels = self.d_model\n",
    "        self.layers = nn.ModuleList(\n",
    "            [ResidualBlock(dt_scale,d_model,d_inner,dt_rank,d_state,bias,d_conv,conv_bias,dt_init,dt_max,dt_min,dt_init_floor,grad_ckpt)\n",
    "             for _ in range(n_layers)])\n",
    "        self.interactions = nn.ModuleList([\n",
    "            InteractionBlock(d_model=d_model,extra_extractor=(True if i == n_layers - 1 else False),grad_ckpt=grad_ckpt)\n",
    "            for i in range(n_layers)\n",
    "        ])\n",
    "        # self.norm_f = RMSNorm(config.d_model)\n",
    "\n",
    "    def forward(self, x,xs,h,blocks,interaction_indexes):\n",
    "        #  x : (B, L, D)\n",
    "        #  caches : [cache(layer) for all layers], cache : (h, inputs)\n",
    "\n",
    "        #  y : (B, L, D)\n",
    "        #  caches : [cache(layer) for all layers], cache : (h, inputs)\n",
    "        for i,index in enumerate(interaction_indexes):\n",
    "            xs, h[i] = self.layers[i](xs, h[i])\n",
    "            x,xs = self.interactions[i](x,xs,blocks[index[0]:index[1]])\n",
    "\n",
    "        return x, xs, h\n",
    "def build_neck(cfg,encoder):\n",
    "    in_channel = encoder.num_channels\n",
    "    d_model = cfg[\"MODEL\"][\"NECK\"][\"D_MODEL\"]\n",
    "    n_layers = cfg[\"MODEL\"][\"NECK\"][\"N_LAYERS\"]\n",
    "    d_state = cfg[\"MODEL\"][\"NECK\"][\"D_STATE\"]\n",
    "    grad_ckpt = cfg[\"MODEL\"][\"ENCODER\"][\"GRAD_CKPT\"]\n",
    "    neck = Mamba_Neck(in_channel=in_channel,d_model=d_model,d_inner=2*d_model,n_layers=n_layers,dt_rank=d_model//16,d_state=d_state,grad_ckpt=grad_ckpt)\n",
    "    return neck\n",
    "\n",
    "def box_xyxy_to_cxcywh(x):\n",
    "    x0, y0, x1, y1 = x.unbind(-1)\n",
    "    b = [(x0 + x1) / 2, (y0 + y1) / 2,\n",
    "         (x1 - x0), (y1 - y0)]\n",
    "    return torch.stack(b, dim=-1)\n",
    "\n",
    "class MLPPredictor(nn.Module):\n",
    "    def __init__(self, inplanes=64, channel=256, feat_sz=20, stride=16):\n",
    "        super(MLPPredictor, self).__init__()\n",
    "        self.feat_sz = feat_sz\n",
    "        self.stride = stride\n",
    "        self.img_sz = self.feat_sz * self.stride\n",
    "\n",
    "        self.num_layers = 3\n",
    "        h = [channel] * (self.num_layers - 1)\n",
    "        self.layers_cls = nn.ModuleList(nn.Linear(n, k)\n",
    "                                        for n, k in zip([inplanes] + h, h + [1]))\n",
    "        self.layers_reg = nn.ModuleList(nn.Linear(n, k)\n",
    "                                        for n, k in zip([inplanes] + h, h + [4]))\n",
    "\n",
    "        # for p in self.parameters():\n",
    "        #     if p.dim() > 1:\n",
    "        #         nn.init.xavier_uniform_(p)\n",
    "\n",
    "    def forward(self, x, gt_score_map=None):\n",
    "        \"\"\" Forward pass with input x. \"\"\"\n",
    "        score_map, offset_map = self.get_score_map(x)\n",
    "\n",
    "        # assert gt_score_map is None\n",
    "        if gt_score_map is None:\n",
    "            bbox = self.cal_bbox(score_map, offset_map)\n",
    "        else:\n",
    "            bbox = self.cal_bbox(gt_score_map.unsqueeze(1), offset_map)\n",
    "\n",
    "        return score_map, bbox, offset_map\n",
    "\n",
    "    def cal_bbox(self, score_map, offset_map, return_score=False):\n",
    "        max_score, idx = torch.max(score_map.flatten(1), dim=1, keepdim=True)\n",
    "        idx_y = torch.div(idx, self.feat_sz, rounding_mode='floor')\n",
    "        idx_x = idx % self.feat_sz\n",
    "\n",
    "        idx = idx.unsqueeze(1).expand(idx.shape[0], 4, 1) # torch.Size([32, 4, 1])\n",
    "        offset = offset_map.flatten(2).gather(dim=2, index=idx).squeeze(-1)\n",
    "        # offset: (l,t,r,b)\n",
    "\n",
    "        # x1, y1, x2, y2\n",
    "        bbox = torch.cat([idx_x.to(torch.float) / self.feat_sz - offset[:, :1], # the offset should not divide the self.feat_sz, since I use the sigmoid to limit it in (0,1)\n",
    "                          idx_y.to(torch.float) / self.feat_sz - offset[:, 1:2],\n",
    "                          idx_x.to(torch.float) / self.feat_sz + offset[:, 2:3],\n",
    "                          idx_y.to(torch.float) / self.feat_sz + offset[:, 3:4],\n",
    "                          ], dim=1)\n",
    "        bbox = box_xyxy_to_cxcywh(bbox)\n",
    "        if return_score:\n",
    "            return bbox, max_score\n",
    "        return bbox\n",
    "\n",
    "    def get_score_map(self, x):\n",
    "\n",
    "        def _sigmoid(x):\n",
    "            y = torch.clamp(x.sigmoid_(), min=1e-4, max=1 - 1e-4)\n",
    "            return y\n",
    "\n",
    "        x_cls = x\n",
    "        for i, layer in enumerate(self.layers_cls):\n",
    "            x_cls = F.relu(layer(x_cls)) if i < self.num_layers - 1 else layer(x_cls)\n",
    "        x_cls = x_cls.permute(0,2,1).reshape(-1,1,self.feat_sz,self.feat_sz)\n",
    "\n",
    "        x_reg = x\n",
    "        for i, layer in enumerate(self.layers_reg):\n",
    "            x_reg = F.relu(layer(x_reg)) if i < self.num_layers - 1 else layer(x_reg)\n",
    "        x_reg = x_reg.permute(0, 2, 1).reshape(-1, 4, self.feat_sz, self.feat_sz)\n",
    "\n",
    "        return _sigmoid(x_cls), _sigmoid(x_reg)\n",
    "    \n",
    "class FrozenBatchNorm2d(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    BatchNorm2d where the batch statistics and the affine parameters are fixed.\n",
    "\n",
    "    Copy-paste from torchvision.misc.ops with added eps before rqsrt,\n",
    "    without which any other models than torchvision.models.resnet[18,34,50,101]\n",
    "    produce nans.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n):\n",
    "        super(FrozenBatchNorm2d, self).__init__()\n",
    "        self.register_buffer(\"weight\", torch.ones(n))\n",
    "        self.register_buffer(\"bias\", torch.zeros(n))\n",
    "        self.register_buffer(\"running_mean\", torch.zeros(n))\n",
    "        self.register_buffer(\"running_var\", torch.ones(n))\n",
    "\n",
    "    def _load_from_state_dict(self, state_dict, prefix, local_metadata, strict,\n",
    "                              missing_keys, unexpected_keys, error_msgs):\n",
    "        num_batches_tracked_key = prefix + 'num_batches_tracked'\n",
    "        if num_batches_tracked_key in state_dict:\n",
    "            del state_dict[num_batches_tracked_key]\n",
    "\n",
    "        super(FrozenBatchNorm2d, self)._load_from_state_dict(\n",
    "            state_dict, prefix, local_metadata, strict,\n",
    "            missing_keys, unexpected_keys, error_msgs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # move reshapes to the beginning\n",
    "        # to make it fuser-friendly\n",
    "        w = self.weight.reshape(1, -1, 1, 1)\n",
    "        b = self.bias.reshape(1, -1, 1, 1)\n",
    "        rv = self.running_var.reshape(1, -1, 1, 1)\n",
    "        rm = self.running_mean.reshape(1, -1, 1, 1)\n",
    "        eps = 1e-5\n",
    "        scale = w * (rv + eps).rsqrt()  # rsqrt(x): 1/sqrt(x), r: reciprocal\n",
    "        bias = b - rm * scale\n",
    "        return x * scale + bias\n",
    "\n",
    "def conv(in_planes, out_planes, kernel_size=3, stride=1, padding=1, dilation=1,\n",
    "         freeze_bn=False):\n",
    "    if freeze_bn:\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride,\n",
    "                      padding=padding, dilation=dilation, bias=True),\n",
    "            FrozenBatchNorm2d(out_planes),\n",
    "            nn.ReLU(inplace=True))\n",
    "    else:\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride,\n",
    "                      padding=padding, dilation=dilation, bias=True),\n",
    "            nn.BatchNorm2d(out_planes),\n",
    "            nn.ReLU(inplace=True))\n",
    "    \n",
    "def conv(in_planes, out_planes, kernel_size=3, stride=1, padding=1, dilation=1,\n",
    "         freeze_bn=False):\n",
    "    if freeze_bn:\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride,\n",
    "                      padding=padding, dilation=dilation, bias=True),\n",
    "            FrozenBatchNorm2d(out_planes),\n",
    "            nn.ReLU(inplace=True))\n",
    "    else:\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride,\n",
    "                      padding=padding, dilation=dilation, bias=True),\n",
    "            nn.BatchNorm2d(out_planes),\n",
    "            nn.ReLU(inplace=True))\n",
    "    \n",
    "class CenterPredictor(nn.Module, ):\n",
    "    def __init__(self, inplanes=64, channel=256, feat_sz=20, stride=16, freeze_bn=False):\n",
    "        super(CenterPredictor, self).__init__()\n",
    "        self.feat_sz = feat_sz\n",
    "        self.stride = stride\n",
    "        self.img_sz = self.feat_sz * self.stride\n",
    "\n",
    "        # corner predict\n",
    "        self.conv1_ctr = conv(inplanes, channel, freeze_bn=freeze_bn)\n",
    "        self.conv2_ctr = conv(channel, channel // 2, freeze_bn=freeze_bn)\n",
    "        self.conv3_ctr = conv(channel // 2, channel // 4, freeze_bn=freeze_bn)\n",
    "        self.conv4_ctr = conv(channel // 4, channel // 8, freeze_bn=freeze_bn)\n",
    "        self.conv5_ctr = nn.Conv2d(channel // 8, 1, kernel_size=1)\n",
    "\n",
    "        # size regress\n",
    "        self.conv1_offset = conv(inplanes, channel, freeze_bn=freeze_bn)\n",
    "        self.conv2_offset = conv(channel, channel // 2, freeze_bn=freeze_bn)\n",
    "        self.conv3_offset = conv(channel // 2, channel // 4, freeze_bn=freeze_bn)\n",
    "        self.conv4_offset = conv(channel // 4, channel // 8, freeze_bn=freeze_bn)\n",
    "        self.conv5_offset = nn.Conv2d(channel // 8, 2, kernel_size=1)\n",
    "\n",
    "        # size regress\n",
    "        self.conv1_size = conv(inplanes, channel, freeze_bn=freeze_bn)\n",
    "        self.conv2_size = conv(channel, channel // 2, freeze_bn=freeze_bn)\n",
    "        self.conv3_size = conv(channel // 2, channel // 4, freeze_bn=freeze_bn)\n",
    "        self.conv4_size = conv(channel // 4, channel // 8, freeze_bn=freeze_bn)\n",
    "        self.conv5_size = nn.Conv2d(channel // 8, 2, kernel_size=1)\n",
    "\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "\n",
    "    def forward(self, x, gt_score_map=None):\n",
    "        \"\"\" Forward pass with input x. \"\"\"\n",
    "        score_map_ctr, size_map, offset_map = self.get_score_map(x) # x: torch.Size([b, c, h, w])\n",
    "        # score_map_ctr: torch.Size([32, 1, 16, 16]) size_map: torch.Size([32, 2, 16, 16]) offset_map: torch.Size([32, 2, 16, 16])\n",
    "\n",
    "        # assert gt_score_map is None\n",
    "        if gt_score_map is None:\n",
    "            bbox = self.cal_bbox(score_map_ctr, size_map, offset_map)\n",
    "        else:\n",
    "            bbox = self.cal_bbox(gt_score_map.unsqueeze(1), size_map, offset_map)\n",
    "\n",
    "        return score_map_ctr, bbox, size_map, offset_map\n",
    "\n",
    "    def cal_bbox(self, score_map_ctr, size_map, offset_map, return_score=False):\n",
    "        max_score, idx = torch.max(score_map_ctr.flatten(1), dim=1, keepdim=True) # score_map_ctr.flatten(1): torch.Size([32, 256]) idx: torch.Size([32, 1]) max_score: torch.Size([32, 1])\n",
    "        idx_y = torch.div(idx, self.feat_sz, rounding_mode='floor')\n",
    "        idx_x = idx % self.feat_sz\n",
    "\n",
    "        idx = idx.unsqueeze(1).expand(idx.shape[0], 2, 1)\n",
    "        size = size_map.flatten(2).gather(dim=2, index=idx) # size_map: torch.Size([32, 2, 16, 16])  size_map.flatten(2): torch.Size([32, 2, 256])\n",
    "        offset = offset_map.flatten(2).gather(dim=2, index=idx).squeeze(-1)\n",
    "\n",
    "        # bbox = torch.cat([idx_x - size[:, 0] / 2, idx_y - size[:, 1] / 2,\n",
    "        #                   idx_x + size[:, 0] / 2, idx_y + size[:, 1] / 2], dim=1) / self.feat_sz\n",
    "        # cx, cy, w, h\n",
    "        bbox = torch.cat([(idx_x.to(torch.float) + offset[:, :1]) / self.feat_sz,\n",
    "                          (idx_y.to(torch.float) + offset[:, 1:]) / self.feat_sz,\n",
    "                          size.squeeze(-1)], dim=1)\n",
    "\n",
    "        if return_score:\n",
    "            return bbox, max_score\n",
    "        return bbox\n",
    "\n",
    "    def get_pred(self, score_map_ctr, size_map, offset_map):\n",
    "        max_score, idx = torch.max(score_map_ctr.flatten(1), dim=1, keepdim=True)\n",
    "        idx_y = idx // self.feat_sz\n",
    "        idx_x = idx % self.feat_sz\n",
    "\n",
    "        idx = idx.unsqueeze(1).expand(idx.shape[0], 2, 1)\n",
    "        size = size_map.flatten(2).gather(dim=2, index=idx)\n",
    "        offset = offset_map.flatten(2).gather(dim=2, index=idx).squeeze(-1)\n",
    "\n",
    "        # bbox = torch.cat([idx_x - size[:, 0] / 2, idx_y - size[:, 1] / 2,\n",
    "        #                   idx_x + size[:, 0] / 2, idx_y + size[:, 1] / 2], dim=1) / self.feat_sz\n",
    "        return size * self.feat_sz, offset\n",
    "\n",
    "    def get_score_map(self, x):\n",
    "\n",
    "        def _sigmoid(x):\n",
    "            y = torch.clamp(x.sigmoid_(), min=1e-4, max=1 - 1e-4)\n",
    "            return y\n",
    "\n",
    "        # ctr branch\n",
    "        x_ctr1 = self.conv1_ctr(x)\n",
    "        x_ctr2 = self.conv2_ctr(x_ctr1)\n",
    "        x_ctr3 = self.conv3_ctr(x_ctr2)\n",
    "        x_ctr4 = self.conv4_ctr(x_ctr3)\n",
    "        score_map_ctr = self.conv5_ctr(x_ctr4)\n",
    "\n",
    "        # offset branch\n",
    "        x_offset1 = self.conv1_offset(x)\n",
    "        x_offset2 = self.conv2_offset(x_offset1)\n",
    "        x_offset3 = self.conv3_offset(x_offset2)\n",
    "        x_offset4 = self.conv4_offset(x_offset3)\n",
    "        score_map_offset = self.conv5_offset(x_offset4)\n",
    "\n",
    "        # size branch\n",
    "        x_size1 = self.conv1_size(x)\n",
    "        x_size2 = self.conv2_size(x_size1)\n",
    "        x_size3 = self.conv3_size(x_size2)\n",
    "        x_size4 = self.conv4_size(x_size3)\n",
    "        score_map_size = self.conv5_size(x_size4)\n",
    "        return _sigmoid(score_map_ctr), _sigmoid(score_map_size), score_map_offset\n",
    "    \n",
    "def build_decoder(cfg, encoder):\n",
    "    num_channels_enc = encoder.num_channels\n",
    "    stride = cfg[\"MODEL\"][\"ENCODER\"][\"STRIDE\"]\n",
    "    if cfg[\"MODEL\"][\"DECODER\"][\"TYPE\"] == \"MLP\":\n",
    "        in_channel = num_channels_enc\n",
    "        hidden_dim = cfg[\"MODEL\"][\"DECODER\"][\"NUM_CHANNELS\"]\n",
    "        feat_sz = int(cfg[\"DATA\"][\"SEARCH\"][\"SIZE\"] / stride)\n",
    "        mlp_head = MLPPredictor(inplanes=in_channel, channel=hidden_dim,\n",
    "                                feat_sz=feat_sz, stride=stride)\n",
    "        return mlp_head\n",
    "    elif \"CORNER\" in cfg[\"MODEL\"][\"DECODER\"][\"TYPE\"]:\n",
    "        feat_sz = int(cfg[\"DATA\"][\"SEARCH\"][\"SIZE\"] / stride)\n",
    "        channel = getattr(cfg[\"MODEL\"], \"NUM_CHANNELS\", 256)\n",
    "        print(\"head channel: %d\" % channel)\n",
    "        if cfg[\"MODEL\"][\"HEAD\"][\"TYPE\"] == \"CORNER\":\n",
    "            corner_head = Corner_Predictor(inplanes=cfg[\"MODEL\"][\"HIDDEN_DIM\"], channel=channel,\n",
    "                                           feat_sz=feat_sz, stride=stride)\n",
    "        else:\n",
    "            raise ValueError()\n",
    "        return corner_head\n",
    "    elif cfg[\"MODEL\"][\"DECODER\"][\"TYPE\"] == \"CENTER\":\n",
    "        in_channel = num_channels_enc\n",
    "        out_channel = cfg[\"MODEL\"][\"DECODER\"][\"NUM_CHANNELS\"]\n",
    "        feat_sz = int(cfg[\"DATA\"][\"SEARCH\"][\"SIZE\"] / stride)\n",
    "        center_head = CenterPredictor(inplanes=in_channel, channel=out_channel,\n",
    "                                      feat_sz=feat_sz, stride=stride)\n",
    "        return center_head\n",
    "    else:\n",
    "        raise ValueError(\"HEAD TYPE %s is not supported.\" % cfg[\"MODEL\"][\"HEAD_TYPE\"])\n",
    "    \n",
    "class Corner_Predictor(nn.Module):\n",
    "    \"\"\" Corner Predictor module\"\"\"\n",
    "\n",
    "    def __init__(self, inplanes=64, channel=256, feat_sz=20, stride=16, freeze_bn=False):\n",
    "        super(Corner_Predictor, self).__init__()\n",
    "        self.feat_sz = feat_sz\n",
    "        self.stride = stride\n",
    "        self.img_sz = self.feat_sz * self.stride\n",
    "        '''top-left corner'''\n",
    "        self.conv1_tl = conv(inplanes, channel, freeze_bn=freeze_bn)\n",
    "        self.conv2_tl = conv(channel, channel // 2, freeze_bn=freeze_bn)\n",
    "        self.conv3_tl = conv(channel // 2, channel // 4, freeze_bn=freeze_bn)\n",
    "        self.conv4_tl = conv(channel // 4, channel // 8, freeze_bn=freeze_bn)\n",
    "        self.conv5_tl = nn.Conv2d(channel // 8, 1, kernel_size=1)\n",
    "\n",
    "        '''bottom-right corner'''\n",
    "        self.conv1_br = conv(inplanes, channel, freeze_bn=freeze_bn)\n",
    "        self.conv2_br = conv(channel, channel // 2, freeze_bn=freeze_bn)\n",
    "        self.conv3_br = conv(channel // 2, channel // 4, freeze_bn=freeze_bn)\n",
    "        self.conv4_br = conv(channel // 4, channel // 8, freeze_bn=freeze_bn)\n",
    "        self.conv5_br = nn.Conv2d(channel // 8, 1, kernel_size=1)\n",
    "\n",
    "        '''about coordinates and indexs'''\n",
    "        with torch.no_grad():\n",
    "            self.indice = torch.arange(0, self.feat_sz).view(-1, 1) * self.stride\n",
    "            # generate mesh-grid\n",
    "            self.coord_x = self.indice.repeat((self.feat_sz, 1)) \\\n",
    "                .view((self.feat_sz * self.feat_sz,)).float().cuda()\n",
    "            self.coord_y = self.indice.repeat((1, self.feat_sz)) \\\n",
    "                .view((self.feat_sz * self.feat_sz,)).float().cuda()\n",
    "\n",
    "    def forward(self, x, return_dist=False, softmax=True):\n",
    "        \"\"\" Forward pass with input x. \"\"\"\n",
    "        score_map_tl, score_map_br = self.get_score_map(x)\n",
    "        if return_dist:\n",
    "            coorx_tl, coory_tl, prob_vec_tl = self.soft_argmax(score_map_tl, return_dist=True, softmax=softmax)\n",
    "            coorx_br, coory_br, prob_vec_br = self.soft_argmax(score_map_br, return_dist=True, softmax=softmax)\n",
    "            return torch.stack((coorx_tl, coory_tl, coorx_br, coory_br), dim=1) / self.img_sz, prob_vec_tl, prob_vec_br\n",
    "        else:\n",
    "            coorx_tl, coory_tl = self.soft_argmax(score_map_tl)\n",
    "            coorx_br, coory_br = self.soft_argmax(score_map_br)\n",
    "            return torch.stack((coorx_tl, coory_tl, coorx_br, coory_br), dim=1) / self.img_sz\n",
    "\n",
    "    def get_score_map(self, x):\n",
    "        # top-left branch\n",
    "        x_tl1 = self.conv1_tl(x)\n",
    "        x_tl2 = self.conv2_tl(x_tl1)\n",
    "        x_tl3 = self.conv3_tl(x_tl2)\n",
    "        x_tl4 = self.conv4_tl(x_tl3)\n",
    "        score_map_tl = self.conv5_tl(x_tl4)\n",
    "\n",
    "        # bottom-right branch\n",
    "        x_br1 = self.conv1_br(x)\n",
    "        x_br2 = self.conv2_br(x_br1)\n",
    "        x_br3 = self.conv3_br(x_br2)\n",
    "        x_br4 = self.conv4_br(x_br3)\n",
    "        score_map_br = self.conv5_br(x_br4)\n",
    "        return score_map_tl, score_map_br\n",
    "\n",
    "    def soft_argmax(self, score_map, return_dist=False, softmax=True):\n",
    "        \"\"\" get soft-argmax coordinate for a given heatmap \"\"\"\n",
    "        score_vec = score_map.view((-1, self.feat_sz * self.feat_sz))  # (batch, feat_sz * feat_sz)\n",
    "        prob_vec = nn.functional.softmax(score_vec, dim=1)\n",
    "        exp_x = torch.sum((self.coord_x * prob_vec), dim=1)\n",
    "        exp_y = torch.sum((self.coord_y * prob_vec), dim=1)\n",
    "        if return_dist:\n",
    "            if softmax:\n",
    "                return exp_x, exp_y, prob_vec\n",
    "            else:\n",
    "                return exp_x, exp_y, score_vec\n",
    "        else:\n",
    "            return exp_x, exp_y\n",
    "        \n",
    "class Preprocessor(object):\n",
    "    def __init__(self):\n",
    "        self.mean = torch.tensor([0.485, 0.456, 0.406]).view((1, 3, 1, 1)).cuda()\n",
    "        self.std = torch.tensor([0.229, 0.224, 0.225]).view((1, 3, 1, 1)).cuda()\n",
    "        self.mm_mean = torch.tensor([0.485, 0.456, 0.406, 0.485, 0.456, 0.406]).view((1, 6, 1, 1)).cuda()\n",
    "        self.mm_std = torch.tensor([0.229, 0.224, 0.225, 0.229, 0.224, 0.225]).view((1, 6, 1, 1)).cuda()\n",
    "\n",
    "    def process(self, img_arr: np.ndarray):\n",
    "        if img_arr.shape[-1] == 6:\n",
    "            mean = self.mm_mean\n",
    "            std = self.mm_std\n",
    "        else:\n",
    "            mean = self.mean\n",
    "            std = self.std\n",
    "        # Deal with the image patch\n",
    "        img_tensor = torch.tensor(img_arr).cuda().float().permute((2,0,1)).unsqueeze(dim=0)\n",
    "        # img_tensor = torch.tensor(img_arr).float().permute((2,0,1)).unsqueeze(dim=0)\n",
    "        img_tensor_norm = ((img_tensor / 255.0) - mean) / std  # (1,3,H,W)\n",
    "        return img_tensor_norm\n",
    "    \n",
    "def hann1d(sz: int, centered = True) -> torch.Tensor:\n",
    "    \"\"\"1D cosine window.\"\"\"\n",
    "    if centered:\n",
    "        return 0.5 * (1 - torch.cos((2 * math.pi / (sz + 1)) * torch.arange(1, sz + 1).float()))\n",
    "    w = 0.5 * (1 + torch.cos((2 * math.pi / (sz + 2)) * torch.arange(0, sz//2 + 1).float()))\n",
    "    return torch.cat([w, w[1:sz-sz//2].flip((0,))])\n",
    "    \n",
    "def hann2d(sz: torch.Tensor, centered = True) -> torch.Tensor:\n",
    "    \"\"\"2D cosine window.\"\"\"\n",
    "    return hann1d(sz[0].item(), centered).reshape(1, 1, -1, 1) * hann1d(sz[1].item(), centered).reshape(1, 1, 1, -1)    \n",
    "\n",
    "def sample_target(im, target_bb, search_area_factor, output_sz=None):\n",
    "    \"\"\" Extracts a square crop centered at target_bb box, of area search_area_factor^2 times target_bb area\n",
    "\n",
    "    args:\n",
    "        im - cv image\n",
    "        target_bb - target box [x, y, w, h]\n",
    "        search_area_factor - Ratio of crop size to target size\n",
    "        output_sz - (float) Size to which the extracted crop is resized (always square). If None, no resizing is done.\n",
    "\n",
    "    returns:\n",
    "        cv image - extracted crop\n",
    "        float - the factor by which the crop has been resized to make the crop size equal output_size\n",
    "    \"\"\"\n",
    "    if not isinstance(target_bb, list):\n",
    "        x, y, w, h = target_bb.tolist()\n",
    "    else:\n",
    "        x, y, w, h = target_bb\n",
    "    # Crop image\n",
    "    crop_sz = math.ceil(math.sqrt(w * h) * search_area_factor)\n",
    "\n",
    "    if crop_sz < 1:\n",
    "        raise Exception('Too small bounding box.')\n",
    "\n",
    "    x1 = round(x + 0.5 * w - crop_sz * 0.5)\n",
    "    x2 = x1 + crop_sz\n",
    "\n",
    "    y1 = round(y + 0.5 * h - crop_sz * 0.5)\n",
    "    y2 = y1 + crop_sz\n",
    "\n",
    "    x1_pad = max(0, -x1)\n",
    "    x2_pad = max(x2 - im.shape[1] + 1, 0)\n",
    "\n",
    "    y1_pad = max(0, -y1)\n",
    "    y2_pad = max(y2 - im.shape[0] + 1, 0)\n",
    "\n",
    "    # Crop target\n",
    "    im_crop = im[y1 + y1_pad:y2 - y2_pad, x1 + x1_pad:x2 - x2_pad, :]\n",
    "\n",
    "    # Pad\n",
    "    im_crop_padded = cv2.copyMakeBorder(im_crop, y1_pad, y2_pad, x1_pad, x2_pad, cv2.BORDER_CONSTANT)\n",
    "    # deal with attention mask\n",
    "    H, W, _ = im_crop_padded.shape\n",
    "\n",
    "    if output_sz is not None:\n",
    "        resize_factor = output_sz / crop_sz\n",
    "        im_crop_padded = cv2.resize(im_crop_padded, (output_sz, output_sz))\n",
    "\n",
    "        return im_crop_padded, resize_factor\n",
    "\n",
    "    else:\n",
    "        return im_crop_padded, 1.0\n",
    "def transform_image_to_crop(box_in: torch.Tensor, box_extract: torch.Tensor, resize_factor: float,\n",
    "                            crop_sz: torch.Tensor, normalize=False) -> torch.Tensor:\n",
    "    \"\"\" Transform the box co-ordinates from the original image co-ordinates to the co-ordinates of the cropped image\n",
    "    args:\n",
    "        box_in - the box for which the co-ordinates are to be transformed\n",
    "        box_extract - the box about which the image crop has been extracted.\n",
    "        resize_factor - the ratio between the original image scale and the scale of the image crop\n",
    "        crop_sz - size of the cropped image\n",
    "\n",
    "    returns:\n",
    "        torch.Tensor - transformed co-ordinates of box_in\n",
    "    \"\"\"\n",
    "    box_extract_center = box_extract[0:2] + 0.5 * box_extract[2:4]\n",
    "\n",
    "    box_in_center = box_in[0:2] + 0.5 * box_in[2:4]\n",
    "\n",
    "    box_out_center = (crop_sz - 1) / 2 + (box_in_center - box_extract_center) * resize_factor\n",
    "    box_out_wh = box_in[2:4] * resize_factor\n",
    "\n",
    "    box_out = torch.cat((box_out_center - 0.5 * box_out_wh, box_out_wh))\n",
    "    if normalize:\n",
    "        return box_out / (crop_sz[0]-1)\n",
    "    else:\n",
    "        return box_out\n",
    "def clip_box(box: list, H, W, margin=0):\n",
    "    x1, y1, w, h = box\n",
    "    x2, y2 = x1 + w, y1 + h\n",
    "    x1 = min(max(0, x1), W-margin)\n",
    "    x2 = min(max(margin, x2), W)\n",
    "    y1 = min(max(0, y1), H-margin)\n",
    "    y2 = min(max(margin, y2), H)\n",
    "    w = max(margin, x2-x1)\n",
    "    h = max(margin, y2-y1)\n",
    "    return [x1, y1, w, h]\n",
    "\n",
    "class BaseTracker:\n",
    "    \"\"\"Base class for all trackers.\"\"\"\n",
    "\n",
    "    def __init__(self, params):\n",
    "        self.params = params\n",
    "        self.visdom = None\n",
    "\n",
    "    def predicts_segmentation_mask(self):\n",
    "        return False\n",
    "\n",
    "    def initialize(self, image, info: dict) -> dict:\n",
    "        \"\"\"Overload this function in your tracker. This should initialize the model.\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def track(self, image, info: dict = None) -> dict:\n",
    "        \"\"\"Overload this function in your tracker. This should track in the frame and update the model.\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def visdom_draw_tracking(self, image, box, segmentation=None):\n",
    "        # Упрощенная обработка box без OrderedDict\n",
    "        if isinstance(box, dict):  # Проверяем на обычный dict вместо OrderedDict\n",
    "            box = list(box.values())  # Берем только значения\n",
    "        elif not isinstance(box, (list, tuple)):  # Если не коллекция\n",
    "            box = (box,)  # Превращаем в кортеж\n",
    "        \n",
    "        # Визуализация\n",
    "        if segmentation is None:\n",
    "            self.visdom.register((image, *box), 'Tracking', 1, 'Tracking')\n",
    "        else:\n",
    "            self.visdom.register((image, *box, segmentation), 'Tracking', 1, 'Tracking')\n",
    "\n",
    "\n",
    "class MCITRACK(BaseTracker):\n",
    "    def __init__(self, params):\n",
    "        \n",
    "        super(MCITRACK, self).__init__(params)\n",
    "        network = build_mcitrack(params.cfg)\n",
    "        network.load_state_dict(torch.load(\"MCITRACK_ep0300.pth.tar\", map_location='cpu')['net'], strict=True)\n",
    "        \n",
    "        self.cfg = params.cfg\n",
    "        self.network = network.cuda()\n",
    "        self.network.eval()\n",
    "        self.preprocessor = Preprocessor()\n",
    "        self.state = None\n",
    "\n",
    "        self.fx_sz = self.cfg[\"TEST\"][\"SEARCH_SIZE\"] // self.cfg[\"MODEL\"][\"ENCODER\"][\"STRIDE\"]\n",
    "        if self.cfg[\"TEST\"][\"WINDOW\"] == True:  # for window penalty\n",
    "            self.output_window = hann2d(torch.tensor([self.fx_sz, self.fx_sz]).long(), centered=True).cuda()\n",
    "\n",
    "        self.num_template = self.cfg[\"TEST\"][\"NUM_TEMPLATES\"]\n",
    "\n",
    "   \n",
    "        self.frame_id = 0\n",
    "        # for update\n",
    "        self.h_state = [None] * self.cfg[\"MODEL\"][\"NECK\"][\"N_LAYERS\"]\n",
    "\n",
    "\n",
    "\n",
    "        self.memory_bank = self.cfg[\"TEST\"][\"MB\"][\"DEFAULT\"]\n",
    "        self.update_h_t = self.cfg[\"TEST\"][\"UPH\"][\"DEFAULT\"]\n",
    "        self.update_threshold = self.cfg[\"TEST\"][\"UPT\"][\"DEFAULT\"]\n",
    "        self.update_intervals = self.cfg[\"TEST\"][\"INTER\"][\"DEFAULT\"]\n",
    "        print(\"Update threshold is: \", self.memory_bank)\n",
    "\n",
    "    def initialize(self, image, info: dict):\n",
    "\n",
    "\n",
    "        # get the initial templates\n",
    "        z_patch_arr, resize_factor = sample_target(image, info['init_bbox'], self.params.template_factor,\n",
    "                                                   output_sz=self.params.template_size)\n",
    "        z_patch_arr = z_patch_arr\n",
    "        template = self.preprocessor.process(z_patch_arr)\n",
    "        self.template_list = [template] * self.num_template\n",
    "\n",
    "        self.state = info['init_bbox']\n",
    "        prev_box_crop = transform_image_to_crop(torch.tensor(info['init_bbox']),\n",
    "                                                torch.tensor(info['init_bbox']),\n",
    "                                                resize_factor,\n",
    "                                                torch.Tensor([self.params.template_size, self.params.template_size]),\n",
    "                                                normalize=True)\n",
    "        self.template_anno_list = [prev_box_crop.to(template.device).unsqueeze(0)] * self.num_template\n",
    "        self.frame_id = 0\n",
    "        self.memory_template_list = self.template_list.copy()\n",
    "        self.memory_template_anno_list = self.template_anno_list.copy()\n",
    "\n",
    "\n",
    "    def track(self, image, info: dict = None):\n",
    "        H, W, _ = image.shape\n",
    "        self.frame_id += 1\n",
    "        x_patch_arr, resize_factor = sample_target(image, self.state, self.params.search_factor,\n",
    "                                                   output_sz=self.params.search_size)  # (x1, y1, w, h)\n",
    "        search = self.preprocessor.process(x_patch_arr)\n",
    "        search_list = [search]\n",
    "\n",
    "        # run the encoder\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            enc_opt = self.network.forward_encoder(self.template_list, search_list, self.template_anno_list)\n",
    "\n",
    "        # run the time neck\n",
    "        with torch.no_grad():\n",
    "            hidden_state = self.h_state.copy()\n",
    "            encoder_out,out_neck, h = self.network.forward_neck(enc_opt, hidden_state)\n",
    "        # run the decoder\n",
    "        with torch.no_grad():\n",
    "            out_dict = self.network.forward_decoder(feature=out_neck)\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            out_dict = self.network.forward(\n",
    "                template_list=self.template_list,\n",
    "                search_list=search_list,\n",
    "                template_anno_list=self.template_anno_list,\n",
    "                \n",
    "                gt_score_map=None\n",
    "            )\n",
    "            \n",
    "        \"\"\"print(len(self.template_list))\n",
    "        for i in self.template_list:\n",
    "            print(i.shape)\n",
    "        print(len(search_list))\n",
    "        for i in search_list:\n",
    "            print(i.shape)\n",
    "        print(len(self.template_anno_list))\n",
    "        for i in self.template_anno_list:\n",
    "            print(i.shape)\n",
    "        print(len(self.h_state))\n",
    "        \n",
    "        print(\"-\"*50)\n",
    "        \"\"\"\n",
    "        \n",
    "\n",
    "        # add hann windows\n",
    "        pred_score_map = out_dict['score_map']\n",
    "        if self.cfg[\"TEST\"][\"WINDOW\"] == True:  # for window penalty\n",
    "            response = self.output_window * pred_score_map\n",
    "        else:\n",
    "            response = pred_score_map\n",
    "        if 'size_map' in out_dict.keys():\n",
    "            pred_boxes, conf_score = self.network.decoder.cal_bbox(response, out_dict['size_map'],\n",
    "                                                                   out_dict['offset_map'], return_score=True)\n",
    "        else:\n",
    "            pred_boxes, conf_score = self.network.decoder.cal_bbox(response,\n",
    "                                                                   out_dict['offset_map'],\n",
    "                                                                   return_score=True)\n",
    "        pred_boxes = pred_boxes.view(-1, 4)\n",
    "        # Baseline: Take the mean of all pred boxes as the final result\n",
    "        pred_box = (pred_boxes.mean(dim=0) * self.params.search_size / resize_factor).tolist()  # (cx, cy, w, h) [0,1]\n",
    "        # get the final box result\n",
    "        self.state = clip_box(self.map_box_back(pred_box, resize_factor), H, W, margin=10)\n",
    "        # update hiden state\n",
    "        self.h_state = h\n",
    "        if conf_score.item() < self.update_h_t:\n",
    "            self.h_state = [None] * self.cfg[\"MODEL\"][\"NECK\"][\"N_LAYERS\"]\n",
    "\n",
    "        # update the template\n",
    "        if self.num_template > 1:\n",
    "            if (conf_score > self.update_threshold):\n",
    "                z_patch_arr, resize_factor = sample_target(image, self.state, self.params.template_factor,\n",
    "                                                           output_sz=self.params.template_size)\n",
    "                template = self.preprocessor.process(z_patch_arr)\n",
    "                self.memory_template_list.append(template)\n",
    "                prev_box_crop = transform_image_to_crop(torch.tensor(self.state),\n",
    "                                                        torch.tensor(self.state),\n",
    "                                                        resize_factor,\n",
    "                                                        torch.Tensor(\n",
    "                                                            [self.params.template_size, self.params.template_size]),\n",
    "                                                        normalize=True)\n",
    "                self.memory_template_anno_list.append(prev_box_crop.to(template.device).unsqueeze(0))\n",
    "                if len(self.memory_template_list) > self.memory_bank:\n",
    "                    self.memory_template_list.pop(0)\n",
    "                    self.memory_template_anno_list.pop(0)\n",
    "        if (self.frame_id % self.update_intervals == 0):\n",
    "            assert len(self.memory_template_anno_list) == len(self.memory_template_list)\n",
    "            len_list = len(self.memory_template_anno_list)\n",
    "            interval = len_list // self.num_template\n",
    "            for i in range(1, self.num_template):\n",
    "                idx = interval * i\n",
    "                if idx > len_list:\n",
    "                    idx = len_list\n",
    "                self.template_list.append(self.memory_template_list[idx])\n",
    "                self.template_list.pop(1)\n",
    "                self.template_anno_list.append(self.memory_template_anno_list[idx])\n",
    "                self.template_anno_list.pop(1)\n",
    "        assert len(self.template_list) == self.num_template\n",
    "\n",
    "\n",
    "\n",
    "        return {\"target_bbox\": self.state,\n",
    "                \"best_score\": conf_score}\n",
    "\n",
    "    def map_box_back(self, pred_box: list, resize_factor: float):\n",
    "        cx_prev, cy_prev = self.state[0] + 0.5 * self.state[2], self.state[1] + 0.5 * self.state[3]\n",
    "        cx, cy, w, h = pred_box\n",
    "        half_side = 0.5 * self.params.search_size / resize_factor\n",
    "        cx_real = cx + (cx_prev - half_side)\n",
    "        cy_real = cy + (cy_prev - half_side)\n",
    "        return [cx_real - 0.5 * w, cy_real - 0.5 * h, w, h]\n",
    "\n",
    "    def map_box_back_batch(self, pred_box: torch.Tensor, resize_factor: float):\n",
    "        cx_prev, cy_prev = self.state[0] + 0.5 * self.state[2], self.state[1] + 0.5 * self.state[3]\n",
    "        cx, cy, w, h = pred_box.unbind(-1)  # (N,4) --> (N,)\n",
    "        half_side = 0.5 * self.params.search_size / resize_factor\n",
    "        cx_real = cx + (cx_prev - half_side)\n",
    "        cy_real = cy + (cy_prev - half_side)\n",
    "        return torch.stack([cx_real - 0.5 * w, cy_real - 0.5 * h, w, h], dim=-1)\n",
    "\n",
    "class MCITrack(nn.Module):\n",
    "    \"\"\" This is the base class for MCITrack \"\"\"\n",
    "    def __init__(self, encoder, decoder, neck, cfg,\n",
    "                 num_frames=1, num_template=1, decoder_type=\"CENTER\"):\n",
    "        \"\"\"\n",
    "        Initializes the model.\n",
    "\n",
    "        Parameters:\n",
    "            encoder: torch module of the encoder to be used. See encoder.py\n",
    "            decoder: torch module of the decoder architecture. See decoder.py\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder_type = decoder_type\n",
    "        self.neck = neck\n",
    "\n",
    "        self.num_patch_x = self.encoder.body.num_patches_search\n",
    "        self.num_patch_z = self.encoder.body.num_patches_template\n",
    "        self.fx_sz = int(math.sqrt(self.num_patch_x))\n",
    "        self.fz_sz = int(math.sqrt(self.num_patch_z))\n",
    "\n",
    "        self.decoder = decoder\n",
    "\n",
    "        self.num_frames = num_frames\n",
    "        self.num_template = num_template\n",
    "        self.freeze_en = cfg[\"TRAIN\"][\"FREEZE_ENCODER\"]\n",
    "        self.interaction_indexes = cfg[\"MODEL\"][\"ENCODER\"][\"INTERACTION_INDEXES\"]\n",
    "\n",
    "    def forward(self, template_list, search_list, template_anno_list, gt_score_map=None):\n",
    "        \"\"\"\n",
    "        Forward pass that sequentially executes the encoder, neck, and decoder operations.\n",
    "\n",
    "        Args:\n",
    "            template_list: List of template images.\n",
    "            search_list: List of search images.\n",
    "            template_anno_list: List of template annotations.\n",
    "            neck_h_state: Hidden state for the neck.\n",
    "            gt_score_map: Ground truth score map (optional, used in decoder).\n",
    "\n",
    "        Returns:\n",
    "            dict: Output from the decoder, including predicted bounding boxes, score maps, etc.\n",
    "        \"\"\"\n",
    "        # Step 1: Forward pass through the encoder\n",
    "        \n",
    "        neck_h_state=[None,None,None,None]\n",
    "        \n",
    "        xz = self.encoder(template_list, search_list, template_anno_list)\n",
    "\n",
    "        # Step 2: Forward pass through the neck\n",
    "        xs = xz[:, 0:self.num_patch_x]  # Extract patch embeddings\n",
    "        x, xs, h = self.neck(\n",
    "            xz, xs, neck_h_state, \n",
    "            self.encoder.body.blocks, \n",
    "            self.interaction_indexes\n",
    "        )\n",
    "        x = self.encoder.body.fc_norm(x)\n",
    "        xs = xs + x[:, 0:self.num_patch_x]  # Updated patch embeddings\n",
    "\n",
    "        # Step 3: Forward pass through the decoder\n",
    "        bs, HW, C = xs.size()\n",
    "        if self.decoder_type in ['CORNER', 'CENTER']:\n",
    "            xs = xs.permute((0, 2, 1)).contiguous()\n",
    "            xs = xs.view(bs, C, self.fx_sz, self.fx_sz)\n",
    "\n",
    "        if self.decoder_type == \"CORNER\":\n",
    "            # Run the corner head\n",
    "            pred_box, score_map = self.decoder(xs, True)\n",
    "            outputs_coord = box_xyxy_to_cxcywh(pred_box)\n",
    "            outputs_coord_new = outputs_coord.view(bs, 1, 4)\n",
    "            return {\n",
    "                'pred_boxes': outputs_coord_new,\n",
    "                'score_map': score_map\n",
    "            }\n",
    "\n",
    "        elif self.decoder_type == \"CENTER\":\n",
    "            # Run the center head\n",
    "            score_map_ctr, bbox, size_map, offset_map = self.decoder(xs, gt_score_map)\n",
    "            outputs_coord = bbox\n",
    "            outputs_coord_new = outputs_coord.view(bs, 1, 4)\n",
    "            return {\n",
    "                'pred_boxes': outputs_coord_new,\n",
    "                'score_map': score_map_ctr,\n",
    "                'size_map': size_map,\n",
    "                'offset_map': offset_map\n",
    "            }\n",
    "\n",
    "        elif self.decoder_type == \"MLP\":\n",
    "            # Run the MLP head\n",
    "            score_map, bbox, offset_map = self.decoder(xs, gt_score_map)\n",
    "            outputs_coord = bbox\n",
    "            outputs_coord_new = outputs_coord.view(bs, 1, 4)\n",
    "            return {\n",
    "                'pred_boxes': outputs_coord_new,\n",
    "                'score_map': score_map,\n",
    "                'offset_map': offset_map\n",
    "            }\n",
    "\n",
    "        else:\n",
    "            raise NotImplementedError(f\"Decoder type not supported: {self.decoder_type}\")\n",
    "\n",
    "def build_mcitrack(cfg):\n",
    "    encoder = build_encoder(cfg)\n",
    "    neck = build_neck(cfg,encoder)\n",
    "    decoder = build_decoder(cfg, neck)\n",
    "    model = MCITrack(\n",
    "        encoder,\n",
    "        decoder,\n",
    "        neck,\n",
    "        cfg,\n",
    "        num_frames = cfg[\"DATA\"][\"SEARCH\"][\"NUMBER\"],\n",
    "        num_template = cfg[\"DATA\"][\"TEMPLATE\"][\"NUMBER\"],\n",
    "        decoder_type=cfg[\"MODEL\"][\"DECODER\"][\"TYPE\"],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def get_tracker_class():\n",
    "    return MCITRACK\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "407a71c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = {}\n",
    "\n",
    "# MODEL\n",
    "cfg[\"MODEL\"] = {}\n",
    "\n",
    "# MODEL.ENCODER\n",
    "cfg[\"MODEL\"][\"ENCODER\"] = {\n",
    "    \"TYPE\": \"dinov2_vitb14\",  # encoder model\n",
    "    \"DROP_PATH\": 0,\n",
    "    \"PRETRAIN_TYPE\": \"mae\",  # mae, default, or scratch. This parameter is not activated for dinov2.\n",
    "    \"USE_CHECKPOINT\": False,  # to save the memory.\n",
    "    \"STRIDE\": 14,\n",
    "    \"POS_TYPE\": 'interpolate',  # type of loading the positional encoding. \"interpolate\" or \"index\".\n",
    "    \"TOKEN_TYPE_INDICATE\": False,  # add a token_type_embedding to indicate the search, template_foreground, template_background\n",
    "    \"INTERACTION_INDEXES\": [[0, 6], [6, 12], [12, 18], [18, 24]],\n",
    "    \"GRAD_CKPT\": False\n",
    "}\n",
    "\n",
    "# MODEL.NECK\n",
    "cfg[\"MODEL\"][\"NECK\"] = {\n",
    "    \"N_LAYERS\": 4,\n",
    "    \"D_MODEL\": 512,\n",
    "    \"D_STATE\": 16  # MAMABA_HIDDEN_STATE\n",
    "}\n",
    "\n",
    "# MODEL.DECODER\n",
    "cfg[\"MODEL\"][\"DECODER\"] = {\n",
    "    \"TYPE\": \"CENTER\",  # MLP, CORNER, CENTER\n",
    "    \"NUM_CHANNELS\": 256\n",
    "}\n",
    "\n",
    "# TRAIN\n",
    "cfg[\"TRAIN\"] = {\n",
    "    \"LR\": 0.0001,\n",
    "    \"WEIGHT_DECAY\": 0.0001,\n",
    "    \"EPOCH\": 500,\n",
    "    \"LR_DROP_EPOCH\": 400,\n",
    "    \"BATCH_SIZE\": 8,\n",
    "    \"NUM_WORKER\": 8,\n",
    "    \"OPTIMIZER\": \"ADAMW\",\n",
    "    \"ENCODER_MULTIPLIER\": 0.1,  # encoder's LR = this factor * LR\n",
    "    \"FREEZE_ENCODER\": False,  # for freezing the parameters of encoder\n",
    "    \"ENCODER_OPEN\": [],  # only for debug, open some layers of encoder when FREEZE_ENCODER is True\n",
    "    \"CE_WEIGHT\": 1.0,  # weight for cross-entropy loss\n",
    "    \"GIOU_WEIGHT\": 2.0,\n",
    "    \"L1_WEIGHT\": 5.0,\n",
    "    \"PRINT_INTERVAL\": 50,  # interval to print the training log\n",
    "    \"GRAD_CLIP_NORM\": 0.1,\n",
    "    \"FIX_BN\": False,\n",
    "    \"ENCODER_W\": \"\",\n",
    "    \"TYPE\": \"normal\",  # normal, peft or fft\n",
    "    \"PRETRAINED_PATH\": None\n",
    "}\n",
    "\n",
    "# TRAIN.SCHEDULER\n",
    "cfg[\"TRAIN\"][\"SCHEDULER\"] = {\n",
    "    \"TYPE\": \"step\",\n",
    "    \"DECAY_RATE\": 0.1\n",
    "}\n",
    "\n",
    "# DATA\n",
    "cfg[\"DATA\"] = {\n",
    "    \"MEAN\": [0.485, 0.456, 0.406],\n",
    "    \"STD\": [0.229, 0.224, 0.225],\n",
    "    \"MAX_SAMPLE_INTERVAL\": 200,\n",
    "    \"SAMPLER_MODE\": \"order\",\n",
    "    \"LOADER\": \"tracking\"\n",
    "}\n",
    "\n",
    "# DATA.TRAIN\n",
    "cfg[\"DATA\"][\"TRAIN\"] = {\n",
    "    \"DATASETS_NAME\": [\"LASOT\", \"GOT10K_vottrain\"],\n",
    "    \"DATASETS_RATIO\": [1, 1],\n",
    "    \"SAMPLE_PER_EPOCH\": 60000\n",
    "}\n",
    "\n",
    "# DATA.SEARCH\n",
    "cfg[\"DATA\"][\"SEARCH\"] = {\n",
    "    \"NUMBER\": 1,  # number of search region, only support 1 for now.\n",
    "    \"SIZE\": 256,\n",
    "    \"FACTOR\": 4.0,\n",
    "    \"CENTER_JITTER\": 3.5,\n",
    "    \"SCALE_JITTER\": 0.5\n",
    "}\n",
    "\n",
    "# DATA.TEMPLATE\n",
    "cfg[\"DATA\"][\"TEMPLATE\"] = {\n",
    "    \"NUMBER\": 1,\n",
    "    \"SIZE\": 128,\n",
    "    \"FACTOR\": 2.0,\n",
    "    \"CENTER_JITTER\": 0,\n",
    "    \"SCALE_JITTER\": 0\n",
    "}\n",
    "\n",
    "# TEST\n",
    "cfg[\"TEST\"] = {\n",
    "    \"TEMPLATE_FACTOR\": 4.0,\n",
    "    \"TEMPLATE_SIZE\": 256,\n",
    "    \"SEARCH_FACTOR\": 2.0,\n",
    "    \"SEARCH_SIZE\": 128,\n",
    "    \"EPOCH\": 500,\n",
    "    \"WINDOW\": False,  # window penalty\n",
    "    \"NUM_TEMPLATES\": 1\n",
    "}\n",
    "\n",
    "# TEST.UPT\n",
    "cfg[\"TEST\"][\"UPT\"] = {\n",
    "    \"DEFAULT\": 1,\n",
    "    \"LASOT\": 0,\n",
    "    \"LASOT_EXTENSION_SUBSET\": 0,\n",
    "    \"TRACKINGNET\": 0,\n",
    "    \"TNL2K\": 0,\n",
    "    \"NFS\": 0,\n",
    "    \"UAV\": 0,\n",
    "    \"VOT20\": 0,\n",
    "    \"GOT10K_TEST\": 0\n",
    "}\n",
    "\n",
    "# TEST.UPH\n",
    "cfg[\"TEST\"][\"UPH\"] = {\n",
    "    \"DEFAULT\": 1,\n",
    "    \"LASOT\": 0,\n",
    "    \"LASOT_EXTENSION_SUBSET\": 0,\n",
    "    \"TRACKINGNET\": 0,\n",
    "    \"TNL2K\": 0,\n",
    "    \"NFS\": 0,\n",
    "    \"UAV\": 0,\n",
    "    \"VOT20\": 0,\n",
    "    \"GOT10K_TEST\": 0\n",
    "}\n",
    "\n",
    "# TEST.INTER\n",
    "cfg[\"TEST\"][\"INTER\"] = {\n",
    "    \"DEFAULT\": 999999,\n",
    "    \"LASOT\": 0,\n",
    "    \"LASOT_EXTENSION_SUBSET\": 0,\n",
    "    \"TRACKINGNET\": 0,\n",
    "    \"TNL2K\": 0,\n",
    "    \"NFS\": 0,\n",
    "    \"UAV\": 0,\n",
    "    \"VOT20\": 0,\n",
    "    \"GOT10K_TEST\": 0\n",
    "}\n",
    "\n",
    "# TEST.MB\n",
    "cfg[\"TEST\"][\"MB\"] = {\n",
    "    \"DEFAULT\": 500,\n",
    "    \"LASOT\": 0,\n",
    "    \"LASOT_EXTENSION_SUBSET\": 0,\n",
    "    \"TRACKINGNET\": 0,\n",
    "    \"TNL2K\": 0,\n",
    "    \"NFS\": 0,\n",
    "    \"UAV\": 0,\n",
    "    \"VOT20\": 0,\n",
    "    \"GOT10K_TEST\": 0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f2804bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test config:  {'MODEL': {'ENCODER': {'TYPE': 'fastitpnt', 'DROP_PATH': 0.1, 'PRETRAIN_TYPE': './fast_itpn_tiny_1600e_1k.pt', 'USE_CHECKPOINT': False, 'STRIDE': 16, 'POS_TYPE': 'index', 'TOKEN_TYPE_INDICATE': True, 'INTERACTION_INDEXES': [[4, 7], [7, 10], [10, 13], [13, 16]], 'GRAD_CKPT': False}, 'NECK': {'N_LAYERS': 4, 'D_MODEL': 384, 'D_STATE': 16}, 'DECODER': {'TYPE': 'CENTER', 'NUM_CHANNELS': 256}}, 'TRAIN': {'LR': 0.0004, 'WEIGHT_DECAY': 0.0001, 'EPOCH': 300, 'LR_DROP_EPOCH': 240, 'BATCH_SIZE': 64, 'NUM_WORKER': 10, 'OPTIMIZER': 'ADAMW', 'ENCODER_MULTIPLIER': 0.1, 'FREEZE_ENCODER': False, 'ENCODER_OPEN': [], 'CE_WEIGHT': 1.0, 'GIOU_WEIGHT': 2.0, 'L1_WEIGHT': 5.0, 'PRINT_INTERVAL': 50, 'GRAD_CLIP_NORM': 0.1, 'FIX_BN': False, 'ENCODER_W': '', 'TYPE': 'normal', 'PRETRAINED_PATH': None, 'SCHEDULER': {'TYPE': 'step', 'DECAY_RATE': 0.1}}, 'DATA': {'MEAN': [0.485, 0.456, 0.406], 'STD': [0.229, 0.224, 0.225], 'MAX_SAMPLE_INTERVAL': 400, 'SAMPLER_MODE': 'order', 'LOADER': 'tracking', 'TRAIN': {'DATASETS_NAME': ['LASOT', 'GOT10K_vottrain', 'COCO17', 'TRACKINGNET', 'VASTTRACK'], 'DATASETS_RATIO': [1, 1, 1, 1, 1], 'SAMPLE_PER_EPOCH': 60000}, 'SEARCH': {'NUMBER': 2, 'SIZE': 224, 'FACTOR': 4.0, 'CENTER_JITTER': 3.5, 'SCALE_JITTER': 0.5}, 'TEMPLATE': {'NUMBER': 5, 'SIZE': 112, 'FACTOR': 2.0, 'CENTER_JITTER': 0, 'SCALE_JITTER': 0}}, 'TEST': {'TEMPLATE_FACTOR': 2.0, 'TEMPLATE_SIZE': 112, 'SEARCH_FACTOR': 4.0, 'SEARCH_SIZE': 224, 'EPOCH': 300, 'WINDOW': True, 'NUM_TEMPLATES': 5, 'UPT': {'DEFAULT': 1, 'LASOT': 0.8, 'LASOT_EXTENSION_SUBSET': 0.85, 'TRACKINGNET': 0.5, 'TNL2K': 0.5, 'NFS': 0.8, 'UAV': 0.2, 'VOT20': 0.4, 'GOT10K_TEST': 0}, 'UPH': {'DEFAULT': 1, 'LASOT': 0.88, 'LASOT_EXTENSION_SUBSET': 0.97, 'TRACKINGNET': 0.9, 'TNL2K': 0.9, 'NFS': 0.92, 'UAV': 0.91, 'VOT20': 0.94, 'GOT10K_TEST': 0}, 'INTER': {'DEFAULT': 999999, 'LASOT': 70, 'LASOT_EXTENSION_SUBSET': 50, 'TRACKINGNET': 20, 'TNL2K': 20, 'NFS': 90, 'UAV': 1, 'VOT20': 1, 'GOT10K_TEST': 0}, 'MB': {'DEFAULT': 500, 'LASOT': 500, 'LASOT_EXTENSION_SUBSET': 500, 'TRACKINGNET': 200, 'TNL2K': 500, 'NFS': 500, 'UAV': 400, 'VOT20': 500, 'GOT10K_TEST': 0}}}\n"
     ]
    }
   ],
   "source": [
    "class TrackerParams:\n",
    "    \"\"\"Class for tracker parameters.\"\"\"\n",
    "    def set_default_values(self, default_vals: dict):\n",
    "        for name, val in default_vals.items():\n",
    "            if not hasattr(self, name):\n",
    "                setattr(self, name, val)\n",
    "\n",
    "    def get(self, name: str, *default):\n",
    "        \"\"\"Get a parameter value with the given name. If it does not exists, it return the default value given as a\n",
    "        second argument or returns an error if no default value is given.\"\"\"\n",
    "        if len(default) > 1:\n",
    "            raise ValueError('Can only give one default value.')\n",
    "\n",
    "        if not default:\n",
    "            return getattr(self, name)\n",
    "\n",
    "        return getattr(self, name, default[0])\n",
    "\n",
    "    def has(self, name: str):\n",
    "        \"\"\"Check if there exist a parameter with the given name.\"\"\"\n",
    "        return hasattr(self, name)\n",
    "\n",
    "def _update_config(base_cfg, exp_cfg):\n",
    "    if isinstance(base_cfg, dict) and isinstance(exp_cfg, dict):\n",
    "        for k, v in exp_cfg.items():\n",
    "            if k in base_cfg:\n",
    "                if not isinstance(v, dict):\n",
    "                    base_cfg[k] = v\n",
    "                else:\n",
    "                    _update_config(base_cfg[k], v)\n",
    "            else:\n",
    "                raise ValueError(\"{} not exist in config.py\".format(k))\n",
    "    else:\n",
    "        return\n",
    "\n",
    "def update_config_from_file(filename):\n",
    "    exp_config = None\n",
    "    with open(filename) as f:\n",
    "        exp_config = yaml.safe_load(f)\n",
    "        _update_config(cfg, exp_config)\n",
    "    \n",
    "def parameters(yaml_name: str):\n",
    "    params = TrackerParams()\n",
    "\n",
    "    yaml_file = \"mcitrack_t224.yaml\"\n",
    "    update_config_from_file(yaml_file)\n",
    "    params.cfg = cfg\n",
    "    print(\"test config: \", cfg)\n",
    "\n",
    "    params.yaml_name = yaml_name\n",
    "    # template and search region\n",
    "    params.template_factor = cfg[\"TEST\"][\"TEMPLATE_FACTOR\"]\n",
    "    params.template_size = cfg[\"TEST\"][\"TEMPLATE_SIZE\"]\n",
    "    params.search_factor = cfg[\"TEST\"][\"SEARCH_FACTOR\"]\n",
    "    params.search_size = cfg[\"TEST\"][\"SEARCH_SIZE\"]\n",
    "\n",
    "    # Network checkpoint path\n",
    "    params.checkpoint = \"fast_itpn_tiny_1600e_1k.pt\"\n",
    "    # whether to save boxes from all queries\n",
    "    params.save_all_boxes = False\n",
    "\n",
    "    return params\n",
    "\n",
    "params = parameters(\"./mcitrack_t224.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae331c67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update threshold is:  500\n"
     ]
    }
   ],
   "source": [
    "treacker = MCITRACK(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "097a57d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MCITrack(\n",
       "  (encoder): Encoder(\n",
       "    (body): Fast_iTPN(\n",
       "      (patch_embed): ConvPatchEmbed(\n",
       "        (proj): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))\n",
       "      )\n",
       "      (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "      (blocks): ModuleList(\n",
       "        (0): ConvMlpBlock(\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): ConvMlp(\n",
       "            (fc1): Conv2d(96, 288, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act): GELU(approximate=none)\n",
       "            (ffn_ln): LayerNorm((288,), eps=1e-06, elementwise_affine=True)\n",
       "            (fc2): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): ConvPatchMerge(\n",
       "          (norm): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
       "          (reduction): Conv2d(96, 192, kernel_size=(2, 2), stride=(2, 2))\n",
       "        )\n",
       "        (2): ConvMlpBlock(\n",
       "          (drop_path): DropPath(p=0.007692307699471712)\n",
       "          (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): ConvMlp(\n",
       "            (fc1): Conv2d(192, 576, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (act): GELU(approximate=none)\n",
       "            (ffn_ln): LayerNorm((576,), eps=1e-06, elementwise_affine=True)\n",
       "            (fc2): Conv2d(576, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): ConvPatchMerge(\n",
       "          (norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "          (reduction): Conv2d(192, 384, kernel_size=(2, 2), stride=(2, 2))\n",
       "        )\n",
       "        (4): Block(\n",
       "          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (q_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "            (k_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "            (v_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path): DropPath(p=0.015384615398943424)\n",
       "          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): SwiGLU(\n",
       "            (w1): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (w2): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (act): SiLU()\n",
       "            (ffn_ln): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "            (w3): Linear(in_features=1152, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): Block(\n",
       "          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (q_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "            (k_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "            (v_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path): DropPath(p=0.023076923564076424)\n",
       "          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): SwiGLU(\n",
       "            (w1): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (w2): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (act): SiLU()\n",
       "            (ffn_ln): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "            (w3): Linear(in_features=1152, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): Block(\n",
       "          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (q_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "            (k_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "            (v_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path): DropPath(p=0.03076923079788685)\n",
       "          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): SwiGLU(\n",
       "            (w1): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (w2): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (act): SiLU()\n",
       "            (ffn_ln): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "            (w3): Linear(in_features=1152, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): Block(\n",
       "          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (q_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "            (k_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "            (v_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path): DropPath(p=0.03846153989434242)\n",
       "          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): SwiGLU(\n",
       "            (w1): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (w2): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (act): SiLU()\n",
       "            (ffn_ln): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "            (w3): Linear(in_features=1152, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): Block(\n",
       "          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (q_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "            (k_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "            (v_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path): DropPath(p=0.04615384712815285)\n",
       "          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): SwiGLU(\n",
       "            (w1): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (w2): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (act): SiLU()\n",
       "            (ffn_ln): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "            (w3): Linear(in_features=1152, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): Block(\n",
       "          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (q_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "            (k_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "            (v_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path): DropPath(p=0.05384615436196327)\n",
       "          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): SwiGLU(\n",
       "            (w1): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (w2): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (act): SiLU()\n",
       "            (ffn_ln): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "            (w3): Linear(in_features=1152, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): Block(\n",
       "          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (q_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "            (k_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "            (v_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path): DropPath(p=0.0615384615957737)\n",
       "          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): SwiGLU(\n",
       "            (w1): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (w2): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (act): SiLU()\n",
       "            (ffn_ln): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "            (w3): Linear(in_features=1152, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): Block(\n",
       "          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (q_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "            (k_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "            (v_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path): DropPath(p=0.06923077255487442)\n",
       "          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): SwiGLU(\n",
       "            (w1): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (w2): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (act): SiLU()\n",
       "            (ffn_ln): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "            (w3): Linear(in_features=1152, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (12): Block(\n",
       "          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (q_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "            (k_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "            (v_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path): DropPath(p=0.07692307978868484)\n",
       "          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): SwiGLU(\n",
       "            (w1): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (w2): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (act): SiLU()\n",
       "            (ffn_ln): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "            (w3): Linear(in_features=1152, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (13): Block(\n",
       "          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (q_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "            (k_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "            (v_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path): DropPath(p=0.08461538702249527)\n",
       "          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): SwiGLU(\n",
       "            (w1): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (w2): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (act): SiLU()\n",
       "            (ffn_ln): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "            (w3): Linear(in_features=1152, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (14): Block(\n",
       "          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (q_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "            (k_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "            (v_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path): DropPath(p=0.0923076942563057)\n",
       "          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): SwiGLU(\n",
       "            (w1): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (w2): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (act): SiLU()\n",
       "            (ffn_ln): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "            (w3): Linear(in_features=1152, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (15): Block(\n",
       "          (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (q_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "            (k_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "            (v_proj): Linear(in_features=384, out_features=384, bias=False)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path): DropPath(p=0.10000000149011612)\n",
       "          (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): SwiGLU(\n",
       "            (w1): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (w2): Linear(in_features=384, out_features=1152, bias=True)\n",
       "            (act): SiLU()\n",
       "            (ffn_ln): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "            (w3): Linear(in_features=1152, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (norm): Identity()\n",
       "      (fc_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "      (head): Identity()\n",
       "    )\n",
       "  )\n",
       "  (neck): Mamba_Neck(\n",
       "    (layers): ModuleList(\n",
       "      (0): ResidualBlock(\n",
       "        (mixer): MambaBlock(\n",
       "          (in_proj): Linear(in_features=384, out_features=1536, bias=False)\n",
       "          (conv1d): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(1,), groups=768)\n",
       "          (x_proj): Linear(in_features=768, out_features=56, bias=False)\n",
       "          (dt_proj): Linear(in_features=24, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=384, bias=False)\n",
       "        )\n",
       "        (norm): RMSNorm()\n",
       "      )\n",
       "      (1): ResidualBlock(\n",
       "        (mixer): MambaBlock(\n",
       "          (in_proj): Linear(in_features=384, out_features=1536, bias=False)\n",
       "          (conv1d): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(1,), groups=768)\n",
       "          (x_proj): Linear(in_features=768, out_features=56, bias=False)\n",
       "          (dt_proj): Linear(in_features=24, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=384, bias=False)\n",
       "        )\n",
       "        (norm): RMSNorm()\n",
       "      )\n",
       "      (2): ResidualBlock(\n",
       "        (mixer): MambaBlock(\n",
       "          (in_proj): Linear(in_features=384, out_features=1536, bias=False)\n",
       "          (conv1d): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(1,), groups=768)\n",
       "          (x_proj): Linear(in_features=768, out_features=56, bias=False)\n",
       "          (dt_proj): Linear(in_features=24, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=384, bias=False)\n",
       "        )\n",
       "        (norm): RMSNorm()\n",
       "      )\n",
       "      (3): ResidualBlock(\n",
       "        (mixer): MambaBlock(\n",
       "          (in_proj): Linear(in_features=384, out_features=1536, bias=False)\n",
       "          (conv1d): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(1,), groups=768)\n",
       "          (x_proj): Linear(in_features=768, out_features=56, bias=False)\n",
       "          (dt_proj): Linear(in_features=24, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=384, bias=False)\n",
       "        )\n",
       "        (norm): RMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (interactions): ModuleList(\n",
       "      (0): InteractionBlock(\n",
       "        (injector): Injector(\n",
       "          (query_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (feat_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (extractor): Extractor(\n",
       "          (query_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (feat_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
       "          )\n",
       "          (ffn): ConvFFN(\n",
       "            (fc1): Linear(in_features=384, out_features=96, bias=True)\n",
       "            (dwconv): DWConv(\n",
       "              (dwconv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)\n",
       "            )\n",
       "            (act): GELU(approximate=none)\n",
       "            (fc2): Linear(in_features=96, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ffn_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (drop_path): DropPath(drop_prob=0.100)\n",
       "        )\n",
       "      )\n",
       "      (1): InteractionBlock(\n",
       "        (injector): Injector(\n",
       "          (query_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (feat_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (extractor): Extractor(\n",
       "          (query_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (feat_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
       "          )\n",
       "          (ffn): ConvFFN(\n",
       "            (fc1): Linear(in_features=384, out_features=96, bias=True)\n",
       "            (dwconv): DWConv(\n",
       "              (dwconv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)\n",
       "            )\n",
       "            (act): GELU(approximate=none)\n",
       "            (fc2): Linear(in_features=96, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ffn_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (drop_path): DropPath(drop_prob=0.100)\n",
       "        )\n",
       "      )\n",
       "      (2): InteractionBlock(\n",
       "        (injector): Injector(\n",
       "          (query_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (feat_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (extractor): Extractor(\n",
       "          (query_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (feat_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
       "          )\n",
       "          (ffn): ConvFFN(\n",
       "            (fc1): Linear(in_features=384, out_features=96, bias=True)\n",
       "            (dwconv): DWConv(\n",
       "              (dwconv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)\n",
       "            )\n",
       "            (act): GELU(approximate=none)\n",
       "            (fc2): Linear(in_features=96, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ffn_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (drop_path): DropPath(drop_prob=0.100)\n",
       "        )\n",
       "      )\n",
       "      (3): InteractionBlock(\n",
       "        (injector): Injector(\n",
       "          (query_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (feat_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (extractor): Extractor(\n",
       "          (query_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (feat_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
       "          )\n",
       "          (ffn): ConvFFN(\n",
       "            (fc1): Linear(in_features=384, out_features=96, bias=True)\n",
       "            (dwconv): DWConv(\n",
       "              (dwconv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)\n",
       "            )\n",
       "            (act): GELU(approximate=none)\n",
       "            (fc2): Linear(in_features=96, out_features=384, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (ffn_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "          (drop_path): DropPath(drop_prob=0.100)\n",
       "        )\n",
       "        (extra_extractors): Sequential(\n",
       "          (0): Extractor(\n",
       "            (query_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "            (feat_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
       "            )\n",
       "            (ffn): ConvFFN(\n",
       "              (fc1): Linear(in_features=384, out_features=96, bias=True)\n",
       "              (dwconv): DWConv(\n",
       "                (dwconv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)\n",
       "              )\n",
       "              (act): GELU(approximate=none)\n",
       "              (fc2): Linear(in_features=96, out_features=384, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ffn_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "            (drop_path): DropPath(drop_prob=0.100)\n",
       "          )\n",
       "          (1): Extractor(\n",
       "            (query_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "            (feat_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "            (attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
       "            )\n",
       "            (ffn): ConvFFN(\n",
       "              (fc1): Linear(in_features=384, out_features=96, bias=True)\n",
       "              (dwconv): DWConv(\n",
       "                (dwconv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96)\n",
       "              )\n",
       "              (act): GELU(approximate=none)\n",
       "              (fc2): Linear(in_features=96, out_features=384, bias=True)\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ffn_norm): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "            (drop_path): DropPath(drop_prob=0.100)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): CenterPredictor(\n",
       "    (conv1_ctr): Sequential(\n",
       "      (0): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "    (conv2_ctr): Sequential(\n",
       "      (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "    (conv3_ctr): Sequential(\n",
       "      (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "    (conv4_ctr): Sequential(\n",
       "      (0): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "    (conv5_ctr): Conv2d(32, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (conv1_offset): Sequential(\n",
       "      (0): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "    (conv2_offset): Sequential(\n",
       "      (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "    (conv3_offset): Sequential(\n",
       "      (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "    (conv4_offset): Sequential(\n",
       "      (0): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "    (conv5_offset): Conv2d(32, 2, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (conv1_size): Sequential(\n",
       "      (0): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "    (conv2_size): Sequential(\n",
       "      (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "    (conv3_size): Sequential(\n",
       "      (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "    (conv4_size): Sequential(\n",
       "      (0): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "    )\n",
       "    (conv5_size): Conv2d(32, 2, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network = build_mcitrack(params.cfg)\n",
    "network.load_state_dict(torch.load(\"MCITRACK_ep0300.pth.tar\", map_location='cpu')['net'], strict=True)\n",
    "cfg = params.cfg\n",
    "network = network.cuda()\n",
    "network.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "64d5c940",
   "metadata": {},
   "outputs": [],
   "source": [
    "list1 = [torch.zeros(1, 3, 112, 112).to('cuda') for _ in range(5)]  # 5 тензоров размером [1, 3, 112, 112]\n",
    "list2 = [torch.zeros(1, 3, 224, 224).to('cuda')]                    # 1 тензор размером [1, 3, 224, 224]\n",
    "list3 = [torch.zeros(1, 4).to('cuda') for _ in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fc77a871",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = network.forward(list1,list2,list3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7855da66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred_boxes\n",
      "torch.Size([1, 1, 4])\n",
      "score_map\n",
      "torch.Size([1, 1, 14, 14])\n",
      "size_map\n",
      "torch.Size([1, 2, 14, 14])\n",
      "offset_map\n",
      "torch.Size([1, 2, 14, 14])\n"
     ]
    }
   ],
   "source": [
    "for i in  res:\n",
    "    print(i)\n",
    "    print(res[i].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0aa6b8c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_13268\\1057810194.py:196: TracerWarning: Converting a tensor to a Python integer might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  x = x.transpose(1,2).view(B,C,int(N**0.5),int(N**0.5)).contiguous()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.4461, 0.3130, 0.9952, 0.4058]]], device='cuda:0')\n",
      "tensor([[[[0.0017, 0.0058, 0.0073, 0.0080, 0.0086, 0.0088, 0.0089, 0.0090,\n",
      "           0.0089, 0.0086, 0.0083, 0.0089, 0.0104, 0.0176],\n",
      "          [0.0053, 0.1473, 0.1681, 0.1671, 0.1688, 0.1728, 0.1756, 0.1772,\n",
      "           0.1769, 0.1739, 0.1736, 0.1831, 0.1925, 0.1635],\n",
      "          [0.0086, 0.1550, 0.1743, 0.1716, 0.1702, 0.1735, 0.1762, 0.1775,\n",
      "           0.1770, 0.1765, 0.1781, 0.1818, 0.1933, 0.1698],\n",
      "          [0.0085, 0.1391, 0.1579, 0.1513, 0.1500, 0.1527, 0.1550, 0.1560,\n",
      "           0.1558, 0.1567, 0.1605, 0.1668, 0.1756, 0.1507],\n",
      "          [0.0082, 0.1351, 0.1519, 0.1452, 0.1442, 0.1469, 0.1492, 0.1499,\n",
      "           0.1494, 0.1489, 0.1511, 0.1575, 0.1682, 0.1403],\n",
      "          [0.0080, 0.1352, 0.1528, 0.1464, 0.1455, 0.1481, 0.1509, 0.1515,\n",
      "           0.1498, 0.1488, 0.1502, 0.1557, 0.1654, 0.1376],\n",
      "          [0.0079, 0.1332, 0.1510, 0.1456, 0.1448, 0.1467, 0.1490, 0.1493,\n",
      "           0.1477, 0.1469, 0.1484, 0.1538, 0.1632, 0.1356],\n",
      "          [0.0077, 0.1280, 0.1447, 0.1405, 0.1400, 0.1413, 0.1432, 0.1438,\n",
      "           0.1427, 0.1419, 0.1428, 0.1468, 0.1539, 0.1274],\n",
      "          [0.0073, 0.1221, 0.1370, 0.1338, 0.1339, 0.1351, 0.1371, 0.1378,\n",
      "           0.1369, 0.1360, 0.1368, 0.1406, 0.1466, 0.1227],\n",
      "          [0.0069, 0.1195, 0.1336, 0.1306, 0.1308, 0.1317, 0.1335, 0.1343,\n",
      "           0.1333, 0.1325, 0.1332, 0.1373, 0.1429, 0.1202],\n",
      "          [0.0068, 0.1216, 0.1366, 0.1333, 0.1330, 0.1332, 0.1345, 0.1352,\n",
      "           0.1340, 0.1334, 0.1341, 0.1379, 0.1421, 0.1183],\n",
      "          [0.0067, 0.1217, 0.1374, 0.1351, 0.1355, 0.1358, 0.1370, 0.1375,\n",
      "           0.1361, 0.1352, 0.1356, 0.1382, 0.1399, 0.1154],\n",
      "          [0.0091, 0.1172, 0.1338, 0.1337, 0.1356, 0.1373, 0.1389, 0.1392,\n",
      "           0.1378, 0.1367, 0.1362, 0.1371, 0.1359, 0.1133],\n",
      "          [0.0164, 0.1050, 0.1129, 0.1107, 0.1124, 0.1151, 0.1168, 0.1171,\n",
      "           0.1162, 0.1151, 0.1134, 0.1148, 0.1121, 0.0996]]]], device='cuda:0')\n",
      "tensor([[[[0.5772, 0.7406, 0.8264, 0.8509, 0.8436, 0.8428, 0.8338, 0.8240,\n",
      "           0.8404, 0.8260, 0.8007, 0.7944, 0.7862, 0.6766],\n",
      "          [0.8556, 0.9722, 0.9902, 0.9935, 0.9925, 0.9928, 0.9923, 0.9914,\n",
      "           0.9924, 0.9917, 0.9900, 0.9903, 0.9891, 0.9580],\n",
      "          [0.8796, 0.9757, 0.9934, 0.9955, 0.9947, 0.9953, 0.9945, 0.9938,\n",
      "           0.9946, 0.9935, 0.9930, 0.9950, 0.9952, 0.9801],\n",
      "          [0.8881, 0.9768, 0.9936, 0.9954, 0.9932, 0.9935, 0.9921, 0.9916,\n",
      "           0.9927, 0.9918, 0.9924, 0.9957, 0.9961, 0.9838],\n",
      "          [0.8786, 0.9737, 0.9917, 0.9936, 0.9894, 0.9891, 0.9866, 0.9858,\n",
      "           0.9873, 0.9862, 0.9890, 0.9947, 0.9953, 0.9815],\n",
      "          [0.8642, 0.9708, 0.9896, 0.9913, 0.9850, 0.9844, 0.9808, 0.9794,\n",
      "           0.9812, 0.9806, 0.9864, 0.9937, 0.9943, 0.9774],\n",
      "          [0.8364, 0.9670, 0.9877, 0.9890, 0.9822, 0.9818, 0.9774, 0.9751,\n",
      "           0.9776, 0.9785, 0.9858, 0.9928, 0.9928, 0.9713],\n",
      "          [0.7819, 0.9578, 0.9838, 0.9845, 0.9780, 0.9786, 0.9738, 0.9711,\n",
      "           0.9748, 0.9780, 0.9845, 0.9909, 0.9898, 0.9606],\n",
      "          [0.6982, 0.9376, 0.9748, 0.9792, 0.9695, 0.9722, 0.9682, 0.9662,\n",
      "           0.9710, 0.9768, 0.9807, 0.9865, 0.9831, 0.9437],\n",
      "          [0.6061, 0.9075, 0.9574, 0.9710, 0.9646, 0.9667, 0.9633, 0.9632,\n",
      "           0.9705, 0.9749, 0.9744, 0.9787, 0.9736, 0.9224],\n",
      "          [0.5335, 0.8727, 0.9323, 0.9575, 0.9593, 0.9642, 0.9629, 0.9628,\n",
      "           0.9692, 0.9700, 0.9656, 0.9692, 0.9614, 0.8952],\n",
      "          [0.5075, 0.8294, 0.9001, 0.9357, 0.9443, 0.9564, 0.9578, 0.9566,\n",
      "           0.9620, 0.9615, 0.9548, 0.9541, 0.9401, 0.8592],\n",
      "          [0.4638, 0.7358, 0.8377, 0.8811, 0.8991, 0.9208, 0.9274, 0.9279,\n",
      "           0.9341, 0.9315, 0.9220, 0.9196, 0.8998, 0.7949],\n",
      "          [0.4239, 0.5514, 0.6899, 0.7401, 0.7681, 0.8028, 0.8166, 0.8201,\n",
      "           0.8273, 0.8224, 0.8108, 0.8138, 0.8006, 0.6552]],\n",
      "\n",
      "         [[0.5403, 0.7133, 0.7545, 0.8550, 0.9198, 0.9468, 0.9553, 0.9566,\n",
      "           0.9520, 0.9425, 0.9250, 0.8953, 0.8641, 0.7589],\n",
      "          [0.4770, 0.5112, 0.4855, 0.5106, 0.5221, 0.5248, 0.5225, 0.5153,\n",
      "           0.4967, 0.4639, 0.4323, 0.4199, 0.5022, 0.5225],\n",
      "          [0.4515, 0.4599, 0.4315, 0.4530, 0.4570, 0.4578, 0.4543, 0.4501,\n",
      "           0.4396, 0.4196, 0.3939, 0.3866, 0.4058, 0.4516],\n",
      "          [0.4431, 0.4630, 0.4531, 0.4769, 0.4809, 0.4861, 0.4873, 0.4886,\n",
      "           0.4812, 0.4628, 0.4398, 0.4305, 0.4417, 0.5133],\n",
      "          [0.4502, 0.4817, 0.4714, 0.4978, 0.5062, 0.5137, 0.5170, 0.5188,\n",
      "           0.5099, 0.4939, 0.4819, 0.4762, 0.4846, 0.5683],\n",
      "          [0.4538, 0.4902, 0.4754, 0.4968, 0.5070, 0.5212, 0.5361, 0.5396,\n",
      "           0.5293, 0.5119, 0.5190, 0.5096, 0.5096, 0.5944],\n",
      "          [0.4500, 0.4933, 0.4745, 0.4826, 0.4918, 0.4958, 0.5027, 0.5058,\n",
      "           0.5026, 0.5001, 0.5167, 0.5062, 0.5086, 0.5979],\n",
      "          [0.4513, 0.5125, 0.4998, 0.4907, 0.4974, 0.4977, 0.4959, 0.4972,\n",
      "           0.4983, 0.5146, 0.5276, 0.5150, 0.5643, 0.6431],\n",
      "          [0.4539, 0.5402, 0.5392, 0.5472, 0.5424, 0.5387, 0.5308, 0.5335,\n",
      "           0.5444, 0.5546, 0.5568, 0.5391, 0.6567, 0.7045],\n",
      "          [0.4560, 0.5604, 0.5590, 0.5941, 0.5929, 0.5923, 0.5873, 0.5851,\n",
      "           0.5900, 0.5966, 0.5787, 0.5764, 0.7290, 0.7411],\n",
      "          [0.4573, 0.5684, 0.5588, 0.5951, 0.6125, 0.6088, 0.6071, 0.6107,\n",
      "           0.6131, 0.6010, 0.5805, 0.6401, 0.7647, 0.7495],\n",
      "          [0.4498, 0.5639, 0.5458, 0.5781, 0.5963, 0.6049, 0.6060, 0.6052,\n",
      "           0.6038, 0.5926, 0.5804, 0.6327, 0.7461, 0.7273],\n",
      "          [0.4266, 0.5143, 0.4937, 0.5227, 0.5430, 0.5574, 0.5647, 0.5686,\n",
      "           0.5673, 0.5599, 0.5558, 0.5556, 0.6416, 0.6221],\n",
      "          [0.3911, 0.4213, 0.4495, 0.4765, 0.4973, 0.5182, 0.5319, 0.5386,\n",
      "           0.5363, 0.5243, 0.5120, 0.5016, 0.5028, 0.4577]]]], device='cuda:0')\n",
      "tensor([[[[ 2.6477,  3.8590,  3.6015,  3.2603,  2.8152,  2.1030,  1.2728,\n",
      "            0.4141, -0.2902, -1.1164, -1.8373, -2.5343, -3.3442, -3.6955],\n",
      "          [ 4.5145,  6.2701,  5.6004,  4.8984,  3.8277,  2.6016,  1.5155,\n",
      "            0.4119, -0.8290, -1.8641, -2.8882, -4.0113, -5.3161, -5.7375],\n",
      "          [ 5.1948,  7.2971,  6.2730,  5.2151,  3.7309,  2.6488,  1.5413,\n",
      "            0.3792, -0.8651, -2.0143, -3.0748, -4.4070, -5.7541, -6.5775],\n",
      "          [ 5.4904,  7.6469,  6.4074,  5.1050,  3.6769,  2.5773,  1.4220,\n",
      "            0.3153, -0.9023, -1.9799, -3.0963, -4.5139, -5.8887, -7.0245],\n",
      "          [ 5.4692,  7.6354,  6.3143,  4.9661,  3.6156,  2.4646,  1.3703,\n",
      "            0.2657, -0.8914, -1.8881, -3.0710, -4.5199, -5.9502, -7.1743],\n",
      "          [ 5.3061,  7.5959,  6.1839,  4.9471,  3.5843,  2.4681,  1.3657,\n",
      "            0.2674, -0.8755, -1.8498, -3.0408, -4.5046, -6.0851, -7.2041],\n",
      "          [ 5.0035,  7.5344,  6.0351,  5.0152,  3.6178,  2.5543,  1.4100,\n",
      "            0.3082, -0.8766, -1.8630, -3.0591, -4.5035, -6.3421, -7.1495],\n",
      "          [ 4.5456,  7.3853,  5.9717,  5.0491,  3.7038,  2.6570,  1.5455,\n",
      "            0.3146, -0.9218, -1.9767, -3.1718, -4.4916, -6.6598, -6.9870],\n",
      "          [ 3.9110,  7.1355,  5.9099,  4.9344,  3.9417,  2.7460,  1.6966,\n",
      "            0.4421, -0.8593, -2.0619, -3.2496, -4.6400, -6.9079, -6.6631],\n",
      "          [ 3.2743,  6.8064,  5.7237,  4.7154,  3.9982,  3.0016,  1.7944,\n",
      "            0.5946, -0.8563, -2.2247, -3.3126, -5.1365, -6.9138, -6.1693],\n",
      "          [ 2.7775,  6.4134,  5.3730,  4.6266,  3.8595,  3.1063,  1.9435,\n",
      "            0.6815, -0.9165, -2.3988, -3.8112, -5.3702, -6.5979, -5.5238],\n",
      "          [ 2.3747,  5.9327,  4.8182,  4.2081,  3.7506,  2.9382,  1.8308,\n",
      "            0.4919, -1.3867, -2.8919, -4.1545, -5.2075, -5.9150, -4.7464],\n",
      "          [ 1.8608,  5.3407,  3.9838,  3.0035,  2.7692,  2.2021,  1.1627,\n",
      "           -0.1140, -1.6031, -2.7990, -3.7415, -4.3948, -4.7976, -3.6896],\n",
      "          [ 1.0300,  3.5684,  2.1992,  1.4021,  1.2173,  0.8937,  0.1999,\n",
      "           -0.5646, -1.4621, -2.2198, -2.7223, -3.0449, -3.2588, -2.5169]],\n",
      "\n",
      "         [[ 1.3714,  3.2164,  4.1694,  4.8189,  4.8744,  4.7216,  4.4495,\n",
      "            4.1752,  3.9629,  3.7440,  3.6123,  3.5577,  3.3610,  2.5032],\n",
      "          [ 1.2012,  3.0553,  3.8627,  4.1451,  3.9062,  3.5052,  3.1473,\n",
      "            2.9394,  2.8346,  2.7299,  2.7982,  3.0550,  3.1764,  2.6569],\n",
      "          [ 0.3632,  2.4401,  2.9668,  2.9124,  2.6019,  2.2961,  2.2158,\n",
      "            2.2006,  2.1212,  1.9517,  1.8616,  2.1443,  2.3814,  2.1210],\n",
      "          [-0.3705,  1.2753,  1.8190,  1.5693,  1.4009,  1.5805,  1.6386,\n",
      "            1.6038,  1.5269,  1.3637,  1.1836,  1.1149,  1.2443,  1.2316],\n",
      "          [-0.8818,  0.3892,  0.8494,  0.4675,  0.6299,  0.8130,  0.8642,\n",
      "            0.8594,  0.8225,  0.7074,  0.5155,  0.2238,  0.3170,  0.4903],\n",
      "          [-1.3738, -0.4465, -0.3466, -0.5960, -0.4785, -0.2931, -0.2086,\n",
      "           -0.1643, -0.1657, -0.1430, -0.2237, -0.4613, -0.4720, -0.1199],\n",
      "          [-1.9436, -1.1724, -1.1888, -1.1398, -1.1092, -0.9365, -0.7822,\n",
      "           -0.7450, -0.7102, -0.6756, -0.7971, -0.9703, -1.1024, -0.6135],\n",
      "          [-2.5478, -1.8642, -2.0427, -1.8767, -1.7315, -1.5495, -1.3448,\n",
      "           -1.2418, -1.1648, -1.2127, -1.3026, -1.4295, -1.7204, -1.0922],\n",
      "          [-3.0548, -2.4201, -2.7243, -2.6761, -2.3661, -2.1779, -2.0254,\n",
      "           -1.9432, -1.8876, -1.8581, -1.9245, -2.1703, -2.3564, -1.5216],\n",
      "          [-3.4085, -2.7509, -3.1350, -3.2525, -3.1377, -2.8497, -2.7273,\n",
      "           -2.6953, -2.6504, -2.5421, -2.6329, -2.8854, -2.8988, -1.8126],\n",
      "          [-3.6605, -3.0102, -3.4246, -3.6681, -3.6680, -3.6033, -3.5451,\n",
      "           -3.4694, -3.4402, -3.3713, -3.4119, -3.4391, -3.2386, -1.9860],\n",
      "          [-3.8646, -3.2918, -3.6832, -4.0276, -4.1695, -4.1732, -4.2672,\n",
      "           -4.2885, -4.3122, -4.1894, -4.0294, -3.8175, -3.5095, -2.1621],\n",
      "          [-4.0414, -3.7622, -4.1161, -4.4329, -4.6733, -4.8532, -5.0060,\n",
      "           -5.0318, -5.0125, -4.8268, -4.5614, -4.2337, -3.8421, -2.4218],\n",
      "          [-3.1794, -3.1103, -3.4968, -3.7182, -3.9248, -4.1256, -4.2491,\n",
      "           -4.2716, -4.2203, -4.0576, -3.8290, -3.4969, -3.1718, -2.1321]]]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "class ModelWrapper(torch.nn.Module):\n",
    "    def __init__(self, original_model):\n",
    "        super(ModelWrapper, self).__init__()\n",
    "        self.original_model = original_model\n",
    "\n",
    "    def forward(self, template_list, search_list,template_anno_list):\n",
    "        \n",
    "        output_dict = self.original_model(template_list, search_list,template_anno_list)\n",
    "        \n",
    "        \n",
    "        return (output_dict['pred_boxes'],\n",
    "                output_dict['score_map'],\n",
    "                output_dict['size_map'],\n",
    "                output_dict['offset_map'])\n",
    "\n",
    "\n",
    "model = network\n",
    "model.eval()\n",
    "\n",
    "\n",
    "wrapped_model = ModelWrapper(model)\n",
    "\n",
    "\n",
    "template_list = [torch.zeros(1, 3, 112, 112).to('cuda') for _ in range(5)]  # 5 тензоров размером [1, 3, 112, 112]\n",
    "search_list = [torch.zeros(1, 3, 224, 224).to('cuda')]                    # 1 тензор размером [1, 3, 224, 224]\n",
    "template_anno_list = [torch.zeros(1, 4).to('cuda') for _ in range(5)]\n",
    "\n",
    "traced_model = torch.jit.trace(wrapped_model, (template_list, search_list,template_anno_list))\n",
    "\n",
    "\n",
    "optimized_model = torch.jit.optimize_for_inference(traced_model)\n",
    "\n",
    "\n",
    "optimized_model.save(\"MCITrack.pt\")\n",
    "\n",
    "\n",
    "loaded_model = torch.jit.load(\"MCITrack.pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = loaded_model(template_list, search_list,template_anno_list)\n",
    "\n",
    "for output in outputs:\n",
    "    print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99787837",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "class ModelWrapper(torch.nn.Module):\n",
    "    def __init__(self, original_model):\n",
    "        super(ModelWrapper, self).__init__()\n",
    "        self.original_model = original_model\n",
    "\n",
    "    def forward(self, template_list, search_list,template_anno_list):\n",
    "        \n",
    "        output_dict = self.original_model(template_list, search_list,template_anno_list)\n",
    "        \n",
    "        \n",
    "        return (output_dict['pred_boxes'],\n",
    "                output_dict['score_map'],\n",
    "                output_dict['size_map'],\n",
    "                output_dict['offset_map'])\n",
    "\n",
    "\n",
    "\n",
    "model = network\n",
    "model.eval()\n",
    "\n",
    "\n",
    "wrapped_model = ModelWrapper(model)\n",
    "\n",
    "template_list = [torch.zeros(1, 3, 112, 112).to('cuda') for _ in range(5)]  # 5 тензоров размером [1, 3, 112, 112]\n",
    "search_list = [torch.zeros(1, 3, 224, 224).to('cuda')]                    # 1 тензор размером [1, 3, 224, 224]\n",
    "template_anno_list = [torch.zeros(1, 4).to('cuda') for _ in range(5)]\n",
    "\n",
    "# Важно: для onnx-модели модель должна быть на cpu или cuda, и входы должны быть на том же устройстве.\n",
    "wrapped_model = wrapped_model.to('cuda')\n",
    "wrapped_model.eval()\n",
    "\n",
    "# Указываем пути для сохранения\n",
    "onnx_path = \"MCITrac.onnx\"\n",
    "\n",
    "# Экспортируем модель в ONNX\n",
    "torch.onnx.export(\n",
    "    wrapped_model,                                   # Модель\n",
    "    (template_list, search_list,template_anno_list),                                # Входные данные (tuple)\n",
    "    onnx_path,                                       # Имя файла\n",
    "    export_params=True,                              # Экспортировать параметры (веса)\n",
    "    opset_version=16,                                # Версия ONNX opset\n",
    "    do_constant_folding=True,                        # Оптимизация констант\n",
    "    input_names = ['template_list', 'search_list','template_anno_list'],                        # Имена входов\n",
    "    output_names = ['pred_boxes','score_map','size_map','offset_map'],                   # Имена выходов\n",
    "    #dynamic_axes={'z': {0: 'batch_size'},            # Динамическая ось для батча\n",
    "    #              'x': {0: 'batch_size'},\n",
    "    #              'pred_boxes': {0: 'batch_size'}},\n",
    "    verbose=True                                     # Показывать подробности\n",
    ")\n",
    "\n",
    "print(f'Model has been exported to {onnx_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1666b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trtexec --onnx=MCITrac.onnx  --saveEngine=MCITrac.trt  --fp16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196c2084",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Throughput: 61.4365 qps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca95d414",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime as ort\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1ae7ecd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can't read frame\n"
     ]
    }
   ],
   "source": [
    "\n",
    "file = \"12-машина_поле_ТВ.mp4\"\n",
    "video = cv2.VideoCapture(file)\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "fps=video.get(cv2.CAP_PROP_FPS)\n",
    "video_vriter = cv2.VideoWriter(file.split('.')[0]+\"_\"+\".avi\", fourcc, fps, (1920, 1080))\n",
    "\n",
    "\n",
    "ok, image = video.read()\n",
    "if not video.isOpened():\n",
    "    print(\"Could not open video\")\n",
    "    sys.exit()\n",
    "    \n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "x, y, w, h = cv2.selectROI( image, fromCenter=False)\n",
    "init_state = [x, y, w, h]\n",
    "def _build_init_info(box):\n",
    "            return {'init_bbox': box}\n",
    "treacker.initialize(image, _build_init_info(init_state))\n",
    "counter = 0\n",
    "while True:\n",
    "            ok, image = video.read()\n",
    "            if not ok:\n",
    "                print(\"Can't read frame\")\n",
    "                break\n",
    "\n",
    "            \n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            start = time.time() \n",
    "            out  = treacker.track(image)\n",
    "            state = [int(s) for s in out['target_bbox']]\n",
    "            best_score=out[\"best_score\"].cpu().numpy()[0][0]\n",
    "            end_time = (time.time() - start)\n",
    "            \n",
    "            \n",
    "            org = (50, 50)\n",
    "\n",
    "            # fontScale\n",
    "            fontScale = 1\n",
    "            font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "            # Blue color in BGR\n",
    "            color = (255, 0, 0)\n",
    "            # Line thickness of 2 px\n",
    "            thickness = 2              \n",
    "            # Using cv2.putText() method\n",
    "            image = cv2.putText(image, str(best_score), org, font, \n",
    "                            fontScale, color, thickness, cv2.LINE_AA)\n",
    "            image = cv2.putText(image, str(end_time), (50,100), font, \n",
    "                            fontScale, color, thickness, cv2.LINE_AA)\n",
    "\n",
    "            x, y, w, h = [int(x) for x in state]\n",
    "\n",
    "            color = (200, 0, 0)  # Цвет в формате BGR\n",
    "            cv2.rectangle(image, (x, y), (x + w, y + h), color, 2)\n",
    "\n",
    "\n",
    "            cv2.imshow(\"tracking\", image)\n",
    "            video_vriter.write(image)\n",
    "\n",
    "\n",
    "            k = cv2.waitKey(1)            \n",
    "            if k == 32:  # SPACE\n",
    "                ok, image = video.read()                             \n",
    "                x, y, w, h = cv2.selectROI( image, fromCenter=False)\n",
    "                init_state = [x, y, w, h]\n",
    "                treacker.initialize(image, _build_init_info(init_state))\n",
    "            if k == 27:  # ESC\n",
    "                break\n",
    "        \n",
    "                \n",
    "                \n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "video.release()\n",
    "video_vriter.release()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e8df45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1d032c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Метрики\n",
    "import numpy as np\n",
    "\n",
    "def iou(boxA, boxB):\n",
    "    # boxA, boxB: [x, y, w, h]\n",
    "    xA = max(boxA[0], boxB[0])\n",
    "    yA = max(boxA[1], boxB[1])\n",
    "    xB = min(boxA[0] + boxA[2], boxB[0] + boxB[2])\n",
    "    yB = min(boxA[1] + boxA[3], boxB[1] + boxB[3])\n",
    "\n",
    "    interW = max(0, xB - xA)\n",
    "    interH = max(0, yB - yA)\n",
    "    interArea = interW * interH\n",
    "\n",
    "    boxAArea = boxA[2] * boxA[3]\n",
    "    boxBArea = boxB[2] * boxB[3]\n",
    "    unionArea = boxAArea + boxBArea - interArea\n",
    "\n",
    "    if unionArea == 0:\n",
    "        return 0.0\n",
    "    return interArea / unionArea\n",
    "\n",
    "def precision(boxA, boxB):\n",
    "    # центры bbox\n",
    "    centerA = (boxA[0] + boxA[2]/2, boxA[1] + boxA[3]/2)\n",
    "    centerB = (boxB[0] + boxB[2]/2, boxB[1] + boxB[3]/2)\n",
    "    dist = np.sqrt((centerA[0] - centerB[0])**2 + (centerA[1] - centerB[1])**2)\n",
    "    return dist\n",
    "\n",
    "# Пример для одного кадра\n",
    "gt_bbox = [50, 50, 100, 100]      # ground truth bbox\n",
    "pred_bbox = [60, 60, 80, 80]      # predicted bbox\n",
    "\n",
    "iou_val = iou(gt_bbox, pred_bbox)\n",
    "print(f'IoU: {iou_val:.2f}')\n",
    "\n",
    "# Success Rate при пороге 0.5\n",
    "sr_thresh = 0.5\n",
    "sr = 1 if iou_val >= sr_thresh else 0\n",
    "print(f'Success Rate @0.5: {sr}')\n",
    "\n",
    "# Precision при пороге 20 пикселей\n",
    "prec_thresh = 20\n",
    "prec_dist = precision(gt_bbox, pred_bbox)\n",
    "prec = 1 if prec_dist <= prec_thresh else 0\n",
    "print(f'Precision @20px: {prec}')\n",
    "\n",
    "# --- Для серии кадров (пример) ---\n",
    "\n",
    "gt_bboxes = [\n",
    "    [50, 50, 100, 100],\n",
    "    [60, 60, 100, 100]\n",
    "]\n",
    "pred_bboxes = [\n",
    "    [60, 60, 80, 80],\n",
    "    [65, 65, 90, 90]\n",
    "]\n",
    "\n",
    "ious = [iou(gt, pred) for gt, pred in zip(gt_bboxes, pred_bboxes)]\n",
    "ao = np.mean(ious)\n",
    "sr = np.mean([1 if val >= sr_thresh else 0 for val in ious])\n",
    "precisions = [precision(gt, pred) for gt, pred in zip(gt_bboxes, pred_bboxes)]\n",
    "prec = np.mean([1 if d <= prec_thresh else 0 for d in precisions])\n",
    "\n",
    "print(f'Average Overlap (AO): {ao:.2f}')\n",
    "print(f'Success Rate (SR@0.5): {sr:.2f}')\n",
    "print(f'Precision @20px: {prec:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fab2287",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b56fbba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5201b2e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0c4216",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b0fbe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video saved as: GOT-10k_Val_000001_output.avi\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import  os\n",
    "gt_bboxes = []\n",
    "pred_bboxes = []\n",
    "seq_path = \"val/GOT-10k_Val_000001\"\n",
    "txt_files = glob.glob(os.path.join(seq_path, '*.txt'))\n",
    "if not txt_files:\n",
    "    raise FileNotFoundError(f\"No .txt files found in {seq_path}\")\n",
    "\n",
    "img_files = sorted(glob.glob(os.path.join(seq_path, '*.jpg')))\n",
    "with open(txt_files[0], 'r') as f:\n",
    "    gt_bboxes = [list(map(float, line.strip().split(','))) for line in f]\n",
    "\n",
    "# Получаем размер первого изображения\n",
    "sample_img = cv2.imread(img_files[0])\n",
    "if sample_img is None:\n",
    "    raise ValueError(f\"Failed to read sample image: {img_files[0]}\")\n",
    "height, width = sample_img.shape[:2]\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "output_filename = f\"{seq_path.split('/')[-1]}_output.avi\"\n",
    "video_vriter = cv2.VideoWriter(output_filename, fourcc, 10, (width, height))  \n",
    "\n",
    "assert len(img_files) == len(gt_bboxes), \"Количество кадров и bbox'ов не совпадает\"\n",
    "\n",
    "x, y, w, h = map(int, gt_bboxes[0])\n",
    "init_state = [x, y, w, h]\n",
    "\n",
    "def _build_init_info(box):\n",
    "            return {'init_bbox': box}\n",
    "\n",
    "counter = 0\n",
    "\n",
    "for img_file, bbox in zip(img_files, gt_bboxes):\n",
    "        \n",
    "        # Читаем изображение\n",
    "        img = cv2.imread(img_file)\n",
    "        if img is None:\n",
    "            print(f\"Не удалось загрузить изображение: {img_file}\")\n",
    "            continue\n",
    "        \n",
    "        treacker.initialize(img, _build_init_info(init_state))\n",
    "        out  = treacker.track(img)\n",
    "        state = [int(s) for s in out['target_bbox']]   \n",
    "        org = (50, 50)\n",
    "\n",
    "        # fontScale\n",
    "        fontScale = 1\n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "        #color = (0, 0, 255)\n",
    "        # Line thickness of 2 px\n",
    "        thickness = 2              \n",
    "                \n",
    "        x, y, w, h = [int(x) for x in state]\n",
    "\n",
    "        color = (0, 0, 200)  # Цвет в формате BGR\n",
    "        cv2.rectangle(img, (x, y), (x + w, y + h), color, 2)\n",
    "        # Рисуем bounding box\n",
    "        x1, y1, w1, h1 = map(int, bbox)\n",
    "        cv2.rectangle(img, (x1, y1), (x1+w1, y1+h1), (0, 255, 0), 2)\n",
    "        bbox_pred = x, y, w, h\n",
    "        \n",
    "        gt_bboxes.append(bbox)\n",
    "        pred_bboxes.append(bbox_pred)\n",
    "\n",
    "        cv2.imshow(seq_path, img)\n",
    "        video_vriter.write(img)\n",
    "\n",
    "\n",
    "        # Выход по нажатию 'q' или ESC\n",
    "        key = cv2.waitKey(100) & 0xFF\n",
    "        if key == ord('q') or key == 27:\n",
    "            break\n",
    "        \n",
    "                \n",
    "                \n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "video.release()\n",
    "video_vriter.release()\n",
    "print(f\"Video saved as: {output_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0f852e52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Overlap (AO): 0.46\n",
      "Success Rate (SR@0.5): 0.33\n",
      "Precision @20px: 0.05\n"
     ]
    }
   ],
   "source": [
    "ious = [iou(gt, pred) for gt, pred in zip(gt_bboxes, pred_bboxes)]\n",
    "ao = np.mean(ious)\n",
    "sr = np.mean([1 if val >= sr_thresh else 0 for val in ious])\n",
    "precisions = [precision(gt, pred) for gt, pred in zip(gt_bboxes, pred_bboxes)]\n",
    "prec = np.mean([1 if d <= prec_thresh else 0 for d in precisions])\n",
    "\n",
    "print(f'Average Overlap (AO): {ao:.2f}')\n",
    "print(f'Success Rate (SR@0.5): {sr:.2f}')\n",
    "print(f'Precision @20px: {prec:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1455645",
   "metadata": {},
   "outputs": [],
   "source": [
    "GOT-10k_Val_000001\n",
    "Average Overlap (AO): 0.46\n",
    "Success Rate (SR@0.5): 0.33\n",
    "Precision @20px: 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e49827",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7941bdb2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3_10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
