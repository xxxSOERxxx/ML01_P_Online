{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7bb640b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "429c4397",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\timm\\models\\layers\\__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import yaml\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils import checkpoint\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import cv2\n",
    "import sys\n",
    "import numpy as np\n",
    "import fastitpn as fastitpn_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4bde567b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBase(nn.Module):\n",
    "\n",
    "    def __init__(self, encoder: nn.Module, train_encoder: bool, open_layers: list, num_channels: int):\n",
    "        super().__init__()\n",
    "        open_blocks = open_layers[2:]\n",
    "        open_items = open_layers[0:2]\n",
    "        for name, parameter in encoder.named_parameters():\n",
    "\n",
    "            if not train_encoder:\n",
    "                freeze = True\n",
    "                for open_block in open_blocks:\n",
    "                    if open_block in name:\n",
    "                        freeze = False\n",
    "                if name in open_items:\n",
    "                    freeze = False\n",
    "                if freeze == True:\n",
    "                    parameter.requires_grad_(False)  # here should allow users to specify which layers to freeze !\n",
    "\n",
    "        self.body = encoder\n",
    "        self.num_channels = num_channels\n",
    "\n",
    "    def forward(self, template_list, search_list, template_anno_list):\n",
    "        xs = self.body(template_list, search_list, template_anno_list)\n",
    "        return xs\n",
    "\n",
    "\n",
    "#fast_itpn_tiny_1600e_1k\n",
    "\n",
    "class Encoder(EncoderBase):\n",
    "    \"\"\"FastITPN encoder.\"\"\"\n",
    "    def __init__(self, name: str,\n",
    "                 train_encoder: bool,\n",
    "                 pretrain_type: str,\n",
    "                 search_size: int,\n",
    "                 search_number: int,\n",
    "                 template_size: int,\n",
    "                 template_number: int,\n",
    "                 open_layers: list,\n",
    "                 cfg=None):\n",
    "        if \"fastitpn\" in name.lower():\n",
    "            encoder = getattr(fastitpn_module, name)(\n",
    "                pretrained=True,\n",
    "                search_size=search_size,\n",
    "                template_size=template_size,\n",
    "                drop_rate=0.0,\n",
    "                drop_path_rate=0.1,\n",
    "                attn_drop_rate=0.0,\n",
    "                init_values=0.1,\n",
    "                drop_block_rate=None,\n",
    "                use_mean_pooling=True,\n",
    "                grad_ckpt=cfg[\"MODEL\"][\"ENCODER\"][\"GRAD_CKPT\"],\n",
    "                pos_type=cfg[\"MODEL\"][\"ENCODER\"][\"POS_TYPE\"],\n",
    "                token_type_indicate=cfg[\"MODEL\"][\"ENCODER\"][\"TOKEN_TYPE_INDICATE\"],\n",
    "                pretrain_type = cfg[\"MODEL\"][\"ENCODER\"][\"PRETRAIN_TYPE\"],\n",
    "            )\n",
    "            if \"itpnb\" in name:\n",
    "                num_channels = 512\n",
    "            elif \"itpnl\" in name:\n",
    "                num_channels = 768\n",
    "            elif \"itpnt\" in name:\n",
    "                num_channels = 384\n",
    "            elif \"itpns\" in name:\n",
    "                num_channels = 384\n",
    "            else:\n",
    "                num_channels = 512\n",
    "        else:\n",
    "            raise ValueError()\n",
    "        super().__init__(encoder, train_encoder, open_layers, num_channels)\n",
    "\n",
    "def build_encoder(cfg):\n",
    "    train_encoder = (cfg[\"TRAIN\"][\"ENCODER_MULTIPLIER\"] > 0) and (cfg[\"TRAIN\"][\"FREEZE_ENCODER\"] == False)\n",
    "    encoder = Encoder(cfg[\"MODEL\"][\"ENCODER\"][\"TYPE\"], train_encoder,\n",
    "                      cfg[\"MODEL\"][\"ENCODER\"][\"PRETRAIN_TYPE\"],\n",
    "                      cfg[\"DATA\"][\"SEARCH\"][\"SIZE\"], cfg[\"DATA\"][\"SEARCH\"][\"NUMBER\"],\n",
    "                      cfg[\"DATA\"][\"TEMPLATE\"][\"SIZE\"], cfg[\"DATA\"][\"TEMPLATE\"][\"NUMBER\"],\n",
    "                      cfg[\"TRAIN\"][\"ENCODER_OPEN\"], cfg)\n",
    "    return encoder\n",
    "\n",
    "class MambaBlock(nn.Module):\n",
    "    def __init__(self,dt_scale, d_model,d_inner,dt_rank,d_state,bias,d_conv,conv_bias,dt_init,dt_max,dt_min,dt_init_floor):\n",
    "        super().__init__()\n",
    "        #  projects block input from D to 2*ED (two branches)\n",
    "        self.dt_scale = dt_scale\n",
    "        self.d_model = d_model\n",
    "        self.d_inner = d_inner\n",
    "        self.dt_rank = dt_rank\n",
    "        self.d_state = d_state\n",
    "        self.in_proj = nn.Linear(self.d_model, 2 * self.d_inner, bias=bias)\n",
    "\n",
    "        self.conv1d = nn.Conv1d(in_channels=self.d_inner, out_channels=self.d_inner,\n",
    "                                kernel_size=d_conv, bias=conv_bias,\n",
    "                                groups=self.d_inner,\n",
    "                                padding=(d_conv - 1)//2)\n",
    "\n",
    "        #  projects x to input-dependent Δ, B, C\n",
    "        self.x_proj = nn.Linear(self.d_inner, self.dt_rank + 2 * self.d_state, bias=False)\n",
    "\n",
    "        #  projects Δ from dt_rank to d_inner\n",
    "        self.dt_proj = nn.Linear(self.dt_rank, self.d_inner, bias=True)\n",
    "\n",
    "        #  dt initialization\n",
    "        #  dt weights\n",
    "        dt_init_std = self.dt_rank ** -0.5 * self.dt_scale\n",
    "        if dt_init == \"constant\":\n",
    "            nn.init.constant_(self.dt_proj.weight, dt_init_std)\n",
    "        elif dt_init == \"random\":\n",
    "            nn.init.uniform_(self.dt_proj.weight, -dt_init_std, dt_init_std)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        # dt bias\n",
    "        dt = torch.exp(\n",
    "            torch.rand(self.d_inner) * (math.log(dt_max) - math.log(dt_min)) + math.log(dt_min)\n",
    "        ).clamp(min=dt_init_floor)\n",
    "        inv_dt = dt + torch.log(\n",
    "            -torch.expm1(-dt))  #  inverse of softplus: https://github.com/pytorch/pytorch/issues/72759\n",
    "        with torch.no_grad():\n",
    "            self.dt_proj.bias.copy_(inv_dt)\n",
    "        # self.dt_proj.bias._no_reinit = True # initialization would set all Linear.bias to zero, need to mark this one as _no_reinit\n",
    "        #  todo : explain why removed\n",
    "\n",
    "        # S4D real initialization\n",
    "        A = torch.arange(1, self.d_state + 1, dtype=torch.float32).repeat(self.d_inner, 1)\n",
    "        self.A_log = nn.Parameter(\n",
    "            torch.log(A))  # why store A in log ? to keep A < 0 (cf -torch.exp(...)) ? for gradient stability ?\n",
    "        self.D = nn.Parameter(torch.ones(self.d_inner))\n",
    "\n",
    "        #  projects block output from ED back to D\n",
    "        self.out_proj = nn.Linear(self.d_inner, self.d_model, bias=bias)\n",
    "    def forward(self, x, h):\n",
    "        #  x : (B,L, D)\n",
    "        # h : (B,L, ED, N)\n",
    "\n",
    "        #  y : (B, L, D)\n",
    "\n",
    "\n",
    "        xz = self.in_proj(x)  # (B, L,2*ED)\n",
    "        x, z = xz.chunk(2, dim=-1)  #  (B,L, ED), (B,L, ED)\n",
    "        x_cache = x.permute(0,2,1)#(B, ED,L)\n",
    "\n",
    "        #  x branch\n",
    "        x = self.conv1d( x_cache).permute(0,2,1) #  (B,L , ED)\n",
    "\n",
    "        x = F.silu(x)\n",
    "        y, h = self.ssm_step(x, h)\n",
    "        #y->B,L,ED;h->B,L,ED,N\n",
    "\n",
    "        #  z branch\n",
    "        z = F.silu(z)\n",
    "\n",
    "        output = y * z\n",
    "        output = self.out_proj(output)  #  (B, L, D)\n",
    "\n",
    "        return output, h\n",
    "\n",
    "    def ssm_step(self, x, h):\n",
    "        #  x : (B, L, ED)\n",
    "        #  h : (B, L, ED, N)\n",
    "\n",
    "        A = -torch.exp(\n",
    "            self.A_log.float())  # (ED, N) # todo : ne pas le faire tout le temps, puisque c'est indépendant de la timestep\n",
    "        D = self.D.float()\n",
    "        #  TODO remove .float()\n",
    "\n",
    "        deltaBC = self.x_proj(x)  #  (B, L, dt_rank+2*N)\n",
    "\n",
    "        delta, B, C = torch.split(deltaBC, [self.dt_rank, self.d_state, self.d_state],\n",
    "                                  dim=-1)  #  (B, L,dt_rank), (B, L, N), (B, L, N)\n",
    "        delta = F.softplus(self.dt_proj(delta))  #  (B, L, ED)\n",
    "\n",
    "        deltaA = torch.exp(delta.unsqueeze(-1) * A)  #  (B,L, ED, N)\n",
    "        deltaB = delta.unsqueeze(-1) * B.unsqueeze(2)  #  (B,L, ED, N)\n",
    "\n",
    "        BX = deltaB * (x.unsqueeze(-1))  #  (B, L,ED, N)\n",
    "\n",
    "        if h is None:\n",
    "            h = torch.zeros(x.size(0), x.size(1), self.d_inner, self.d_state, device=deltaA.device)  #  (B, L, ED, N)\n",
    "\n",
    "        h = deltaA * h + BX  #  (B, L, ED, N)\n",
    "\n",
    "        y = (h @ C.unsqueeze(-1)).squeeze(3)  #  (B, L, ED, N) @ (B, L, N, 1) -> (B, L, ED, 1)\n",
    "\n",
    "        y = y + D * x#B,L,ED\n",
    "\n",
    "        #  todo : pq h.squeeze(1) ??\n",
    "        return y, h\n",
    "    \n",
    "class DWConv(nn.Module):\n",
    "    def __init__(self, dim=768):\n",
    "        super().__init__()\n",
    "        self.dwconv = nn.Conv2d(dim, dim, 3, 1, 1, bias=True, groups=dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(1,0,2)\n",
    "        B, N, C = x.shape\n",
    "        x = x.transpose(1,2).view(B,C,int(N**0.5),int(N**0.5)).contiguous()\n",
    "        x = self.dwconv(x).flatten(2).transpose(1, 2)#B,N,C\n",
    "        x = x.permute(1,0,2)\n",
    "        return x\n",
    "class ConvFFN(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None,\n",
    "                 act_layer=nn.GELU, drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.dwconv = DWConv(hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.dwconv(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "    \n",
    "class ConvFFN(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None,\n",
    "                 act_layer=nn.GELU, drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.dwconv = DWConv(hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.dwconv(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "def drop_path(x, drop_prob: float = 0., training: bool = False, scale_by_keep: bool = True):\n",
    "\n",
    "    if drop_prob == 0. or not training:\n",
    "        return x\n",
    "    keep_prob = 1 - drop_prob\n",
    "    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n",
    "    random_tensor = x.new_empty(shape).bernoulli_(keep_prob)\n",
    "    if keep_prob > 0.0 and scale_by_keep:\n",
    "        random_tensor.div_(keep_prob)\n",
    "    return x * random_tensor\n",
    "\n",
    "class DropPath(nn.Module):\n",
    "   \n",
    "    def __init__(self, drop_prob: float = 0., scale_by_keep: bool = True):\n",
    "        super(DropPath, self).__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.scale_by_keep = scale_by_keep\n",
    "\n",
    "    def forward(self, x):\n",
    "        return drop_path(x, self.drop_prob, self.training, self.scale_by_keep)\n",
    "\n",
    "    def extra_repr(self):\n",
    "        return f'drop_prob={round(self.drop_prob,3):0.3f}'\n",
    "    \n",
    "\n",
    "class Extractor(nn.Module):\n",
    "    def __init__(self, d_model, num_heads=8, dropout=0.1, drop_path=0.1,\n",
    "                 norm_layer=lambda x: nn.LayerNorm(x, eps=1e-6)):  # Замена partial на лямбду\n",
    "        super().__init__()\n",
    "        self.query_norm = norm_layer(d_model)\n",
    "        self.feat_norm = norm_layer(d_model)\n",
    "        self.attn = nn.MultiheadAttention(d_model, num_heads, dropout=dropout)\n",
    "        # convffn\n",
    "        self.ffn = ConvFFN(in_features=d_model, hidden_features=int(d_model * 0.25), drop=0.)\n",
    "        self.ffn_norm = norm_layer(d_model)\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "\n",
    "    def forward(self, query, feat):\n",
    "\n",
    "        def _inner_forward(query, feat):\n",
    "            # query:l,b,d;feat:l,b,d\n",
    "            attn = self.attn(self.query_norm(query),\n",
    "                             self.feat_norm(feat), self.feat_norm(feat))[0]\n",
    "            query = query + attn\n",
    "\n",
    "            query = query + self.drop_path(self.ffn(self.ffn_norm(query)))\n",
    "            return query\n",
    "\n",
    "        query = _inner_forward(query, feat)\n",
    "\n",
    "        return query\n",
    " \n",
    "class Injector(nn.Module):\n",
    "    def __init__(self, d_model, n_heads=8, norm_layer=lambda x: nn.LayerNorm(x, eps=1e-6), dropout=0.1, init_values=0.):\n",
    "        super().__init__()\n",
    "        self.query_norm = norm_layer(d_model)\n",
    "        self.feat_norm = norm_layer(d_model)\n",
    "        self.attn = nn.MultiheadAttention(d_model, n_heads, dropout=dropout)\n",
    "        self.gamma = nn.Parameter(init_values * torch.ones((d_model)), requires_grad=True)\n",
    "        \n",
    "    def forward(self, query,feat):\n",
    "            #query:l,b,d;feat:l,b,d\n",
    "        def _inner_forward(query, feat):\n",
    "\n",
    "            attn = self.attn(self.query_norm(query),\n",
    "                             self.feat_norm(feat),self.feat_norm(feat))[0]\n",
    "            return query + self.gamma * attn\n",
    "        query = _inner_forward(query, feat)\n",
    "        return query    \n",
    "    \n",
    "    \n",
    "\n",
    "class InteractionBlock(nn.Module):\n",
    "    def __init__(self, d_model, extra_extractor, grad_ckpt):\n",
    "        super().__init__()\n",
    "        self.grad_ckpt = grad_ckpt\n",
    "        self.injector = Injector(d_model=d_model)\n",
    "        self.extractor = Extractor(d_model=d_model)\n",
    "        if extra_extractor:\n",
    "            self.extra_extractors = nn.Sequential(*[\n",
    "                Extractor(d_model=d_model)\n",
    "                for _ in range(2)])\n",
    "        else:\n",
    "            self.extra_extractors = None\n",
    "\n",
    "    def forward(self,x,xs,blocks):\n",
    "        x = self.injector(x.permute(1,0,2),xs.permute(1,0,2)).permute(1,0,2)\n",
    "        for idx,blk in enumerate(blocks):\n",
    "            x = checkpoint.checkpoint(blk, x, None,use_reentrant=False) if self.grad_ckpt else blk(x,None)\n",
    "        xs = checkpoint.checkpoint(self.extractor, xs.permute(1,0,2),x.permute(1,0,2),use_reentrant=False).permute(1,0,2) \\\n",
    "            if self.grad_ckpt else self.extractor(xs.permute(1, 0, 2), x.permute(1, 0, 2)).permute(1, 0, 2)  # b,n,c\n",
    "        # xs = self.extractor(xs.permute(1,0,2),x.permute(1,0,2)).permute(1,0,2)#b,n,c\n",
    "        if self.extra_extractors is not None:\n",
    "            for extractor in self.extra_extractors:\n",
    "                xs = checkpoint.checkpoint(extractor, xs.permute(1, 0, 2), x.permute(1, 0, 2), use_reentrant=False).permute(1, 0, 2) \\\n",
    "                    if self.grad_ckpt else extractor(xs.permute(1, 0, 2), x.permute(1, 0, 2)).permute(1, 0,2)  # b,n,c\n",
    "                # xs = extractor(xs.permute(1,0,2),x.permute(1,0,2)).permute(1,0,2)\n",
    "        return x,xs\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, d_model: int, eps: float = 1e-5):\n",
    "        super().__init__()\n",
    "\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(d_model))\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps) * self.weight\n",
    "\n",
    "        return output\n",
    "    \n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self,dt_scale, d_model,d_inner,dt_rank,d_state,bias,d_conv,conv_bias,dt_init,dt_max,dt_min,dt_init_floor,grad_ckpt):\n",
    "        super().__init__()\n",
    "\n",
    "        self.grad_ckpt = grad_ckpt\n",
    "        self.mixer = MambaBlock(dt_scale,d_model,d_inner,dt_rank,d_state,bias,d_conv,conv_bias,dt_init,dt_max,dt_min,dt_init_floor)\n",
    "        self.norm = RMSNorm(d_model)\n",
    "\n",
    "    def forward(self, x, h):\n",
    "        #  x : (B, L, D)\n",
    "        # h : (B, L, ED, N)\n",
    "        #  output : (B,L, D)\n",
    "\n",
    "        x = self.norm(x)\n",
    "        output, h = checkpoint.checkpoint(self.mixer,x,h,use_reentrant=False) if self.grad_ckpt else self.mixer(x, h)\n",
    "        output = output + x\n",
    "        return output, h\n",
    "    \n",
    "class Mamba_Neck(nn.Module):\n",
    "    def __init__(self, in_channel=512,d_model=512,d_inner=1024,bias=False,n_layers=4,dt_rank=32,d_state=16,d_conv=3,dt_min=0.001,\n",
    "                 dt_max=0.1,dt_init='random',dt_scale=1.0,conv_bias=True,dt_init_floor=0.0001,grad_ckpt=False):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_inner = d_inner\n",
    "        self.bias = bias\n",
    "        self.dt_rank = dt_rank\n",
    "        self.d_state = d_state\n",
    "        self.dt_scale = dt_scale\n",
    "        self.num_channels = self.d_model\n",
    "        self.layers = nn.ModuleList(\n",
    "            [ResidualBlock(dt_scale,d_model,d_inner,dt_rank,d_state,bias,d_conv,conv_bias,dt_init,dt_max,dt_min,dt_init_floor,grad_ckpt)\n",
    "             for _ in range(n_layers)])\n",
    "        self.interactions = nn.ModuleList([\n",
    "            InteractionBlock(d_model=d_model,extra_extractor=(True if i == n_layers - 1 else False),grad_ckpt=grad_ckpt)\n",
    "            for i in range(n_layers)\n",
    "        ])\n",
    "        # self.norm_f = RMSNorm(config.d_model)\n",
    "\n",
    "    def forward(self, x,xs,h,blocks,interaction_indexes):\n",
    "        #  x : (B, L, D)\n",
    "        #  caches : [cache(layer) for all layers], cache : (h, inputs)\n",
    "\n",
    "        #  y : (B, L, D)\n",
    "        #  caches : [cache(layer) for all layers], cache : (h, inputs)\n",
    "        for i,index in enumerate(interaction_indexes):\n",
    "            xs, h[i] = self.layers[i](xs, h[i])\n",
    "            x,xs = self.interactions[i](x,xs,blocks[index[0]:index[1]])\n",
    "\n",
    "        return x, xs, h\n",
    "def build_neck(cfg,encoder):\n",
    "    in_channel = encoder.num_channels\n",
    "    d_model = cfg[\"MODEL\"][\"NECK\"][\"D_MODEL\"]\n",
    "    n_layers = cfg[\"MODEL\"][\"NECK\"][\"N_LAYERS\"]\n",
    "    d_state = cfg[\"MODEL\"][\"NECK\"][\"D_STATE\"]\n",
    "    grad_ckpt = cfg[\"MODEL\"][\"ENCODER\"][\"GRAD_CKPT\"]\n",
    "    neck = Mamba_Neck(in_channel=in_channel,d_model=d_model,d_inner=2*d_model,n_layers=n_layers,dt_rank=d_model//16,d_state=d_state,grad_ckpt=grad_ckpt)\n",
    "    return neck\n",
    "\n",
    "def box_xyxy_to_cxcywh(x):\n",
    "    x0, y0, x1, y1 = x.unbind(-1)\n",
    "    b = [(x0 + x1) / 2, (y0 + y1) / 2,\n",
    "         (x1 - x0), (y1 - y0)]\n",
    "    return torch.stack(b, dim=-1)\n",
    "\n",
    "class MLPPredictor(nn.Module):\n",
    "    def __init__(self, inplanes=64, channel=256, feat_sz=20, stride=16):\n",
    "        super(MLPPredictor, self).__init__()\n",
    "        self.feat_sz = feat_sz\n",
    "        self.stride = stride\n",
    "        self.img_sz = self.feat_sz * self.stride\n",
    "\n",
    "        self.num_layers = 3\n",
    "        h = [channel] * (self.num_layers - 1)\n",
    "        self.layers_cls = nn.ModuleList(nn.Linear(n, k)\n",
    "                                        for n, k in zip([inplanes] + h, h + [1]))\n",
    "        self.layers_reg = nn.ModuleList(nn.Linear(n, k)\n",
    "                                        for n, k in zip([inplanes] + h, h + [4]))\n",
    "\n",
    "        # for p in self.parameters():\n",
    "        #     if p.dim() > 1:\n",
    "        #         nn.init.xavier_uniform_(p)\n",
    "\n",
    "    def forward(self, x, gt_score_map=None):\n",
    "        \"\"\" Forward pass with input x. \"\"\"\n",
    "        score_map, offset_map = self.get_score_map(x)\n",
    "\n",
    "        # assert gt_score_map is None\n",
    "        if gt_score_map is None:\n",
    "            bbox = self.cal_bbox(score_map, offset_map)\n",
    "        else:\n",
    "            bbox = self.cal_bbox(gt_score_map.unsqueeze(1), offset_map)\n",
    "\n",
    "        return score_map, bbox, offset_map\n",
    "\n",
    "    def cal_bbox(self, score_map, offset_map, return_score=False):\n",
    "        max_score, idx = torch.max(score_map.flatten(1), dim=1, keepdim=True)\n",
    "        idx_y = torch.div(idx, self.feat_sz, rounding_mode='floor')\n",
    "        idx_x = idx % self.feat_sz\n",
    "\n",
    "        idx = idx.unsqueeze(1).expand(idx.shape[0], 4, 1) # torch.Size([32, 4, 1])\n",
    "        offset = offset_map.flatten(2).gather(dim=2, index=idx).squeeze(-1)\n",
    "        # offset: (l,t,r,b)\n",
    "\n",
    "        # x1, y1, x2, y2\n",
    "        bbox = torch.cat([idx_x.to(torch.float) / self.feat_sz - offset[:, :1], # the offset should not divide the self.feat_sz, since I use the sigmoid to limit it in (0,1)\n",
    "                          idx_y.to(torch.float) / self.feat_sz - offset[:, 1:2],\n",
    "                          idx_x.to(torch.float) / self.feat_sz + offset[:, 2:3],\n",
    "                          idx_y.to(torch.float) / self.feat_sz + offset[:, 3:4],\n",
    "                          ], dim=1)\n",
    "        bbox = box_xyxy_to_cxcywh(bbox)\n",
    "        if return_score:\n",
    "            return bbox, max_score\n",
    "        return bbox\n",
    "\n",
    "    def get_score_map(self, x):\n",
    "\n",
    "        def _sigmoid(x):\n",
    "            y = torch.clamp(x.sigmoid_(), min=1e-4, max=1 - 1e-4)\n",
    "            return y\n",
    "\n",
    "        x_cls = x\n",
    "        for i, layer in enumerate(self.layers_cls):\n",
    "            x_cls = F.relu(layer(x_cls)) if i < self.num_layers - 1 else layer(x_cls)\n",
    "        x_cls = x_cls.permute(0,2,1).reshape(-1,1,self.feat_sz,self.feat_sz)\n",
    "\n",
    "        x_reg = x\n",
    "        for i, layer in enumerate(self.layers_reg):\n",
    "            x_reg = F.relu(layer(x_reg)) if i < self.num_layers - 1 else layer(x_reg)\n",
    "        x_reg = x_reg.permute(0, 2, 1).reshape(-1, 4, self.feat_sz, self.feat_sz)\n",
    "\n",
    "        return _sigmoid(x_cls), _sigmoid(x_reg)\n",
    "    \n",
    "class FrozenBatchNorm2d(torch.nn.Module):\n",
    "\n",
    "\n",
    "    def __init__(self, n):\n",
    "        super(FrozenBatchNorm2d, self).__init__()\n",
    "        self.register_buffer(\"weight\", torch.ones(n))\n",
    "        self.register_buffer(\"bias\", torch.zeros(n))\n",
    "        self.register_buffer(\"running_mean\", torch.zeros(n))\n",
    "        self.register_buffer(\"running_var\", torch.ones(n))\n",
    "\n",
    "    def _load_from_state_dict(self, state_dict, prefix, local_metadata, strict,\n",
    "                              missing_keys, unexpected_keys, error_msgs):\n",
    "        num_batches_tracked_key = prefix + 'num_batches_tracked'\n",
    "        if num_batches_tracked_key in state_dict:\n",
    "            del state_dict[num_batches_tracked_key]\n",
    "\n",
    "        super(FrozenBatchNorm2d, self)._load_from_state_dict(\n",
    "            state_dict, prefix, local_metadata, strict,\n",
    "            missing_keys, unexpected_keys, error_msgs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # move reshapes to the beginning\n",
    "        # to make it fuser-friendly\n",
    "        w = self.weight.reshape(1, -1, 1, 1)\n",
    "        b = self.bias.reshape(1, -1, 1, 1)\n",
    "        rv = self.running_var.reshape(1, -1, 1, 1)\n",
    "        rm = self.running_mean.reshape(1, -1, 1, 1)\n",
    "        eps = 1e-5\n",
    "        scale = w * (rv + eps).rsqrt()  # rsqrt(x): 1/sqrt(x), r: reciprocal\n",
    "        bias = b - rm * scale\n",
    "        return x * scale + bias\n",
    "\n",
    "def conv(in_planes, out_planes, kernel_size=3, stride=1, padding=1, dilation=1,\n",
    "         freeze_bn=False):\n",
    "    if freeze_bn:\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride,\n",
    "                      padding=padding, dilation=dilation, bias=True),\n",
    "            FrozenBatchNorm2d(out_planes),\n",
    "            nn.ReLU(inplace=True))\n",
    "    else:\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride,\n",
    "                      padding=padding, dilation=dilation, bias=True),\n",
    "            nn.BatchNorm2d(out_planes),\n",
    "            nn.ReLU(inplace=True))\n",
    "    \n",
    "def conv(in_planes, out_planes, kernel_size=3, stride=1, padding=1, dilation=1,\n",
    "         freeze_bn=False):\n",
    "    if freeze_bn:\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride,\n",
    "                      padding=padding, dilation=dilation, bias=True),\n",
    "            FrozenBatchNorm2d(out_planes),\n",
    "            nn.ReLU(inplace=True))\n",
    "    else:\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride,\n",
    "                      padding=padding, dilation=dilation, bias=True),\n",
    "            nn.BatchNorm2d(out_planes),\n",
    "            nn.ReLU(inplace=True))\n",
    "    \n",
    "class CenterPredictor(nn.Module, ):\n",
    "    def __init__(self, inplanes=64, channel=256, feat_sz=20, stride=16, freeze_bn=False):\n",
    "        super(CenterPredictor, self).__init__()\n",
    "        self.feat_sz = feat_sz\n",
    "        self.stride = stride\n",
    "        self.img_sz = self.feat_sz * self.stride\n",
    "\n",
    "        # corner predict\n",
    "        self.conv1_ctr = conv(inplanes, channel, freeze_bn=freeze_bn)\n",
    "        self.conv2_ctr = conv(channel, channel // 2, freeze_bn=freeze_bn)\n",
    "        self.conv3_ctr = conv(channel // 2, channel // 4, freeze_bn=freeze_bn)\n",
    "        self.conv4_ctr = conv(channel // 4, channel // 8, freeze_bn=freeze_bn)\n",
    "        self.conv5_ctr = nn.Conv2d(channel // 8, 1, kernel_size=1)\n",
    "\n",
    "        # size regress\n",
    "        self.conv1_offset = conv(inplanes, channel, freeze_bn=freeze_bn)\n",
    "        self.conv2_offset = conv(channel, channel // 2, freeze_bn=freeze_bn)\n",
    "        self.conv3_offset = conv(channel // 2, channel // 4, freeze_bn=freeze_bn)\n",
    "        self.conv4_offset = conv(channel // 4, channel // 8, freeze_bn=freeze_bn)\n",
    "        self.conv5_offset = nn.Conv2d(channel // 8, 2, kernel_size=1)\n",
    "\n",
    "        # size regress\n",
    "        self.conv1_size = conv(inplanes, channel, freeze_bn=freeze_bn)\n",
    "        self.conv2_size = conv(channel, channel // 2, freeze_bn=freeze_bn)\n",
    "        self.conv3_size = conv(channel // 2, channel // 4, freeze_bn=freeze_bn)\n",
    "        self.conv4_size = conv(channel // 4, channel // 8, freeze_bn=freeze_bn)\n",
    "        self.conv5_size = nn.Conv2d(channel // 8, 2, kernel_size=1)\n",
    "\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "\n",
    "    def forward(self, x, gt_score_map=None):\n",
    "        \"\"\" Forward pass with input x. \"\"\"\n",
    "        score_map_ctr, size_map, offset_map = self.get_score_map(x) # x: torch.Size([b, c, h, w])\n",
    "        # score_map_ctr: torch.Size([32, 1, 16, 16]) size_map: torch.Size([32, 2, 16, 16]) offset_map: torch.Size([32, 2, 16, 16])\n",
    "\n",
    "        # assert gt_score_map is None\n",
    "        if gt_score_map is None:\n",
    "            bbox = self.cal_bbox(score_map_ctr, size_map, offset_map)\n",
    "        else:\n",
    "            bbox = self.cal_bbox(gt_score_map.unsqueeze(1), size_map, offset_map)\n",
    "\n",
    "        return score_map_ctr, bbox, size_map, offset_map\n",
    "\n",
    "    def cal_bbox(self, score_map_ctr, size_map, offset_map, return_score=False):\n",
    "        max_score, idx = torch.max(score_map_ctr.flatten(1), dim=1, keepdim=True) # score_map_ctr.flatten(1): torch.Size([32, 256]) idx: torch.Size([32, 1]) max_score: torch.Size([32, 1])\n",
    "        idx_y = torch.div(idx, self.feat_sz, rounding_mode='floor')\n",
    "        idx_x = idx % self.feat_sz\n",
    "\n",
    "        idx = idx.unsqueeze(1).expand(idx.shape[0], 2, 1)\n",
    "        size = size_map.flatten(2).gather(dim=2, index=idx) # size_map: torch.Size([32, 2, 16, 16])  size_map.flatten(2): torch.Size([32, 2, 256])\n",
    "        offset = offset_map.flatten(2).gather(dim=2, index=idx).squeeze(-1)\n",
    "\n",
    "        # bbox = torch.cat([idx_x - size[:, 0] / 2, idx_y - size[:, 1] / 2,\n",
    "        #                   idx_x + size[:, 0] / 2, idx_y + size[:, 1] / 2], dim=1) / self.feat_sz\n",
    "        # cx, cy, w, h\n",
    "        bbox = torch.cat([(idx_x.to(torch.float) + offset[:, :1]) / self.feat_sz,\n",
    "                          (idx_y.to(torch.float) + offset[:, 1:]) / self.feat_sz,\n",
    "                          size.squeeze(-1)], dim=1)\n",
    "\n",
    "        if return_score:\n",
    "            return bbox, max_score\n",
    "        return bbox\n",
    "\n",
    "    def get_pred(self, score_map_ctr, size_map, offset_map):\n",
    "        max_score, idx = torch.max(score_map_ctr.flatten(1), dim=1, keepdim=True)\n",
    "        idx_y = idx // self.feat_sz\n",
    "        idx_x = idx % self.feat_sz\n",
    "\n",
    "        idx = idx.unsqueeze(1).expand(idx.shape[0], 2, 1)\n",
    "        size = size_map.flatten(2).gather(dim=2, index=idx)\n",
    "        offset = offset_map.flatten(2).gather(dim=2, index=idx).squeeze(-1)\n",
    "\n",
    "        # bbox = torch.cat([idx_x - size[:, 0] / 2, idx_y - size[:, 1] / 2,\n",
    "        #                   idx_x + size[:, 0] / 2, idx_y + size[:, 1] / 2], dim=1) / self.feat_sz\n",
    "        return size * self.feat_sz, offset\n",
    "\n",
    "    def get_score_map(self, x):\n",
    "\n",
    "        def _sigmoid(x):\n",
    "            y = torch.clamp(x.sigmoid_(), min=1e-4, max=1 - 1e-4)\n",
    "            return y\n",
    "\n",
    "        # ctr branch\n",
    "        x_ctr1 = self.conv1_ctr(x)\n",
    "        x_ctr2 = self.conv2_ctr(x_ctr1)\n",
    "        x_ctr3 = self.conv3_ctr(x_ctr2)\n",
    "        x_ctr4 = self.conv4_ctr(x_ctr3)\n",
    "        score_map_ctr = self.conv5_ctr(x_ctr4)\n",
    "\n",
    "        # offset branch\n",
    "        x_offset1 = self.conv1_offset(x)\n",
    "        x_offset2 = self.conv2_offset(x_offset1)\n",
    "        x_offset3 = self.conv3_offset(x_offset2)\n",
    "        x_offset4 = self.conv4_offset(x_offset3)\n",
    "        score_map_offset = self.conv5_offset(x_offset4)\n",
    "\n",
    "        # size branch\n",
    "        x_size1 = self.conv1_size(x)\n",
    "        x_size2 = self.conv2_size(x_size1)\n",
    "        x_size3 = self.conv3_size(x_size2)\n",
    "        x_size4 = self.conv4_size(x_size3)\n",
    "        score_map_size = self.conv5_size(x_size4)\n",
    "        return _sigmoid(score_map_ctr), _sigmoid(score_map_size), score_map_offset\n",
    "    \n",
    "def build_decoder(cfg, encoder):\n",
    "    num_channels_enc = encoder.num_channels\n",
    "    stride = cfg[\"MODEL\"][\"ENCODER\"][\"STRIDE\"]\n",
    "    if cfg[\"MODEL\"][\"DECODER\"][\"TYPE\"] == \"MLP\":\n",
    "        in_channel = num_channels_enc\n",
    "        hidden_dim = cfg[\"MODEL\"][\"DECODER\"][\"NUM_CHANNELS\"]\n",
    "        feat_sz = int(cfg[\"DATA\"][\"SEARCH\"][\"SIZE\"] / stride)\n",
    "        mlp_head = MLPPredictor(inplanes=in_channel, channel=hidden_dim,\n",
    "                                feat_sz=feat_sz, stride=stride)\n",
    "        return mlp_head\n",
    "    elif \"CORNER\" in cfg[\"MODEL\"][\"DECODER\"][\"TYPE\"]:\n",
    "        feat_sz = int(cfg[\"DATA\"][\"SEARCH\"][\"SIZE\"] / stride)\n",
    "        channel = getattr(cfg[\"MODEL\"], \"NUM_CHANNELS\", 256)\n",
    "        print(\"head channel: %d\" % channel)\n",
    "        if cfg[\"MODEL\"][\"HEAD\"][\"TYPE\"] == \"CORNER\":\n",
    "            corner_head = Corner_Predictor(inplanes=cfg[\"MODEL\"][\"HIDDEN_DIM\"], channel=channel,\n",
    "                                           feat_sz=feat_sz, stride=stride)\n",
    "        else:\n",
    "            raise ValueError()\n",
    "        return corner_head\n",
    "    elif cfg[\"MODEL\"][\"DECODER\"][\"TYPE\"] == \"CENTER\":\n",
    "        in_channel = num_channels_enc\n",
    "        out_channel = cfg[\"MODEL\"][\"DECODER\"][\"NUM_CHANNELS\"]\n",
    "        feat_sz = int(cfg[\"DATA\"][\"SEARCH\"][\"SIZE\"] / stride)\n",
    "        center_head = CenterPredictor(inplanes=in_channel, channel=out_channel,\n",
    "                                      feat_sz=feat_sz, stride=stride)\n",
    "        return center_head\n",
    "    else:\n",
    "        raise ValueError(\"HEAD TYPE %s is not supported.\" % cfg[\"MODEL\"][\"HEAD_TYPE\"])\n",
    "    \n",
    "class Corner_Predictor(nn.Module):\n",
    "    \"\"\" Corner Predictor module\"\"\"\n",
    "\n",
    "    def __init__(self, inplanes=64, channel=256, feat_sz=20, stride=16, freeze_bn=False):\n",
    "        super(Corner_Predictor, self).__init__()\n",
    "        self.feat_sz = feat_sz\n",
    "        self.stride = stride\n",
    "        self.img_sz = self.feat_sz * self.stride\n",
    "        '''top-left corner'''\n",
    "        self.conv1_tl = conv(inplanes, channel, freeze_bn=freeze_bn)\n",
    "        self.conv2_tl = conv(channel, channel // 2, freeze_bn=freeze_bn)\n",
    "        self.conv3_tl = conv(channel // 2, channel // 4, freeze_bn=freeze_bn)\n",
    "        self.conv4_tl = conv(channel // 4, channel // 8, freeze_bn=freeze_bn)\n",
    "        self.conv5_tl = nn.Conv2d(channel // 8, 1, kernel_size=1)\n",
    "\n",
    "        '''bottom-right corner'''\n",
    "        self.conv1_br = conv(inplanes, channel, freeze_bn=freeze_bn)\n",
    "        self.conv2_br = conv(channel, channel // 2, freeze_bn=freeze_bn)\n",
    "        self.conv3_br = conv(channel // 2, channel // 4, freeze_bn=freeze_bn)\n",
    "        self.conv4_br = conv(channel // 4, channel // 8, freeze_bn=freeze_bn)\n",
    "        self.conv5_br = nn.Conv2d(channel // 8, 1, kernel_size=1)\n",
    "\n",
    "        '''about coordinates and indexs'''\n",
    "        with torch.no_grad():\n",
    "            self.indice = torch.arange(0, self.feat_sz).view(-1, 1) * self.stride\n",
    "            # generate mesh-grid\n",
    "            self.coord_x = self.indice.repeat((self.feat_sz, 1)) \\\n",
    "                .view((self.feat_sz * self.feat_sz,)).float().cuda()\n",
    "            self.coord_y = self.indice.repeat((1, self.feat_sz)) \\\n",
    "                .view((self.feat_sz * self.feat_sz,)).float().cuda()\n",
    "\n",
    "    def forward(self, x, return_dist=False, softmax=True):\n",
    "        \"\"\" Forward pass with input x. \"\"\"\n",
    "        score_map_tl, score_map_br = self.get_score_map(x)\n",
    "        if return_dist:\n",
    "            coorx_tl, coory_tl, prob_vec_tl = self.soft_argmax(score_map_tl, return_dist=True, softmax=softmax)\n",
    "            coorx_br, coory_br, prob_vec_br = self.soft_argmax(score_map_br, return_dist=True, softmax=softmax)\n",
    "            return torch.stack((coorx_tl, coory_tl, coorx_br, coory_br), dim=1) / self.img_sz, prob_vec_tl, prob_vec_br\n",
    "        else:\n",
    "            coorx_tl, coory_tl = self.soft_argmax(score_map_tl)\n",
    "            coorx_br, coory_br = self.soft_argmax(score_map_br)\n",
    "            return torch.stack((coorx_tl, coory_tl, coorx_br, coory_br), dim=1) / self.img_sz\n",
    "\n",
    "    def get_score_map(self, x):\n",
    "        # top-left branch\n",
    "        x_tl1 = self.conv1_tl(x)\n",
    "        x_tl2 = self.conv2_tl(x_tl1)\n",
    "        x_tl3 = self.conv3_tl(x_tl2)\n",
    "        x_tl4 = self.conv4_tl(x_tl3)\n",
    "        score_map_tl = self.conv5_tl(x_tl4)\n",
    "\n",
    "        # bottom-right branch\n",
    "        x_br1 = self.conv1_br(x)\n",
    "        x_br2 = self.conv2_br(x_br1)\n",
    "        x_br3 = self.conv3_br(x_br2)\n",
    "        x_br4 = self.conv4_br(x_br3)\n",
    "        score_map_br = self.conv5_br(x_br4)\n",
    "        return score_map_tl, score_map_br\n",
    "\n",
    "    def soft_argmax(self, score_map, return_dist=False, softmax=True):\n",
    "        \"\"\" get soft-argmax coordinate for a given heatmap \"\"\"\n",
    "        score_vec = score_map.view((-1, self.feat_sz * self.feat_sz))  # (batch, feat_sz * feat_sz)\n",
    "        prob_vec = nn.functional.softmax(score_vec, dim=1)\n",
    "        exp_x = torch.sum((self.coord_x * prob_vec), dim=1)\n",
    "        exp_y = torch.sum((self.coord_y * prob_vec), dim=1)\n",
    "        if return_dist:\n",
    "            if softmax:\n",
    "                return exp_x, exp_y, prob_vec\n",
    "            else:\n",
    "                return exp_x, exp_y, score_vec\n",
    "        else:\n",
    "            return exp_x, exp_y\n",
    "        \n",
    "class Preprocessor(object):\n",
    "    def __init__(self):\n",
    "        self.mean = torch.tensor([0.485, 0.456, 0.406]).view((1, 3, 1, 1)).cuda()\n",
    "        self.std = torch.tensor([0.229, 0.224, 0.225]).view((1, 3, 1, 1)).cuda()\n",
    "        self.mm_mean = torch.tensor([0.485, 0.456, 0.406, 0.485, 0.456, 0.406]).view((1, 6, 1, 1)).cuda()\n",
    "        self.mm_std = torch.tensor([0.229, 0.224, 0.225, 0.229, 0.224, 0.225]).view((1, 6, 1, 1)).cuda()\n",
    "\n",
    "    def process(self, img_arr: np.ndarray):\n",
    "        if img_arr.shape[-1] == 6:\n",
    "            mean = self.mm_mean\n",
    "            std = self.mm_std\n",
    "        else:\n",
    "            mean = self.mean\n",
    "            std = self.std\n",
    "        # Deal with the image patch\n",
    "        img_tensor = torch.tensor(img_arr).cuda().float().permute((2,0,1)).unsqueeze(dim=0)\n",
    "        # img_tensor = torch.tensor(img_arr).float().permute((2,0,1)).unsqueeze(dim=0)\n",
    "        img_tensor_norm = ((img_tensor / 255.0) - mean) / std  # (1,3,H,W)\n",
    "        return img_tensor_norm\n",
    "    \n",
    "def hann1d(sz: int, centered = True) -> torch.Tensor:\n",
    "    \"\"\"1D cosine window.\"\"\"\n",
    "    if centered:\n",
    "        return 0.5 * (1 - torch.cos((2 * math.pi / (sz + 1)) * torch.arange(1, sz + 1).float()))\n",
    "    w = 0.5 * (1 + torch.cos((2 * math.pi / (sz + 2)) * torch.arange(0, sz//2 + 1).float()))\n",
    "    return torch.cat([w, w[1:sz-sz//2].flip((0,))])\n",
    "    \n",
    "def hann2d(sz: torch.Tensor, centered = True) -> torch.Tensor:\n",
    "    \"\"\"2D cosine window.\"\"\"\n",
    "    return hann1d(sz[0].item(), centered).reshape(1, 1, -1, 1) * hann1d(sz[1].item(), centered).reshape(1, 1, 1, -1)    \n",
    "\n",
    "def sample_target(im, target_bb, search_area_factor, output_sz=None):\n",
    "   \n",
    "    if not isinstance(target_bb, list):\n",
    "        x, y, w, h = target_bb.tolist()\n",
    "    else:\n",
    "        x, y, w, h = target_bb\n",
    "    # Crop image\n",
    "    crop_sz = math.ceil(math.sqrt(w * h) * search_area_factor)\n",
    "\n",
    "    if crop_sz < 1:\n",
    "        raise Exception('Too small bounding box.')\n",
    "\n",
    "    x1 = round(x + 0.5 * w - crop_sz * 0.5)\n",
    "    x2 = x1 + crop_sz\n",
    "\n",
    "    y1 = round(y + 0.5 * h - crop_sz * 0.5)\n",
    "    y2 = y1 + crop_sz\n",
    "\n",
    "    x1_pad = max(0, -x1)\n",
    "    x2_pad = max(x2 - im.shape[1] + 1, 0)\n",
    "\n",
    "    y1_pad = max(0, -y1)\n",
    "    y2_pad = max(y2 - im.shape[0] + 1, 0)\n",
    "\n",
    "    # Crop target\n",
    "    im_crop = im[y1 + y1_pad:y2 - y2_pad, x1 + x1_pad:x2 - x2_pad, :]\n",
    "\n",
    "    # Pad\n",
    "    im_crop_padded = cv2.copyMakeBorder(im_crop, y1_pad, y2_pad, x1_pad, x2_pad, cv2.BORDER_CONSTANT)\n",
    "    # deal with attention mask\n",
    "    H, W, _ = im_crop_padded.shape\n",
    "\n",
    "    if output_sz is not None:\n",
    "        resize_factor = output_sz / crop_sz\n",
    "        im_crop_padded = cv2.resize(im_crop_padded, (output_sz, output_sz))\n",
    "\n",
    "        return im_crop_padded, resize_factor\n",
    "\n",
    "    else:\n",
    "        return im_crop_padded, 1.0\n",
    "def transform_image_to_crop(box_in: torch.Tensor, box_extract: torch.Tensor, resize_factor: float,\n",
    "                            crop_sz: torch.Tensor, normalize=False) -> torch.Tensor:\n",
    "   \n",
    "    box_extract_center = box_extract[0:2] + 0.5 * box_extract[2:4]\n",
    "\n",
    "    box_in_center = box_in[0:2] + 0.5 * box_in[2:4]\n",
    "\n",
    "    box_out_center = (crop_sz - 1) / 2 + (box_in_center - box_extract_center) * resize_factor\n",
    "    box_out_wh = box_in[2:4] * resize_factor\n",
    "\n",
    "    box_out = torch.cat((box_out_center - 0.5 * box_out_wh, box_out_wh))\n",
    "    if normalize:\n",
    "        return box_out / (crop_sz[0]-1)\n",
    "    else:\n",
    "        return box_out\n",
    "def clip_box(box: list, H, W, margin=0):\n",
    "    x1, y1, w, h = box\n",
    "    x2, y2 = x1 + w, y1 + h\n",
    "    x1 = min(max(0, x1), W-margin)\n",
    "    x2 = min(max(margin, x2), W)\n",
    "    y1 = min(max(0, y1), H-margin)\n",
    "    y2 = min(max(margin, y2), H)\n",
    "    w = max(margin, x2-x1)\n",
    "    h = max(margin, y2-y1)\n",
    "    return [x1, y1, w, h]\n",
    "\n",
    "class BaseTracker():\n",
    "    \"\"\"Base class for all trackers.\"\"\"\n",
    "\n",
    "    def __init__(self, params):\n",
    "        self.params = params\n",
    "        self.visdom = None\n",
    "\n",
    "    def predicts_segmentation_mask(self):\n",
    "        return False\n",
    "\n",
    "    def initialize(self, image, info: dict) -> dict:\n",
    "        \"\"\"Overload this function in your tracker. This should initialize the model.\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def track(self, image, info: dict = None) -> dict:\n",
    "        \"\"\"Overload this function in your tracker. This should track in the frame and update the model.\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def visdom_draw_tracking(self, image, box, segmentation=None):\n",
    "        # Упрощенная обработка box без OrderedDict\n",
    "        if isinstance(box, dict):  # Проверяем на обычный dict вместо OrderedDict\n",
    "            box = list(box.values())  # Берем только значения\n",
    "        elif not isinstance(box, (list, tuple)):  # Если не коллекция\n",
    "            box = (box,)  # Превращаем в кортеж\n",
    "        \n",
    "        # Визуализация\n",
    "        if segmentation is None:\n",
    "            self.visdom.register((image, *box), 'Tracking', 1, 'Tracking')\n",
    "        else:\n",
    "            self.visdom.register((image, *box, segmentation), 'Tracking', 1, 'Tracking')\n",
    "\n",
    "\n",
    "class MCITRACK(BaseTracker):\n",
    "    def __init__(self, params):\n",
    "        \n",
    "        super(MCITRACK, self).__init__(params)\n",
    "        network = build_mcitrack(params.cfg)\n",
    "        network.load_state_dict(torch.load(\"MCITRACK_ep0300.pth.tar\", map_location='cpu')['net'], strict=True)\n",
    "        \n",
    "        self.cfg = params.cfg\n",
    "        self.network = network.cuda()\n",
    "        self.network.eval()\n",
    "        self.preprocessor = Preprocessor()\n",
    "        self.state = None\n",
    "\n",
    "        self.fx_sz = self.cfg[\"TEST\"][\"SEARCH_SIZE\"] // self.cfg[\"MODEL\"][\"ENCODER\"][\"STRIDE\"]\n",
    "        if self.cfg[\"TEST\"][\"WINDOW\"] == True:  # for window penalty\n",
    "            self.output_window = hann2d(torch.tensor([self.fx_sz, self.fx_sz]).long(), centered=True).cuda()\n",
    "\n",
    "        self.num_template = self.cfg[\"TEST\"][\"NUM_TEMPLATES\"]\n",
    "\n",
    "   \n",
    "        self.frame_id = 0\n",
    "        # for update\n",
    "        self.h_state = [None] * self.cfg[\"MODEL\"][\"NECK\"][\"N_LAYERS\"]\n",
    "\n",
    "\n",
    "\n",
    "        self.memory_bank = self.cfg[\"TEST\"][\"MB\"][\"DEFAULT\"]\n",
    "        self.update_h_t = self.cfg[\"TEST\"][\"UPH\"][\"DEFAULT\"]\n",
    "        self.update_threshold = self.cfg[\"TEST\"][\"UPT\"][\"DEFAULT\"]\n",
    "        self.update_intervals = self.cfg[\"TEST\"][\"INTER\"][\"DEFAULT\"]\n",
    "        print(\"Update threshold is: \", self.memory_bank)\n",
    "\n",
    "    def initialize(self, image, info: dict):\n",
    "\n",
    "\n",
    "        # get the initial templates\n",
    "        z_patch_arr, resize_factor = sample_target(image, info['init_bbox'], self.params.template_factor,\n",
    "                                                   output_sz=self.params.template_size)\n",
    "        z_patch_arr = z_patch_arr\n",
    "        template = self.preprocessor.process(z_patch_arr)\n",
    "        self.template_list = [template] * self.num_template\n",
    "\n",
    "        self.state = info['init_bbox']\n",
    "        prev_box_crop = transform_image_to_crop(torch.tensor(info['init_bbox']),\n",
    "                                                torch.tensor(info['init_bbox']),\n",
    "                                                resize_factor,\n",
    "                                                torch.Tensor([self.params.template_size, self.params.template_size]),\n",
    "                                                normalize=True)\n",
    "        self.template_anno_list = [prev_box_crop.to(template.device).unsqueeze(0)] * self.num_template\n",
    "        self.frame_id = 0\n",
    "        self.memory_template_list = self.template_list.copy()\n",
    "        self.memory_template_anno_list = self.template_anno_list.copy()\n",
    "\n",
    "\n",
    "    def track(self, image, info: dict = None):\n",
    "        H, W, _ = image.shape\n",
    "        self.frame_id += 1\n",
    "        x_patch_arr, resize_factor = sample_target(image, self.state, self.params.search_factor,\n",
    "                                                   output_sz=self.params.search_size)  # (x1, y1, w, h)\n",
    "        search = self.preprocessor.process(x_patch_arr)\n",
    "        search_list = [search]\n",
    "\n",
    "        # run the encoder\n",
    "\n",
    "        with torch.no_grad():\n",
    "            out_dict = self.network.forward(\n",
    "                template_list=self.template_list,\n",
    "                search_list=search_list,\n",
    "                template_anno_list=self.template_anno_list,\n",
    "                \n",
    "                gt_score_map=None\n",
    "            )\n",
    "\n",
    "        \n",
    "\n",
    "        # add hann windows\n",
    "        pred_score_map = out_dict['score_map']\n",
    "        if self.cfg[\"TEST\"][\"WINDOW\"] == True:  # for window penalty\n",
    "            response = self.output_window * pred_score_map\n",
    "        else:\n",
    "            response = pred_score_map\n",
    "        if 'size_map' in out_dict.keys():\n",
    "            pred_boxes, conf_score = self.network.decoder.cal_bbox(response, out_dict['size_map'],\n",
    "                                                                   out_dict['offset_map'], return_score=True)\n",
    "        else:\n",
    "            pred_boxes, conf_score = self.network.decoder.cal_bbox(response,\n",
    "                                                                   out_dict['offset_map'],\n",
    "                                                                   return_score=True)\n",
    "        pred_boxes = pred_boxes.view(-1, 4)\n",
    "        # Baseline: Take the mean of all pred boxes as the final result\n",
    "        pred_box = (pred_boxes.mean(dim=0) * self.params.search_size / resize_factor).tolist()  # (cx, cy, w, h) [0,1]\n",
    "        # get the final box result\n",
    "        self.state = clip_box(self.map_box_back(pred_box, resize_factor), H, W, margin=10)\n",
    "        # update hiden state\n",
    "        # self.h_state = h\n",
    "        # if conf_score.item() < self.update_h_t:\n",
    "        #     self.h_state = [None] * self.cfg[\"MODEL\"][\"NECK\"][\"N_LAYERS\"]\n",
    "\n",
    "        # update the template\n",
    "        if self.num_template > 1:\n",
    "            if (conf_score > self.update_threshold):\n",
    "                z_patch_arr, resize_factor = sample_target(image, self.state, self.params.template_factor,\n",
    "                                                           output_sz=self.params.template_size)\n",
    "                template = self.preprocessor.process(z_patch_arr)\n",
    "                self.memory_template_list.append(template)\n",
    "                prev_box_crop = transform_image_to_crop(torch.tensor(self.state),\n",
    "                                                        torch.tensor(self.state),\n",
    "                                                        resize_factor,\n",
    "                                                        torch.Tensor(\n",
    "                                                            [self.params.template_size, self.params.template_size]),\n",
    "                                                        normalize=True)\n",
    "                self.memory_template_anno_list.append(prev_box_crop.to(template.device).unsqueeze(0))\n",
    "                if len(self.memory_template_list) > self.memory_bank:\n",
    "                    self.memory_template_list.pop(0)\n",
    "                    self.memory_template_anno_list.pop(0)\n",
    "        if (self.frame_id % self.update_intervals == 0):\n",
    "            assert len(self.memory_template_anno_list) == len(self.memory_template_list)\n",
    "            len_list = len(self.memory_template_anno_list)\n",
    "            interval = len_list // self.num_template\n",
    "            for i in range(1, self.num_template):\n",
    "                idx = interval * i\n",
    "                if idx > len_list:\n",
    "                    idx = len_list\n",
    "                self.template_list.append(self.memory_template_list[idx])\n",
    "                self.template_list.pop(1)\n",
    "                self.template_anno_list.append(self.memory_template_anno_list[idx])\n",
    "                self.template_anno_list.pop(1)\n",
    "        assert len(self.template_list) == self.num_template\n",
    "\n",
    "\n",
    "\n",
    "        return {\"target_bbox\": self.state,\n",
    "                \"best_score\": conf_score}\n",
    "\n",
    "    def map_box_back(self, pred_box: list, resize_factor: float):\n",
    "        cx_prev, cy_prev = self.state[0] + 0.5 * self.state[2], self.state[1] + 0.5 * self.state[3]\n",
    "        cx, cy, w, h = pred_box\n",
    "        half_side = 0.5 * self.params.search_size / resize_factor\n",
    "        cx_real = cx + (cx_prev - half_side)\n",
    "        cy_real = cy + (cy_prev - half_side)\n",
    "        return [cx_real - 0.5 * w, cy_real - 0.5 * h, w, h]\n",
    "\n",
    "    def map_box_back_batch(self, pred_box: torch.Tensor, resize_factor: float):\n",
    "        cx_prev, cy_prev = self.state[0] + 0.5 * self.state[2], self.state[1] + 0.5 * self.state[3]\n",
    "        cx, cy, w, h = pred_box.unbind(-1)  # (N,4) --> (N,)\n",
    "        half_side = 0.5 * self.params.search_size / resize_factor\n",
    "        cx_real = cx + (cx_prev - half_side)\n",
    "        cy_real = cy + (cy_prev - half_side)\n",
    "        return torch.stack([cx_real - 0.5 * w, cy_real - 0.5 * h, w, h], dim=-1)\n",
    "\n",
    "class MCITrack(nn.Module):\n",
    "    \n",
    "    def __init__(self, encoder, decoder, neck, cfg,\n",
    "                 num_frames=1, num_template=1, decoder_type=\"CENTER\"):\n",
    "      \n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder_type = decoder_type\n",
    "        self.neck = neck\n",
    "\n",
    "        self.num_patch_x = self.encoder.body.num_patches_search\n",
    "        self.num_patch_z = self.encoder.body.num_patches_template\n",
    "        self.fx_sz = int(math.sqrt(self.num_patch_x))\n",
    "        self.fz_sz = int(math.sqrt(self.num_patch_z))\n",
    "\n",
    "        self.decoder = decoder\n",
    "\n",
    "        self.num_frames = num_frames\n",
    "        self.num_template = num_template\n",
    "        self.freeze_en = cfg[\"TRAIN\"][\"FREEZE_ENCODER\"]\n",
    "        self.interaction_indexes = cfg[\"MODEL\"][\"ENCODER\"][\"INTERACTION_INDEXES\"]\n",
    "\n",
    "    def forward(self, template_list, search_list, template_anno_list, gt_score_map=None):\n",
    "    \n",
    "        # Step 1: Forward pass through the encoder\n",
    "        \n",
    "        neck_h_state=[None,None,None,None]\n",
    "        \n",
    "        xz = self.encoder(template_list, search_list, template_anno_list)\n",
    "\n",
    "        # Step 2: Forward pass through the neck\n",
    "        xs = xz[:, 0:self.num_patch_x]  # Extract patch embeddings\n",
    "        x, xs, h = self.neck(\n",
    "            xz, xs, neck_h_state, \n",
    "            self.encoder.body.blocks, \n",
    "            self.interaction_indexes\n",
    "        )\n",
    "        x = self.encoder.body.fc_norm(x)\n",
    "        xs = xs + x[:, 0:self.num_patch_x]  # Updated patch embeddings\n",
    "\n",
    "        # Step 3: Forward pass through the decoder\n",
    "        bs, HW, C = xs.size()\n",
    "        if self.decoder_type in ['CORNER', 'CENTER']:\n",
    "            xs = xs.permute((0, 2, 1)).contiguous()\n",
    "            xs = xs.view(bs, C, self.fx_sz, self.fx_sz)\n",
    "\n",
    "        if self.decoder_type == \"CORNER\":\n",
    "            # Run the corner head\n",
    "            pred_box, score_map = self.decoder(xs, True)\n",
    "            outputs_coord = box_xyxy_to_cxcywh(pred_box)\n",
    "            outputs_coord_new = outputs_coord.view(bs, 1, 4)\n",
    "            return {\n",
    "                'pred_boxes': outputs_coord_new,\n",
    "                'score_map': score_map\n",
    "            }\n",
    "\n",
    "        elif self.decoder_type == \"CENTER\":\n",
    "            # Run the center head\n",
    "            score_map_ctr, bbox, size_map, offset_map = self.decoder(xs, gt_score_map)\n",
    "            outputs_coord = bbox\n",
    "            outputs_coord_new = outputs_coord.view(bs, 1, 4)\n",
    "            return {\n",
    "                'pred_boxes': outputs_coord_new,\n",
    "                'score_map': score_map_ctr,\n",
    "                'size_map': size_map,\n",
    "                'offset_map': offset_map\n",
    "            }\n",
    "\n",
    "        elif self.decoder_type == \"MLP\":\n",
    "            # Run the MLP head\n",
    "            score_map, bbox, offset_map = self.decoder(xs, gt_score_map)\n",
    "            outputs_coord = bbox\n",
    "            outputs_coord_new = outputs_coord.view(bs, 1, 4)\n",
    "            return {\n",
    "                'pred_boxes': outputs_coord_new,\n",
    "                'score_map': score_map,\n",
    "                'offset_map': offset_map\n",
    "            }\n",
    "\n",
    "        else:\n",
    "            raise NotImplementedError(f\"Decoder type not supported: {self.decoder_type}\")\n",
    "\n",
    "def build_mcitrack(cfg):\n",
    "    encoder = build_encoder(cfg)\n",
    "    neck = build_neck(cfg,encoder)\n",
    "    decoder = build_decoder(cfg, neck)\n",
    "    model = MCITrack(\n",
    "        encoder,\n",
    "        decoder,\n",
    "        neck,\n",
    "        cfg,\n",
    "        num_frames = cfg[\"DATA\"][\"SEARCH\"][\"NUMBER\"],\n",
    "        num_template = cfg[\"DATA\"][\"TEMPLATE\"][\"NUMBER\"],\n",
    "        decoder_type=cfg[\"MODEL\"][\"DECODER\"][\"TYPE\"],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def get_tracker_class():\n",
    "    return MCITRACK\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "407a71c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = {}\n",
    "\n",
    "# MODEL\n",
    "cfg[\"MODEL\"] = {}\n",
    "\n",
    "# MODEL.ENCODER\n",
    "cfg[\"MODEL\"][\"ENCODER\"] = {\n",
    "    \"TYPE\": \"dinov2_vitb14\",  # encoder model\n",
    "    \"DROP_PATH\": 0,\n",
    "    \"PRETRAIN_TYPE\": \"mae\",  # mae, default, or scratch. This parameter is not activated for dinov2.\n",
    "    \"USE_CHECKPOINT\": False,  # to save the memory.\n",
    "    \"STRIDE\": 14,\n",
    "    \"POS_TYPE\": 'interpolate',  # type of loading the positional encoding. \"interpolate\" or \"index\".\n",
    "    \"TOKEN_TYPE_INDICATE\": False,  # add a token_type_embedding to indicate the search, template_foreground, template_background\n",
    "    \"INTERACTION_INDEXES\": [[0, 6], [6, 12], [12, 18], [18, 24]],\n",
    "    \"GRAD_CKPT\": False\n",
    "}\n",
    "\n",
    "# MODEL.NECK\n",
    "cfg[\"MODEL\"][\"NECK\"] = {\n",
    "    \"N_LAYERS\": 4,\n",
    "    \"D_MODEL\": 512,\n",
    "    \"D_STATE\": 16  # MAMABA_HIDDEN_STATE\n",
    "}\n",
    "\n",
    "# MODEL.DECODER\n",
    "cfg[\"MODEL\"][\"DECODER\"] = {\n",
    "    \"TYPE\": \"CENTER\",  # MLP, CORNER, CENTER\n",
    "    \"NUM_CHANNELS\": 256\n",
    "}\n",
    "\n",
    "# TRAIN\n",
    "cfg[\"TRAIN\"] = {\n",
    "    \"LR\": 0.0001,\n",
    "    \"WEIGHT_DECAY\": 0.0001,\n",
    "    \"EPOCH\": 500,\n",
    "    \"LR_DROP_EPOCH\": 400,\n",
    "    \"BATCH_SIZE\": 8,\n",
    "    \"NUM_WORKER\": 8,\n",
    "    \"OPTIMIZER\": \"ADAMW\",\n",
    "    \"ENCODER_MULTIPLIER\": 0.1,  # encoder's LR = this factor * LR\n",
    "    \"FREEZE_ENCODER\": False,  # for freezing the parameters of encoder\n",
    "    \"ENCODER_OPEN\": [],  # only for debug, open some layers of encoder when FREEZE_ENCODER is True\n",
    "    \"CE_WEIGHT\": 1.0,  # weight for cross-entropy loss\n",
    "    \"GIOU_WEIGHT\": 2.0,\n",
    "    \"L1_WEIGHT\": 5.0,\n",
    "    \"PRINT_INTERVAL\": 50,  # interval to print the training log\n",
    "    \"GRAD_CLIP_NORM\": 0.1,\n",
    "    \"FIX_BN\": False,\n",
    "    \"ENCODER_W\": \"\",\n",
    "    \"TYPE\": \"normal\",  # normal, peft or fft\n",
    "    \"PRETRAINED_PATH\": None\n",
    "}\n",
    "\n",
    "# TRAIN.SCHEDULER\n",
    "cfg[\"TRAIN\"][\"SCHEDULER\"] = {\n",
    "    \"TYPE\": \"step\",\n",
    "    \"DECAY_RATE\": 0.1\n",
    "}\n",
    "\n",
    "# DATA\n",
    "cfg[\"DATA\"] = {\n",
    "    \"MEAN\": [0.485, 0.456, 0.406],\n",
    "    \"STD\": [0.229, 0.224, 0.225],\n",
    "    \"MAX_SAMPLE_INTERVAL\": 200,\n",
    "    \"SAMPLER_MODE\": \"order\",\n",
    "    \"LOADER\": \"tracking\"\n",
    "}\n",
    "\n",
    "# DATA.TRAIN\n",
    "cfg[\"DATA\"][\"TRAIN\"] = {\n",
    "    \"DATASETS_NAME\": [\"LASOT\", \"GOT10K_vottrain\"],\n",
    "    \"DATASETS_RATIO\": [1, 1],\n",
    "    \"SAMPLE_PER_EPOCH\": 60000\n",
    "}\n",
    "\n",
    "# DATA.SEARCH\n",
    "cfg[\"DATA\"][\"SEARCH\"] = {\n",
    "    \"NUMBER\": 1,  # number of search region, only support 1 for now.\n",
    "    \"SIZE\": 256,\n",
    "    \"FACTOR\": 4.0,\n",
    "    \"CENTER_JITTER\": 3.5,\n",
    "    \"SCALE_JITTER\": 0.5\n",
    "}\n",
    "\n",
    "# DATA.TEMPLATE\n",
    "cfg[\"DATA\"][\"TEMPLATE\"] = {\n",
    "    \"NUMBER\": 1,\n",
    "    \"SIZE\": 128,\n",
    "    \"FACTOR\": 2.0,\n",
    "    \"CENTER_JITTER\": 0,\n",
    "    \"SCALE_JITTER\": 0\n",
    "}\n",
    "\n",
    "# TEST\n",
    "cfg[\"TEST\"] = {\n",
    "    \"TEMPLATE_FACTOR\": 4.0,\n",
    "    \"TEMPLATE_SIZE\": 256,\n",
    "    \"SEARCH_FACTOR\": 2.0,\n",
    "    \"SEARCH_SIZE\": 128,\n",
    "    \"EPOCH\": 500,\n",
    "    \"WINDOW\": False,  # window penalty\n",
    "    \"NUM_TEMPLATES\": 1\n",
    "}\n",
    "\n",
    "# TEST.UPT\n",
    "cfg[\"TEST\"][\"UPT\"] = {\n",
    "    \"DEFAULT\": 1,\n",
    "    \"LASOT\": 0,\n",
    "    \"LASOT_EXTENSION_SUBSET\": 0,\n",
    "    \"TRACKINGNET\": 0,\n",
    "    \"TNL2K\": 0,\n",
    "    \"NFS\": 0,\n",
    "    \"UAV\": 0,\n",
    "    \"VOT20\": 0,\n",
    "    \"GOT10K_TEST\": 0\n",
    "}\n",
    "\n",
    "# TEST.UPH\n",
    "cfg[\"TEST\"][\"UPH\"] = {\n",
    "    \"DEFAULT\": 1,\n",
    "    \"LASOT\": 0,\n",
    "    \"LASOT_EXTENSION_SUBSET\": 0,\n",
    "    \"TRACKINGNET\": 0,\n",
    "    \"TNL2K\": 0,\n",
    "    \"NFS\": 0,\n",
    "    \"UAV\": 0,\n",
    "    \"VOT20\": 0,\n",
    "    \"GOT10K_TEST\": 0\n",
    "}\n",
    "\n",
    "# TEST.INTER\n",
    "cfg[\"TEST\"][\"INTER\"] = {\n",
    "    \"DEFAULT\": 999999,\n",
    "    \"LASOT\": 0,\n",
    "    \"LASOT_EXTENSION_SUBSET\": 0,\n",
    "    \"TRACKINGNET\": 0,\n",
    "    \"TNL2K\": 0,\n",
    "    \"NFS\": 0,\n",
    "    \"UAV\": 0,\n",
    "    \"VOT20\": 0,\n",
    "    \"GOT10K_TEST\": 0\n",
    "}\n",
    "\n",
    "# TEST.MB\n",
    "cfg[\"TEST\"][\"MB\"] = {\n",
    "    \"DEFAULT\": 500,\n",
    "    \"LASOT\": 0,\n",
    "    \"LASOT_EXTENSION_SUBSET\": 0,\n",
    "    \"TRACKINGNET\": 0,\n",
    "    \"TNL2K\": 0,\n",
    "    \"NFS\": 0,\n",
    "    \"UAV\": 0,\n",
    "    \"VOT20\": 0,\n",
    "    \"GOT10K_TEST\": 0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f2804bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test config:  {'MODEL': {'ENCODER': {'TYPE': 'fastitpnt', 'DROP_PATH': 0.1, 'PRETRAIN_TYPE': './fast_itpn_tiny_1600e_1k.pt', 'USE_CHECKPOINT': False, 'STRIDE': 16, 'POS_TYPE': 'index', 'TOKEN_TYPE_INDICATE': True, 'INTERACTION_INDEXES': [[4, 7], [7, 10], [10, 13], [13, 16]], 'GRAD_CKPT': False}, 'NECK': {'N_LAYERS': 4, 'D_MODEL': 384, 'D_STATE': 16}, 'DECODER': {'TYPE': 'CENTER', 'NUM_CHANNELS': 256}}, 'TRAIN': {'LR': 0.0004, 'WEIGHT_DECAY': 0.0001, 'EPOCH': 300, 'LR_DROP_EPOCH': 240, 'BATCH_SIZE': 64, 'NUM_WORKER': 10, 'OPTIMIZER': 'ADAMW', 'ENCODER_MULTIPLIER': 0.1, 'FREEZE_ENCODER': False, 'ENCODER_OPEN': [], 'CE_WEIGHT': 1.0, 'GIOU_WEIGHT': 2.0, 'L1_WEIGHT': 5.0, 'PRINT_INTERVAL': 50, 'GRAD_CLIP_NORM': 0.1, 'FIX_BN': False, 'ENCODER_W': '', 'TYPE': 'normal', 'PRETRAINED_PATH': None, 'SCHEDULER': {'TYPE': 'step', 'DECAY_RATE': 0.1}}, 'DATA': {'MEAN': [0.485, 0.456, 0.406], 'STD': [0.229, 0.224, 0.225], 'MAX_SAMPLE_INTERVAL': 400, 'SAMPLER_MODE': 'order', 'LOADER': 'tracking', 'TRAIN': {'DATASETS_NAME': ['LASOT', 'GOT10K_vottrain', 'COCO17', 'TRACKINGNET', 'VASTTRACK'], 'DATASETS_RATIO': [1, 1, 1, 1, 1], 'SAMPLE_PER_EPOCH': 60000}, 'SEARCH': {'NUMBER': 2, 'SIZE': 224, 'FACTOR': 4.0, 'CENTER_JITTER': 3.5, 'SCALE_JITTER': 0.5}, 'TEMPLATE': {'NUMBER': 5, 'SIZE': 112, 'FACTOR': 2.0, 'CENTER_JITTER': 0, 'SCALE_JITTER': 0}}, 'TEST': {'TEMPLATE_FACTOR': 2.0, 'TEMPLATE_SIZE': 112, 'SEARCH_FACTOR': 4.0, 'SEARCH_SIZE': 224, 'EPOCH': 300, 'WINDOW': True, 'NUM_TEMPLATES': 5, 'UPT': {'DEFAULT': 1, 'LASOT': 0.8, 'LASOT_EXTENSION_SUBSET': 0.85, 'TRACKINGNET': 0.5, 'TNL2K': 0.5, 'NFS': 0.8, 'UAV': 0.2, 'VOT20': 0.4, 'GOT10K_TEST': 0}, 'UPH': {'DEFAULT': 1, 'LASOT': 0.88, 'LASOT_EXTENSION_SUBSET': 0.97, 'TRACKINGNET': 0.9, 'TNL2K': 0.9, 'NFS': 0.92, 'UAV': 0.91, 'VOT20': 0.94, 'GOT10K_TEST': 0}, 'INTER': {'DEFAULT': 999999, 'LASOT': 70, 'LASOT_EXTENSION_SUBSET': 50, 'TRACKINGNET': 20, 'TNL2K': 20, 'NFS': 90, 'UAV': 1, 'VOT20': 1, 'GOT10K_TEST': 0}, 'MB': {'DEFAULT': 500, 'LASOT': 500, 'LASOT_EXTENSION_SUBSET': 500, 'TRACKINGNET': 200, 'TNL2K': 500, 'NFS': 500, 'UAV': 400, 'VOT20': 500, 'GOT10K_TEST': 0}}}\n"
     ]
    }
   ],
   "source": [
    "#Params\n",
    "class TrackerParams:\n",
    "    \"\"\"Class for tracker parameters.\"\"\"\n",
    "    def set_default_values(self, default_vals: dict):\n",
    "        for name, val in default_vals.items():\n",
    "            if not hasattr(self, name):\n",
    "                setattr(self, name, val)\n",
    "\n",
    "    def get(self, name: str, *default):\n",
    "        \"\"\"Get a parameter value with the given name. If it does not exists, it return the default value given as a\n",
    "        second argument or returns an error if no default value is given.\"\"\"\n",
    "        if len(default) > 1:\n",
    "            raise ValueError('Can only give one default value.')\n",
    "\n",
    "        if not default:\n",
    "            return getattr(self, name)\n",
    "\n",
    "        return getattr(self, name, default[0])\n",
    "\n",
    "    def has(self, name: str):\n",
    "        \"\"\"Check if there exist a parameter with the given name.\"\"\"\n",
    "        return hasattr(self, name)\n",
    "\n",
    "def _update_config(base_cfg, exp_cfg):\n",
    "    if isinstance(base_cfg, dict) and isinstance(exp_cfg, dict):\n",
    "        for k, v in exp_cfg.items():\n",
    "            if k in base_cfg:\n",
    "                if not isinstance(v, dict):\n",
    "                    base_cfg[k] = v\n",
    "                else:\n",
    "                    _update_config(base_cfg[k], v)\n",
    "            else:\n",
    "                raise ValueError(\"{} not exist in config.py\".format(k))\n",
    "    else:\n",
    "        return\n",
    "\n",
    "def update_config_from_file(filename):\n",
    "    exp_config = None\n",
    "    with open(filename) as f:\n",
    "        exp_config = yaml.safe_load(f)\n",
    "        _update_config(cfg, exp_config)\n",
    "    \n",
    "def parameters(yaml_name: str):\n",
    "    params = TrackerParams()\n",
    "\n",
    "    yaml_file = \"mcitrack_t224.yaml\"\n",
    "    update_config_from_file(yaml_file)\n",
    "    params.cfg = cfg\n",
    "    print(\"test config: \", cfg)\n",
    "\n",
    "    params.yaml_name = yaml_name\n",
    "    # template and search region\n",
    "    params.template_factor = cfg[\"TEST\"][\"TEMPLATE_FACTOR\"]\n",
    "    params.template_size = cfg[\"TEST\"][\"TEMPLATE_SIZE\"]\n",
    "    params.search_factor = cfg[\"TEST\"][\"SEARCH_FACTOR\"]\n",
    "    params.search_size = cfg[\"TEST\"][\"SEARCH_SIZE\"]\n",
    "\n",
    "    # Network checkpoint path\n",
    "    params.checkpoint = \"fast_itpn_tiny_1600e_1k.pt\"\n",
    "    # whether to save boxes from all queries\n",
    "    params.save_all_boxes = False\n",
    "\n",
    "    return params\n",
    "\n",
    "params = parameters(\"./mcitrack_t224.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae331c67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update threshold is:  500\n"
     ]
    }
   ],
   "source": [
    "treacker = MCITRACK(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097a57d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "network = build_mcitrack(params.cfg)\n",
    "network.load_state_dict(torch.load(\"MCITRACK_ep0300.pth.tar\", map_location='cpu')['net'], strict=True)\n",
    "cfg = params.cfg\n",
    "network = network.cuda()\n",
    "network.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64d5c940",
   "metadata": {},
   "outputs": [],
   "source": [
    "list1 = [torch.zeros(1, 3, 112, 112).to('cuda') for _ in range(5)]  # 5 тензоров размером [1, 3, 112, 112]\n",
    "list2 = [torch.zeros(1, 3, 224, 224).to('cuda')]                    # 1 тензор размером [1, 3, 224, 224]\n",
    "list3 = [torch.zeros(1, 4).to('cuda') for _ in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc77a871",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = network.forward(list1,list2,list3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7855da66",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in  res:\n",
    "    print(i)\n",
    "    print(res[i].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa6b8c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_17528\\1057810194.py:196: TracerWarning: Converting a tensor to a Python integer might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  x = x.transpose(1,2).view(B,C,int(N**0.5),int(N**0.5)).contiguous()\n"
     ]
    }
   ],
   "source": [
    "#ModelWrapper jit\n",
    "class ModelWrapper(torch.nn.Module):\n",
    "    def __init__(self, original_model):\n",
    "        super(ModelWrapper, self).__init__()\n",
    "        self.original_model = original_model\n",
    "\n",
    "    def forward(self, template_list, search_list,template_anno_list):\n",
    "        \n",
    "        output_dict = self.original_model(template_list, search_list,template_anno_list)\n",
    "        \n",
    "        \n",
    "        return (output_dict['pred_boxes'],\n",
    "                output_dict['score_map'],\n",
    "                output_dict['size_map'],\n",
    "                output_dict['offset_map'])\n",
    "\n",
    "\n",
    "model = network\n",
    "model.eval()\n",
    "\n",
    "\n",
    "wrapped_model = ModelWrapper(model)\n",
    "\n",
    "\n",
    "template_list = [torch.zeros(1, 3, 112, 112).to('cuda') for _ in range(5)]  # 5 тензоров размером [1, 3, 112, 112]\n",
    "search_list = [torch.zeros(1, 3, 224, 224).to('cuda')]                    # 1 тензор размером [1, 3, 224, 224]\n",
    "template_anno_list = [torch.zeros(1, 4).to('cuda') for _ in range(5)]\n",
    "\n",
    "traced_model = torch.jit.trace(wrapped_model, (template_list, search_list,template_anno_list))\n",
    "\n",
    "\n",
    "optimized_model = torch.jit.optimize_for_inference(traced_model)\n",
    "\n",
    "\n",
    "optimized_model.save(\"MCITrack.pt\")\n",
    "\n",
    "\n",
    "loaded_model = torch.jit.load(\"MCITrack.pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = loaded_model(template_list, search_list,template_anno_list)\n",
    "\n",
    "for output in outputs:\n",
    "    print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "99787837",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_16344\\2631578009.py:196: TracerWarning: Converting a tensor to a Python integer might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  x = x.transpose(1,2).view(B,C,int(N**0.5),int(N**0.5)).contiguous()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model has been exported to MCITrack.onnx\n"
     ]
    }
   ],
   "source": [
    "#ModelWrapper onnx\n",
    "import torch\n",
    "\n",
    "\n",
    "class ModelWrapper(torch.nn.Module):\n",
    "    def __init__(self, original_model):\n",
    "        super(ModelWrapper, self).__init__()\n",
    "        self.original_model = original_model\n",
    "\n",
    "    def forward(self, template_list, search_list,template_anno_list):\n",
    "        \n",
    "        output_dict = self.original_model(template_list, search_list,template_anno_list)\n",
    "        \n",
    "        \n",
    "        return (output_dict['pred_boxes'],\n",
    "                output_dict['score_map'],\n",
    "                output_dict['size_map'],\n",
    "                output_dict['offset_map'])\n",
    "\n",
    "\n",
    "\n",
    "model = network\n",
    "model.eval()\n",
    "\n",
    "\n",
    "wrapped_model = ModelWrapper(model)\n",
    "\n",
    "template_list = [torch.zeros(1, 3, 112, 112).to('cuda') for _ in range(5)]  # 5 тензоров размером [1, 3, 112, 112]\n",
    "search_list = [torch.zeros(1, 3, 224, 224).to('cuda')]                    # 1 тензор размером [1, 3, 224, 224]\n",
    "template_anno_list = [torch.zeros(1, 4).to('cuda') for _ in range(5)]\n",
    "\n",
    "# Важно: для onnx-модели модель должна быть на cpu или cuda, и входы должны быть на том же устройстве.\n",
    "wrapped_model = wrapped_model.to('cuda')\n",
    "wrapped_model.eval()\n",
    "\n",
    "# Указываем пути для сохранения\n",
    "onnx_path = \"MCITrack.onnx\"\n",
    "\n",
    "# Экспортируем модель в ONNX\n",
    "torch.onnx.export(\n",
    "    wrapped_model,                                   # Модель\n",
    "    (template_list, search_list,template_anno_list),                                # Входные данные (tuple)\n",
    "    onnx_path,                                       # Имя файла\n",
    "    export_params=True,                              # Экспортировать параметры (веса)\n",
    "    opset_version=17,                                # Версия ONNX opset\n",
    "    do_constant_folding=True,                        # Оптимизация констант\n",
    "    input_names = ['template_list', 'search_list','template_anno_list'],                        # Имена входов\n",
    "    output_names = ['pred_boxes','score_map','size_map','offset_map'],                   # Имена выходов\n",
    "    #dynamic_axes={'z': {0: 'batch_size'},            # Динамическая ось для батча\n",
    "    #              'x': {0: 'batch_size'},\n",
    "    #              'pred_boxes': {0: 'batch_size'}},\n",
    "    verbose=True                                     # Показывать подробности\n",
    ")\n",
    "\n",
    "print(f'Model has been exported to {onnx_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1666b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRT\n",
    "import tensorrt as trt\n",
    "#trtexec --onnx=MCITrack.onnx  --saveEngine=MCITrack.trt  --fp16\n",
    "\"\"\" \n",
    "\n",
    "def build_engine(onnx_path, trt_path):\n",
    "    logger = trt.Logger(trt.Logger.WARNING)\n",
    "    builder = trt.Builder(logger)\n",
    "    network = builder.create_network(1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH))\n",
    "    parser = trt.OnnxParser(network, logger)\n",
    "    \n",
    "    with open(onnx_path, 'rb') as model:\n",
    "        if not parser.parse(model.read()):\n",
    "            for error in range(parser.num_errors):\n",
    "                print(parser.get_error(error))\n",
    "            return None\n",
    "    \n",
    "    config = builder.create_builder_config()\n",
    "    config.max_workspace_size = 1 << 30  # 1GB\n",
    "    engine = builder.build_engine(network, config)\n",
    "    \n",
    "    with open(trt_path, 'wb') as f:\n",
    "        f.write(engine.serialize())\n",
    "    \n",
    "    return engine  \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ae7ecd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Трекинг по видео\n",
    "file = \"0516.mp4\"\n",
    "video = cv2.VideoCapture(file)\n",
    "#fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "#fps=video.get(cv2.CAP_PROP_FPS)\n",
    "#video_vriter = cv2.VideoWriter(file.split('.')[0]+\"_\"+\".avi\", fourcc, fps, (1920, 1080))\n",
    "\n",
    "\n",
    "ok, image = video.read()\n",
    "if not video.isOpened():\n",
    "    print(\"Could not open video\")\n",
    "    sys.exit()\n",
    "    \n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "x, y, w, h = cv2.selectROI( image, fromCenter=False)\n",
    "init_state = [x, y, w, h]\n",
    "def _build_init_info(box):\n",
    "            return {'init_bbox': box}\n",
    "treacker.initialize(image, _build_init_info(init_state))\n",
    "counter = 0\n",
    "while True:\n",
    "            ok, image = video.read()\n",
    "            if not ok:\n",
    "                print(\"Can't read frame\")\n",
    "                break\n",
    "\n",
    "            \n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            start = time.time() \n",
    "            out  = treacker.track(image)\n",
    "            state = [int(s) for s in out['target_bbox']]\n",
    "            best_score=out[\"best_score\"].cpu().numpy()[0][0]\n",
    "            end_time = (time.time() - start)\n",
    "            \n",
    "            \n",
    "            org = (50, 50)\n",
    "\n",
    "            # fontScale\n",
    "            fontScale = 1\n",
    "            font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "            # Blue color in BGR\n",
    "            color = (255, 0, 0)\n",
    "            # Line thickness of 2 px\n",
    "            thickness = 2              \n",
    "            # Using cv2.putText() method\n",
    "            image = cv2.putText(image, str(best_score), org, font, \n",
    "                            fontScale, color, thickness, cv2.LINE_AA)\n",
    "            image = cv2.putText(image, str(end_time), (50,100), font, \n",
    "                            fontScale, color, thickness, cv2.LINE_AA)\n",
    "\n",
    "            x, y, w, h = [int(x) for x in state]\n",
    "\n",
    "            color = (0, 0, 255)  # Цвет в формате BGR\n",
    "            cv2.rectangle(image, (x, y), (x + w, y + h), color, 2)\n",
    "\n",
    "\n",
    "            cv2.imshow(\"tracking\", image)\n",
    "            #video_vriter.write(image)\n",
    "\n",
    "\n",
    "            k = cv2.waitKey(1)            \n",
    "            if k == 32:  # SPACE\n",
    "                ok, image = video.read()                             \n",
    "                x, y, w, h = cv2.selectROI( image, fromCenter=False)\n",
    "                init_state = [x, y, w, h]\n",
    "                treacker.initialize(image, _build_init_info(init_state))\n",
    "            if k == 27:  # ESC\n",
    "                break\n",
    "        \n",
    "                \n",
    "                \n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "#video.release()\n",
    "#video_vriter.release()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab1d032c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Метрики\n",
    "import numpy as np\n",
    "\n",
    "def iou(boxA, boxB):\n",
    "    # boxA, boxB: [x, y, w, h]\n",
    "    xA = max(boxA[0], boxB[0])\n",
    "    yA = max(boxA[1], boxB[1])\n",
    "    xB = min(boxA[0] + boxA[2], boxB[0] + boxB[2])\n",
    "    yB = min(boxA[1] + boxA[3], boxB[1] + boxB[3])\n",
    "\n",
    "    interW = max(0, xB - xA)\n",
    "    interH = max(0, yB - yA)\n",
    "    interArea = interW * interH\n",
    "\n",
    "    boxAArea = boxA[2] * boxA[3]\n",
    "    boxBArea = boxB[2] * boxB[3]\n",
    "    unionArea = boxAArea + boxBArea - interArea\n",
    "\n",
    "    if unionArea == 0:\n",
    "        return 0.0\n",
    "    return interArea / unionArea\n",
    "\n",
    "def precision(boxA, boxB):\n",
    "    # центры bbox\n",
    "    centerA = (boxA[0] + boxA[2]/2, boxA[1] + boxA[3]/2)\n",
    "    centerB = (boxB[0] + boxB[2]/2, boxB[1] + boxB[3]/2)\n",
    "    dist = np.sqrt((centerA[0] - centerB[0])**2 + (centerA[1] - centerB[1])**2)\n",
    "    return dist\n",
    "sr_thresh = 0.5\n",
    "prec_thresh = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c2b0fbe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FPS: 28.16\n",
      "Average Overlap (AO): 0.92\n",
      "Success Rate (SR@0.5): 1.00\n",
      "Precision @20px: 0.99\n"
     ]
    }
   ],
   "source": [
    "#Трекинг got10k с метриками\n",
    "import glob\n",
    "import time\n",
    "import  os\n",
    "gt_bboxes = []\n",
    "pred_bboxes = []\n",
    "seq_path = \"val/GOT-10k_Val_000006\"\n",
    "txt_files = glob.glob(os.path.join(seq_path, '*.txt'))\n",
    "if not txt_files:\n",
    "    raise FileNotFoundError(f\"No .txt files found in {seq_path}\")\n",
    "\n",
    "img_files = sorted(glob.glob(os.path.join(seq_path, '*.jpg')))\n",
    "with open(txt_files[0], 'r') as f:\n",
    "    gt_bboxes = [list(map(float, line.strip().split(','))) for line in f]\n",
    "\n",
    "# Получаем размер первого изображения\n",
    "sample_img = cv2.imread(img_files[0])\n",
    "if sample_img is None:\n",
    "    raise ValueError(f\"Failed to read sample image: {img_files[0]}\")\n",
    "\n",
    "#height, width = sample_img.shape[:2]\n",
    "#fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "#output_filename = f\"{seq_path.split('/')[-1]}_output.avi\"\n",
    "#video_vriter = cv2.VideoWriter(output_filename, fourcc, 10, (width, height))  \n",
    "\n",
    "assert len(img_files) == len(gt_bboxes), \"Количество кадров и bbox'ов не совпадает\"\n",
    "\n",
    "x, y, w, h = map(int, gt_bboxes[0])\n",
    "init_state = [x, y, w, h]\n",
    "\n",
    "def _build_init_info(box):\n",
    "            return {'init_bbox': box}\n",
    "\n",
    "counter = 0\n",
    "\n",
    "\n",
    "treacker.initialize(sample_img, _build_init_info(init_state))\n",
    "\n",
    "start_time = time.time()  # Начало замера\n",
    "\n",
    "for img_file, bbox in zip(img_files, gt_bboxes):\n",
    "        \n",
    "        # Читаем изображение\n",
    "        img = cv2.imread(img_file)\n",
    "        if img is None:\n",
    "            print(f\"Не удалось загрузить изображение: {img_file}\")\n",
    "            continue\n",
    "        \n",
    "        \n",
    "        out  = treacker.track(img)\n",
    "        state = [int(s) for s in out['target_bbox']]   \n",
    "                           \n",
    "        # Рисуем bounding box        \n",
    "        x, y, w, h = [int(x) for x in state]\n",
    "\n",
    "        cv2.rectangle(img, (x, y), (x + w, y + h), (0, 0, 200), 2)\n",
    "        \n",
    "        x1, y1, w1, h1 = map(int, bbox)\n",
    "        cv2.rectangle(img, (x1, y1), (x1+w1, y1+h1), (0, 200, 0), 2)\n",
    "        bbox_pred = x, y, w, h\n",
    "        \n",
    "        gt_bboxes.append(bbox)\n",
    "        pred_bboxes.append(bbox_pred)\n",
    "\n",
    "        #cv2.imshow(seq_path, img)\n",
    "        #video_vriter.write(img)\n",
    "        counter+=1\n",
    "\n",
    "\n",
    "        # Выход по нажатию 'q' или ESC\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "        if key == ord('q') or key == 27:\n",
    "            break\n",
    "       \n",
    "        \n",
    "                \n",
    "end_time = time.time()    # Конец замера    \n",
    "total_frames = counter       # Общее количество обработанных кадров\n",
    "total_time = end_time - start_time\n",
    "fps = total_frames / total_time\n",
    "ious = [iou(gt, pred) for gt, pred in zip(gt_bboxes, pred_bboxes)]\n",
    "ao = np.mean(ious)\n",
    "sr = np.mean([1 if val >= sr_thresh else 0 for val in ious])\n",
    "precisions = [precision(gt, pred) for gt, pred in zip(gt_bboxes, pred_bboxes)]\n",
    "prec = np.mean([1 if d <= prec_thresh else 0 for d in precisions])\n",
    "\n",
    "print(f\"FPS: {fps:.2f}\")\n",
    "print(f'Average Overlap (AO): {ao:.2f}')\n",
    "print(f'Success Rate (SR@0.5): {sr:.2f}')\n",
    "print(f'Precision @20px: {prec:.2f}')\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "#video_vriter.release()\n",
    "#print(f\"Video saved as: {output_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fde457f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA доступна: True\n",
      "Версия CUDA (PyTorch): 12.8\n",
      "PyTorch version: 2.7.1+cu128\n",
      "TensorRT версия: 10.11.0.33\n",
      "ONNX версия: 1.18.0\n",
      "cuDNN включён в PyTorch: True\n",
      "Версия cuDNN (из PyTorch): 90701\n",
      "Версия onnxruntime: 1.20.1\n",
      "Device onnxruntime: GPU\n",
      "tensorflow не установлен\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(f\"CUDA доступна: {torch.cuda.is_available()}\")\n",
    "print(f\"Версия CUDA (PyTorch): {torch.version.cuda}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")    \n",
    "try:\n",
    "    import tensorrt as trt\n",
    "    print(f\"TensorRT версия: {trt.__version__}\")\n",
    "except:\n",
    "    print(\"TensorRT не установлен\")\n",
    "    \n",
    "try:\n",
    "    import onnx\n",
    "    print(f\"ONNX версия: {onnx.__version__}\")\n",
    "except:\n",
    "    print(\"ONNX не установлен\") \n",
    "print(f\"cuDNN включён в PyTorch: {torch.backends.cudnn.enabled}\")\n",
    "print(f\"Версия cuDNN (из PyTorch): {torch.backends.cudnn.version()}\")\n",
    "try:\n",
    "    import onnxruntime as ort\n",
    "    print(f\"Версия onnxruntime: {ort.__version__}\")\n",
    "    print(f\"Device onnxruntime: {ort.get_device()}\")  # Должно вернуть 'GPU'\n",
    "except:\n",
    "    print(\"onnxruntime не установлен\")\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    print(f\"Версия tensorflow: {tf.__version__}\")\n",
    "except:\n",
    "    print(\"tensorflow не установлен\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9756baf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pytorch.org/get-started/locally/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f63014f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Доступные провайдеры: ['TensorrtExecutionProvider', 'CUDAExecutionProvider', 'CPUExecutionProvider']\n",
      "Используется: ['CPUExecutionProvider']\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime as ort\n",
    "\n",
    "# Проверка провайдеров\n",
    "print(\"Доступные провайдеры:\", ort.get_available_providers())\n",
    "\n",
    "# Создание сессии с TensorRT\n",
    "try:\n",
    "    sess = ort.InferenceSession(\n",
    "        \"MCITrack.onnx\",\n",
    "        providers=['CUDAExecutionProvider'],\n",
    "        provider_options = [{}],\n",
    "        )\n",
    "    print(\"Используется:\", sess.get_providers())\n",
    "except Exception as e:\n",
    "    print(\"Ошибка:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6b41ae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Доступно GPU: 1\n",
      "Название GPU: NVIDIA GeForce RTX 3060 Ti\n",
      "Вычислительная способность: (8, 6)\n",
      "Общая память: 8191.50 МБ\n",
      "Контекст GPU активен: True\n"
     ]
    }
   ],
   "source": [
    "import pycuda.driver as cuda\n",
    "import pycuda.autoinit  # Автоматически инициализирует GPU\n",
    "\n",
    "# Проверка количества устройств\n",
    "print(f\"Доступно GPU: {cuda.Device.count()}\")\n",
    "\n",
    "# Информация о GPU\n",
    "gpu = cuda.Device(0)\n",
    "print(f\"Название GPU: {gpu.name()}\")\n",
    "print(f\"Вычислительная способность: {gpu.compute_capability()}\")\n",
    "print(f\"Общая память: {gpu.total_memory() / 1024**2:.2f} МБ\")\n",
    "\n",
    "# Проверка контекста (должен быть создан pycuda.autoinit)\n",
    "ctx = cuda.Context.get_current()\n",
    "print(f\"Контекст GPU активен: {ctx is not None}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
