{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "429c4397",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\piuta_en\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\piuta_en\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\timm\\models\\layers\\__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import yaml\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils import checkpoint\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import cv2\n",
    "import sys\n",
    "import numpy as np\n",
    "import fastitpn as fastitpn_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4bde567b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBase(nn.Module):\n",
    "\n",
    "    def __init__(self, encoder: nn.Module, train_encoder: bool, open_layers: list, num_channels: int):\n",
    "        super().__init__()\n",
    "        open_blocks = open_layers[2:]\n",
    "        open_items = open_layers[0:2]\n",
    "        for name, parameter in encoder.named_parameters():\n",
    "\n",
    "            if not train_encoder:\n",
    "                freeze = True\n",
    "                for open_block in open_blocks:\n",
    "                    if open_block in name:\n",
    "                        freeze = False\n",
    "                if name in open_items:\n",
    "                    freeze = False\n",
    "                if freeze == True:\n",
    "                    parameter.requires_grad_(False)  # here should allow users to specify which layers to freeze !\n",
    "\n",
    "        self.body = encoder\n",
    "        self.num_channels = num_channels\n",
    "\n",
    "    def forward(self, template_list, search_list, template_anno_list):\n",
    "        xs = self.body(template_list, search_list, template_anno_list)\n",
    "        return xs\n",
    "\n",
    "\n",
    "#fast_itpn_tiny_1600e_1k\n",
    "\n",
    "class Encoder(EncoderBase):\n",
    "    \"\"\"FastITPN encoder.\"\"\"\n",
    "    def __init__(self, name: str,\n",
    "                 train_encoder: bool,\n",
    "                 pretrain_type: str,\n",
    "                 search_size: int,\n",
    "                 search_number: int,\n",
    "                 template_size: int,\n",
    "                 template_number: int,\n",
    "                 open_layers: list,\n",
    "                 cfg=None):\n",
    "        if \"fastitpn\" in name.lower():\n",
    "            encoder = getattr(fastitpn_module, name)(\n",
    "                pretrained=True,\n",
    "                search_size=search_size,\n",
    "                template_size=template_size,\n",
    "                drop_rate=0.0,\n",
    "                drop_path_rate=0.1,\n",
    "                attn_drop_rate=0.0,\n",
    "                init_values=0.1,\n",
    "                drop_block_rate=None,\n",
    "                use_mean_pooling=True,\n",
    "                grad_ckpt=cfg[\"MODEL\"][\"ENCODER\"][\"GRAD_CKPT\"],\n",
    "                pos_type=cfg[\"MODEL\"][\"ENCODER\"][\"POS_TYPE\"],\n",
    "                token_type_indicate=cfg[\"MODEL\"][\"ENCODER\"][\"TOKEN_TYPE_INDICATE\"],\n",
    "                pretrain_type = cfg[\"MODEL\"][\"ENCODER\"][\"PRETRAIN_TYPE\"],\n",
    "            )\n",
    "            if \"itpnb\" in name:\n",
    "                num_channels = 512\n",
    "            elif \"itpnl\" in name:\n",
    "                num_channels = 768\n",
    "            elif \"itpnt\" in name:\n",
    "                num_channels = 384\n",
    "            elif \"itpns\" in name:\n",
    "                num_channels = 384\n",
    "            else:\n",
    "                num_channels = 512\n",
    "        else:\n",
    "            raise ValueError()\n",
    "        super().__init__(encoder, train_encoder, open_layers, num_channels)\n",
    "\n",
    "def build_encoder(cfg):\n",
    "    train_encoder = (cfg[\"TRAIN\"][\"ENCODER_MULTIPLIER\"] > 0) and (cfg[\"TRAIN\"][\"FREEZE_ENCODER\"] == False)\n",
    "    encoder = Encoder(cfg[\"MODEL\"][\"ENCODER\"][\"TYPE\"], train_encoder,\n",
    "                      cfg[\"MODEL\"][\"ENCODER\"][\"PRETRAIN_TYPE\"],\n",
    "                      cfg[\"DATA\"][\"SEARCH\"][\"SIZE\"], cfg[\"DATA\"][\"SEARCH\"][\"NUMBER\"],\n",
    "                      cfg[\"DATA\"][\"TEMPLATE\"][\"SIZE\"], cfg[\"DATA\"][\"TEMPLATE\"][\"NUMBER\"],\n",
    "                      cfg[\"TRAIN\"][\"ENCODER_OPEN\"], cfg)\n",
    "    return encoder\n",
    "\n",
    "class MambaBlock(nn.Module):\n",
    "    def __init__(self,dt_scale, d_model,d_inner,dt_rank,d_state,bias,d_conv,conv_bias,dt_init,dt_max,dt_min,dt_init_floor):\n",
    "        super().__init__()\n",
    "        #  projects block input from D to 2*ED (two branches)\n",
    "        self.dt_scale = dt_scale\n",
    "        self.d_model = d_model\n",
    "        self.d_inner = d_inner\n",
    "        self.dt_rank = dt_rank\n",
    "        self.d_state = d_state\n",
    "        self.in_proj = nn.Linear(self.d_model, 2 * self.d_inner, bias=bias)\n",
    "\n",
    "        self.conv1d = nn.Conv1d(in_channels=self.d_inner, out_channels=self.d_inner,\n",
    "                                kernel_size=d_conv, bias=conv_bias,\n",
    "                                groups=self.d_inner,\n",
    "                                padding=(d_conv - 1)//2)\n",
    "\n",
    "        #  projects x to input-dependent Δ, B, C\n",
    "        self.x_proj = nn.Linear(self.d_inner, self.dt_rank + 2 * self.d_state, bias=False)\n",
    "\n",
    "        #  projects Δ from dt_rank to d_inner\n",
    "        self.dt_proj = nn.Linear(self.dt_rank, self.d_inner, bias=True)\n",
    "\n",
    "        #  dt initialization\n",
    "        #  dt weights\n",
    "        dt_init_std = self.dt_rank ** -0.5 * self.dt_scale\n",
    "        if dt_init == \"constant\":\n",
    "            nn.init.constant_(self.dt_proj.weight, dt_init_std)\n",
    "        elif dt_init == \"random\":\n",
    "            nn.init.uniform_(self.dt_proj.weight, -dt_init_std, dt_init_std)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        # dt bias\n",
    "        dt = torch.exp(\n",
    "            torch.rand(self.d_inner) * (math.log(dt_max) - math.log(dt_min)) + math.log(dt_min)\n",
    "        ).clamp(min=dt_init_floor)\n",
    "        inv_dt = dt + torch.log(\n",
    "            -torch.expm1(-dt))  #  inverse of softplus: https://github.com/pytorch/pytorch/issues/72759\n",
    "        with torch.no_grad():\n",
    "            self.dt_proj.bias.copy_(inv_dt)\n",
    "        # self.dt_proj.bias._no_reinit = True # initialization would set all Linear.bias to zero, need to mark this one as _no_reinit\n",
    "        #  todo : explain why removed\n",
    "\n",
    "        # S4D real initialization\n",
    "        A = torch.arange(1, self.d_state + 1, dtype=torch.float32).repeat(self.d_inner, 1)\n",
    "        self.A_log = nn.Parameter(\n",
    "            torch.log(A))  # why store A in log ? to keep A < 0 (cf -torch.exp(...)) ? for gradient stability ?\n",
    "        self.D = nn.Parameter(torch.ones(self.d_inner))\n",
    "\n",
    "        #  projects block output from ED back to D\n",
    "        self.out_proj = nn.Linear(self.d_inner, self.d_model, bias=bias)\n",
    "    def forward(self, x, h):\n",
    "        #  x : (B,L, D)\n",
    "        # h : (B,L, ED, N)\n",
    "\n",
    "        #  y : (B, L, D)\n",
    "\n",
    "\n",
    "        xz = self.in_proj(x)  # (B, L,2*ED)\n",
    "        x, z = xz.chunk(2, dim=-1)  #  (B,L, ED), (B,L, ED)\n",
    "        x_cache = x.permute(0,2,1)#(B, ED,L)\n",
    "\n",
    "        #  x branch\n",
    "        x = self.conv1d( x_cache).permute(0,2,1) #  (B,L , ED)\n",
    "\n",
    "        x = F.silu(x)\n",
    "        y, h = self.ssm_step(x, h)\n",
    "        #y->B,L,ED;h->B,L,ED,N\n",
    "\n",
    "        #  z branch\n",
    "        z = F.silu(z)\n",
    "\n",
    "        output = y * z\n",
    "        output = self.out_proj(output)  #  (B, L, D)\n",
    "\n",
    "        return output, h\n",
    "\n",
    "    def ssm_step(self, x, h):\n",
    "        #  x : (B, L, ED)\n",
    "        #  h : (B, L, ED, N)\n",
    "\n",
    "        A = -torch.exp(\n",
    "            self.A_log.float())  # (ED, N) # todo : ne pas le faire tout le temps, puisque c'est indépendant de la timestep\n",
    "        D = self.D.float()\n",
    "        #  TODO remove .float()\n",
    "\n",
    "        deltaBC = self.x_proj(x)  #  (B, L, dt_rank+2*N)\n",
    "\n",
    "        delta, B, C = torch.split(deltaBC, [self.dt_rank, self.d_state, self.d_state],\n",
    "                                  dim=-1)  #  (B, L,dt_rank), (B, L, N), (B, L, N)\n",
    "        delta = F.softplus(self.dt_proj(delta))  #  (B, L, ED)\n",
    "\n",
    "        deltaA = torch.exp(delta.unsqueeze(-1) * A)  #  (B,L, ED, N)\n",
    "        deltaB = delta.unsqueeze(-1) * B.unsqueeze(2)  #  (B,L, ED, N)\n",
    "\n",
    "        BX = deltaB * (x.unsqueeze(-1))  #  (B, L,ED, N)\n",
    "\n",
    "        if h is None:\n",
    "            h = torch.zeros(x.size(0), x.size(1), self.d_inner, self.d_state, device=deltaA.device)  #  (B, L, ED, N)\n",
    "\n",
    "        h = deltaA * h + BX  #  (B, L, ED, N)\n",
    "\n",
    "        y = (h @ C.unsqueeze(-1)).squeeze(3)  #  (B, L, ED, N) @ (B, L, N, 1) -> (B, L, ED, 1)\n",
    "\n",
    "        y = y + D * x#B,L,ED\n",
    "\n",
    "        #  todo : pq h.squeeze(1) ??\n",
    "        return y, h\n",
    "    \n",
    "class DWConv(nn.Module):\n",
    "    def __init__(self, dim=768):\n",
    "        super().__init__()\n",
    "        self.dwconv = nn.Conv2d(dim, dim, 3, 1, 1, bias=True, groups=dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(1,0,2)\n",
    "        B, N, C = x.shape\n",
    "        x = x.transpose(1,2).view(B,C,int(N**0.5),int(N**0.5)).contiguous()\n",
    "        x = self.dwconv(x).flatten(2).transpose(1, 2)#B,N,C\n",
    "        x = x.permute(1,0,2)\n",
    "        return x\n",
    "class ConvFFN(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None,\n",
    "                 act_layer=nn.GELU, drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.dwconv = DWConv(hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.dwconv(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "    \n",
    "class ConvFFN(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None,\n",
    "                 act_layer=nn.GELU, drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.dwconv = DWConv(hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.dwconv(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "def drop_path(x, drop_prob: float = 0., training: bool = False, scale_by_keep: bool = True):\n",
    "\n",
    "    if drop_prob == 0. or not training:\n",
    "        return x\n",
    "    keep_prob = 1 - drop_prob\n",
    "    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n",
    "    random_tensor = x.new_empty(shape).bernoulli_(keep_prob)\n",
    "    if keep_prob > 0.0 and scale_by_keep:\n",
    "        random_tensor.div_(keep_prob)\n",
    "    return x * random_tensor\n",
    "\n",
    "class DropPath(nn.Module):\n",
    "   \n",
    "    def __init__(self, drop_prob: float = 0., scale_by_keep: bool = True):\n",
    "        super(DropPath, self).__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.scale_by_keep = scale_by_keep\n",
    "\n",
    "    def forward(self, x):\n",
    "        return drop_path(x, self.drop_prob, self.training, self.scale_by_keep)\n",
    "\n",
    "    def extra_repr(self):\n",
    "        return f'drop_prob={round(self.drop_prob,3):0.3f}'\n",
    "    \n",
    "\n",
    "class Extractor(nn.Module):\n",
    "    def __init__(self, d_model, num_heads=8, dropout=0.1, drop_path=0.1,\n",
    "                 norm_layer=lambda x: nn.LayerNorm(x, eps=1e-6)):  # Замена partial на лямбду\n",
    "        super().__init__()\n",
    "        self.query_norm = norm_layer(d_model)\n",
    "        self.feat_norm = norm_layer(d_model)\n",
    "        self.attn = nn.MultiheadAttention(d_model, num_heads, dropout=dropout)\n",
    "        # convffn\n",
    "        self.ffn = ConvFFN(in_features=d_model, hidden_features=int(d_model * 0.25), drop=0.)\n",
    "        self.ffn_norm = norm_layer(d_model)\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "\n",
    "    def forward(self, query, feat):\n",
    "\n",
    "        def _inner_forward(query, feat):\n",
    "            # query:l,b,d;feat:l,b,d\n",
    "            attn = self.attn(self.query_norm(query),\n",
    "                             self.feat_norm(feat), self.feat_norm(feat))[0]\n",
    "            query = query + attn\n",
    "\n",
    "            query = query + self.drop_path(self.ffn(self.ffn_norm(query)))\n",
    "            return query\n",
    "\n",
    "        query = _inner_forward(query, feat)\n",
    "\n",
    "        return query\n",
    " \n",
    "class Injector(nn.Module):\n",
    "    def __init__(self, d_model, n_heads=8, norm_layer=lambda x: nn.LayerNorm(x, eps=1e-6), dropout=0.1, init_values=0.):\n",
    "        super().__init__()\n",
    "        self.query_norm = norm_layer(d_model)\n",
    "        self.feat_norm = norm_layer(d_model)\n",
    "        self.attn = nn.MultiheadAttention(d_model, n_heads, dropout=dropout)\n",
    "        self.gamma = nn.Parameter(init_values * torch.ones((d_model)), requires_grad=True)\n",
    "        \n",
    "    def forward(self, query,feat):\n",
    "            #query:l,b,d;feat:l,b,d\n",
    "        def _inner_forward(query, feat):\n",
    "\n",
    "            attn = self.attn(self.query_norm(query),\n",
    "                             self.feat_norm(feat),self.feat_norm(feat))[0]\n",
    "            return query + self.gamma * attn\n",
    "        query = _inner_forward(query, feat)\n",
    "        return query    \n",
    "    \n",
    "    \n",
    "\n",
    "class InteractionBlock(nn.Module):\n",
    "    def __init__(self, d_model, extra_extractor, grad_ckpt):\n",
    "        super().__init__()\n",
    "        self.grad_ckpt = grad_ckpt\n",
    "        self.injector = Injector(d_model=d_model)\n",
    "        self.extractor = Extractor(d_model=d_model)\n",
    "        if extra_extractor:\n",
    "            self.extra_extractors = nn.Sequential(*[\n",
    "                Extractor(d_model=d_model)\n",
    "                for _ in range(2)])\n",
    "        else:\n",
    "            self.extra_extractors = None\n",
    "\n",
    "    def forward(self,x,xs,blocks):\n",
    "        x = self.injector(x.permute(1,0,2),xs.permute(1,0,2)).permute(1,0,2)\n",
    "        for idx,blk in enumerate(blocks):\n",
    "            x = checkpoint.checkpoint(blk, x, None,use_reentrant=False) if self.grad_ckpt else blk(x,None)\n",
    "        xs = checkpoint.checkpoint(self.extractor, xs.permute(1,0,2),x.permute(1,0,2),use_reentrant=False).permute(1,0,2) \\\n",
    "            if self.grad_ckpt else self.extractor(xs.permute(1, 0, 2), x.permute(1, 0, 2)).permute(1, 0, 2)  # b,n,c\n",
    "        # xs = self.extractor(xs.permute(1,0,2),x.permute(1,0,2)).permute(1,0,2)#b,n,c\n",
    "        if self.extra_extractors is not None:\n",
    "            for extractor in self.extra_extractors:\n",
    "                xs = checkpoint.checkpoint(extractor, xs.permute(1, 0, 2), x.permute(1, 0, 2), use_reentrant=False).permute(1, 0, 2) \\\n",
    "                    if self.grad_ckpt else extractor(xs.permute(1, 0, 2), x.permute(1, 0, 2)).permute(1, 0,2)  # b,n,c\n",
    "                # xs = extractor(xs.permute(1,0,2),x.permute(1,0,2)).permute(1,0,2)\n",
    "        return x,xs\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, d_model: int, eps: float = 1e-5):\n",
    "        super().__init__()\n",
    "\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(d_model))\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps) * self.weight\n",
    "\n",
    "        return output\n",
    "    \n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self,dt_scale, d_model,d_inner,dt_rank,d_state,bias,d_conv,conv_bias,dt_init,dt_max,dt_min,dt_init_floor,grad_ckpt):\n",
    "        super().__init__()\n",
    "\n",
    "        self.grad_ckpt = grad_ckpt\n",
    "        self.mixer = MambaBlock(dt_scale,d_model,d_inner,dt_rank,d_state,bias,d_conv,conv_bias,dt_init,dt_max,dt_min,dt_init_floor)\n",
    "        self.norm = RMSNorm(d_model)\n",
    "\n",
    "    def forward(self, x, h):\n",
    "        #  x : (B, L, D)\n",
    "        # h : (B, L, ED, N)\n",
    "        #  output : (B,L, D)\n",
    "\n",
    "        x = self.norm(x)\n",
    "        output, h = checkpoint.checkpoint(self.mixer,x,h,use_reentrant=False) if self.grad_ckpt else self.mixer(x, h)\n",
    "        output = output + x\n",
    "        return output, h\n",
    "    \n",
    "class Mamba_Neck(nn.Module):\n",
    "    def __init__(self, in_channel=512,d_model=512,d_inner=1024,bias=False,n_layers=4,dt_rank=32,d_state=16,d_conv=3,dt_min=0.001,\n",
    "                 dt_max=0.1,dt_init='random',dt_scale=1.0,conv_bias=True,dt_init_floor=0.0001,grad_ckpt=False):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_inner = d_inner\n",
    "        self.bias = bias\n",
    "        self.dt_rank = dt_rank\n",
    "        self.d_state = d_state\n",
    "        self.dt_scale = dt_scale\n",
    "        self.num_channels = self.d_model\n",
    "        self.layers = nn.ModuleList(\n",
    "            [ResidualBlock(dt_scale,d_model,d_inner,dt_rank,d_state,bias,d_conv,conv_bias,dt_init,dt_max,dt_min,dt_init_floor,grad_ckpt)\n",
    "             for _ in range(n_layers)])\n",
    "        self.interactions = nn.ModuleList([\n",
    "            InteractionBlock(d_model=d_model,extra_extractor=(True if i == n_layers - 1 else False),grad_ckpt=grad_ckpt)\n",
    "            for i in range(n_layers)\n",
    "        ])\n",
    "        # self.norm_f = RMSNorm(config.d_model)\n",
    "\n",
    "    def forward(self, x,xs,h,blocks,interaction_indexes):\n",
    "        #  x : (B, L, D)\n",
    "        #  caches : [cache(layer) for all layers], cache : (h, inputs)\n",
    "\n",
    "        #  y : (B, L, D)\n",
    "        #  caches : [cache(layer) for all layers], cache : (h, inputs)\n",
    "        for i,index in enumerate(interaction_indexes):\n",
    "            xs, h[i] = self.layers[i](xs, h[i])\n",
    "            x,xs = self.interactions[i](x,xs,blocks[index[0]:index[1]])\n",
    "\n",
    "        return x, xs, h\n",
    "def build_neck(cfg,encoder):\n",
    "    in_channel = encoder.num_channels\n",
    "    d_model = cfg[\"MODEL\"][\"NECK\"][\"D_MODEL\"]\n",
    "    n_layers = cfg[\"MODEL\"][\"NECK\"][\"N_LAYERS\"]\n",
    "    d_state = cfg[\"MODEL\"][\"NECK\"][\"D_STATE\"]\n",
    "    grad_ckpt = cfg[\"MODEL\"][\"ENCODER\"][\"GRAD_CKPT\"]\n",
    "    neck = Mamba_Neck(in_channel=in_channel,d_model=d_model,d_inner=2*d_model,n_layers=n_layers,dt_rank=d_model//16,d_state=d_state,grad_ckpt=grad_ckpt)\n",
    "    return neck\n",
    "\n",
    "def box_xyxy_to_cxcywh(x):\n",
    "    x0, y0, x1, y1 = x.unbind(-1)\n",
    "    b = [(x0 + x1) / 2, (y0 + y1) / 2,\n",
    "         (x1 - x0), (y1 - y0)]\n",
    "    return torch.stack(b, dim=-1)\n",
    "\n",
    "class MLPPredictor(nn.Module):\n",
    "    def __init__(self, inplanes=64, channel=256, feat_sz=20, stride=16):\n",
    "        super(MLPPredictor, self).__init__()\n",
    "        self.feat_sz = feat_sz\n",
    "        self.stride = stride\n",
    "        self.img_sz = self.feat_sz * self.stride\n",
    "\n",
    "        self.num_layers = 3\n",
    "        h = [channel] * (self.num_layers - 1)\n",
    "        self.layers_cls = nn.ModuleList(nn.Linear(n, k)\n",
    "                                        for n, k in zip([inplanes] + h, h + [1]))\n",
    "        self.layers_reg = nn.ModuleList(nn.Linear(n, k)\n",
    "                                        for n, k in zip([inplanes] + h, h + [4]))\n",
    "\n",
    "        # for p in self.parameters():\n",
    "        #     if p.dim() > 1:\n",
    "        #         nn.init.xavier_uniform_(p)\n",
    "\n",
    "    def forward(self, x, gt_score_map=None):\n",
    "        \"\"\" Forward pass with input x. \"\"\"\n",
    "        score_map, offset_map = self.get_score_map(x)\n",
    "\n",
    "        # assert gt_score_map is None\n",
    "        if gt_score_map is None:\n",
    "            bbox = self.cal_bbox(score_map, offset_map)\n",
    "        else:\n",
    "            bbox = self.cal_bbox(gt_score_map.unsqueeze(1), offset_map)\n",
    "\n",
    "        return score_map, bbox, offset_map\n",
    "\n",
    "    def cal_bbox(self, score_map, offset_map, return_score=False):\n",
    "        max_score, idx = torch.max(score_map.flatten(1), dim=1, keepdim=True)\n",
    "        idx_y = torch.div(idx, self.feat_sz, rounding_mode='floor')\n",
    "        idx_x = idx % self.feat_sz\n",
    "\n",
    "        idx = idx.unsqueeze(1).expand(idx.shape[0], 4, 1) # torch.Size([32, 4, 1])\n",
    "        offset = offset_map.flatten(2).gather(dim=2, index=idx).squeeze(-1)\n",
    "        # offset: (l,t,r,b)\n",
    "\n",
    "        # x1, y1, x2, y2\n",
    "        bbox = torch.cat([idx_x.to(torch.float) / self.feat_sz - offset[:, :1], # the offset should not divide the self.feat_sz, since I use the sigmoid to limit it in (0,1)\n",
    "                          idx_y.to(torch.float) / self.feat_sz - offset[:, 1:2],\n",
    "                          idx_x.to(torch.float) / self.feat_sz + offset[:, 2:3],\n",
    "                          idx_y.to(torch.float) / self.feat_sz + offset[:, 3:4],\n",
    "                          ], dim=1)\n",
    "        bbox = box_xyxy_to_cxcywh(bbox)\n",
    "        if return_score:\n",
    "            return bbox, max_score\n",
    "        return bbox\n",
    "\n",
    "    def get_score_map(self, x):\n",
    "\n",
    "        def _sigmoid(x):\n",
    "            y = torch.clamp(x.sigmoid_(), min=1e-4, max=1 - 1e-4)\n",
    "            return y\n",
    "\n",
    "        x_cls = x\n",
    "        for i, layer in enumerate(self.layers_cls):\n",
    "            x_cls = F.relu(layer(x_cls)) if i < self.num_layers - 1 else layer(x_cls)\n",
    "        x_cls = x_cls.permute(0,2,1).reshape(-1,1,self.feat_sz,self.feat_sz)\n",
    "\n",
    "        x_reg = x\n",
    "        for i, layer in enumerate(self.layers_reg):\n",
    "            x_reg = F.relu(layer(x_reg)) if i < self.num_layers - 1 else layer(x_reg)\n",
    "        x_reg = x_reg.permute(0, 2, 1).reshape(-1, 4, self.feat_sz, self.feat_sz)\n",
    "\n",
    "        return _sigmoid(x_cls), _sigmoid(x_reg)\n",
    "    \n",
    "class FrozenBatchNorm2d(torch.nn.Module):\n",
    "\n",
    "\n",
    "    def __init__(self, n):\n",
    "        super(FrozenBatchNorm2d, self).__init__()\n",
    "        self.register_buffer(\"weight\", torch.ones(n))\n",
    "        self.register_buffer(\"bias\", torch.zeros(n))\n",
    "        self.register_buffer(\"running_mean\", torch.zeros(n))\n",
    "        self.register_buffer(\"running_var\", torch.ones(n))\n",
    "\n",
    "    def _load_from_state_dict(self, state_dict, prefix, local_metadata, strict,\n",
    "                              missing_keys, unexpected_keys, error_msgs):\n",
    "        num_batches_tracked_key = prefix + 'num_batches_tracked'\n",
    "        if num_batches_tracked_key in state_dict:\n",
    "            del state_dict[num_batches_tracked_key]\n",
    "\n",
    "        super(FrozenBatchNorm2d, self)._load_from_state_dict(\n",
    "            state_dict, prefix, local_metadata, strict,\n",
    "            missing_keys, unexpected_keys, error_msgs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # move reshapes to the beginning\n",
    "        # to make it fuser-friendly\n",
    "        w = self.weight.reshape(1, -1, 1, 1)\n",
    "        b = self.bias.reshape(1, -1, 1, 1)\n",
    "        rv = self.running_var.reshape(1, -1, 1, 1)\n",
    "        rm = self.running_mean.reshape(1, -1, 1, 1)\n",
    "        eps = 1e-5\n",
    "        scale = w * (rv + eps).rsqrt()  # rsqrt(x): 1/sqrt(x), r: reciprocal\n",
    "        bias = b - rm * scale\n",
    "        return x * scale + bias\n",
    "\n",
    "def conv(in_planes, out_planes, kernel_size=3, stride=1, padding=1, dilation=1,\n",
    "         freeze_bn=False):\n",
    "    if freeze_bn:\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride,\n",
    "                      padding=padding, dilation=dilation, bias=True),\n",
    "            FrozenBatchNorm2d(out_planes),\n",
    "            nn.ReLU(inplace=True))\n",
    "    else:\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride,\n",
    "                      padding=padding, dilation=dilation, bias=True),\n",
    "            nn.BatchNorm2d(out_planes),\n",
    "            nn.ReLU(inplace=True))\n",
    "    \n",
    "def conv(in_planes, out_planes, kernel_size=3, stride=1, padding=1, dilation=1,\n",
    "         freeze_bn=False):\n",
    "    if freeze_bn:\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride,\n",
    "                      padding=padding, dilation=dilation, bias=True),\n",
    "            FrozenBatchNorm2d(out_planes),\n",
    "            nn.ReLU(inplace=True))\n",
    "    else:\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride,\n",
    "                      padding=padding, dilation=dilation, bias=True),\n",
    "            nn.BatchNorm2d(out_planes),\n",
    "            nn.ReLU(inplace=True))\n",
    "    \n",
    "class CenterPredictor(nn.Module, ):\n",
    "    def __init__(self, inplanes=64, channel=256, feat_sz=20, stride=16, freeze_bn=False):\n",
    "        super(CenterPredictor, self).__init__()\n",
    "        self.feat_sz = feat_sz\n",
    "        self.stride = stride\n",
    "        self.img_sz = self.feat_sz * self.stride\n",
    "\n",
    "        # corner predict\n",
    "        self.conv1_ctr = conv(inplanes, channel, freeze_bn=freeze_bn)\n",
    "        self.conv2_ctr = conv(channel, channel // 2, freeze_bn=freeze_bn)\n",
    "        self.conv3_ctr = conv(channel // 2, channel // 4, freeze_bn=freeze_bn)\n",
    "        self.conv4_ctr = conv(channel // 4, channel // 8, freeze_bn=freeze_bn)\n",
    "        self.conv5_ctr = nn.Conv2d(channel // 8, 1, kernel_size=1)\n",
    "\n",
    "        # size regress\n",
    "        self.conv1_offset = conv(inplanes, channel, freeze_bn=freeze_bn)\n",
    "        self.conv2_offset = conv(channel, channel // 2, freeze_bn=freeze_bn)\n",
    "        self.conv3_offset = conv(channel // 2, channel // 4, freeze_bn=freeze_bn)\n",
    "        self.conv4_offset = conv(channel // 4, channel // 8, freeze_bn=freeze_bn)\n",
    "        self.conv5_offset = nn.Conv2d(channel // 8, 2, kernel_size=1)\n",
    "\n",
    "        # size regress\n",
    "        self.conv1_size = conv(inplanes, channel, freeze_bn=freeze_bn)\n",
    "        self.conv2_size = conv(channel, channel // 2, freeze_bn=freeze_bn)\n",
    "        self.conv3_size = conv(channel // 2, channel // 4, freeze_bn=freeze_bn)\n",
    "        self.conv4_size = conv(channel // 4, channel // 8, freeze_bn=freeze_bn)\n",
    "        self.conv5_size = nn.Conv2d(channel // 8, 2, kernel_size=1)\n",
    "\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "\n",
    "    def forward(self, x, gt_score_map=None):\n",
    "        \"\"\" Forward pass with input x. \"\"\"\n",
    "        score_map_ctr, size_map, offset_map = self.get_score_map(x) # x: torch.Size([b, c, h, w])\n",
    "        # score_map_ctr: torch.Size([32, 1, 16, 16]) size_map: torch.Size([32, 2, 16, 16]) offset_map: torch.Size([32, 2, 16, 16])\n",
    "\n",
    "        # assert gt_score_map is None\n",
    "        if gt_score_map is None:\n",
    "            bbox = self.cal_bbox(score_map_ctr, size_map, offset_map)\n",
    "        else:\n",
    "            bbox = self.cal_bbox(gt_score_map.unsqueeze(1), size_map, offset_map)\n",
    "\n",
    "        return score_map_ctr, bbox, size_map, offset_map\n",
    "\n",
    "    def cal_bbox(self, score_map_ctr, size_map, offset_map, return_score=False):\n",
    "        max_score, idx = torch.max(score_map_ctr.flatten(1), dim=1, keepdim=True) # score_map_ctr.flatten(1): torch.Size([32, 256]) idx: torch.Size([32, 1]) max_score: torch.Size([32, 1])\n",
    "        idx_y = torch.div(idx, self.feat_sz, rounding_mode='floor')\n",
    "        idx_x = idx % self.feat_sz\n",
    "\n",
    "        idx = idx.unsqueeze(1).expand(idx.shape[0], 2, 1)\n",
    "        size = size_map.flatten(2).gather(dim=2, index=idx) # size_map: torch.Size([32, 2, 16, 16])  size_map.flatten(2): torch.Size([32, 2, 256])\n",
    "        offset = offset_map.flatten(2).gather(dim=2, index=idx).squeeze(-1)\n",
    "\n",
    "        # bbox = torch.cat([idx_x - size[:, 0] / 2, idx_y - size[:, 1] / 2,\n",
    "        #                   idx_x + size[:, 0] / 2, idx_y + size[:, 1] / 2], dim=1) / self.feat_sz\n",
    "        # cx, cy, w, h\n",
    "        bbox = torch.cat([(idx_x.to(torch.float) + offset[:, :1]) / self.feat_sz,\n",
    "                          (idx_y.to(torch.float) + offset[:, 1:]) / self.feat_sz,\n",
    "                          size.squeeze(-1)], dim=1)\n",
    "\n",
    "        if return_score:\n",
    "            return bbox, max_score\n",
    "        return bbox\n",
    "\n",
    "    def get_pred(self, score_map_ctr, size_map, offset_map):\n",
    "        max_score, idx = torch.max(score_map_ctr.flatten(1), dim=1, keepdim=True)\n",
    "        idx_y = idx // self.feat_sz\n",
    "        idx_x = idx % self.feat_sz\n",
    "\n",
    "        idx = idx.unsqueeze(1).expand(idx.shape[0], 2, 1)\n",
    "        size = size_map.flatten(2).gather(dim=2, index=idx)\n",
    "        offset = offset_map.flatten(2).gather(dim=2, index=idx).squeeze(-1)\n",
    "\n",
    "        # bbox = torch.cat([idx_x - size[:, 0] / 2, idx_y - size[:, 1] / 2,\n",
    "        #                   idx_x + size[:, 0] / 2, idx_y + size[:, 1] / 2], dim=1) / self.feat_sz\n",
    "        return size * self.feat_sz, offset\n",
    "\n",
    "    def get_score_map(self, x):\n",
    "\n",
    "        def _sigmoid(x):\n",
    "            y = torch.clamp(x.sigmoid_(), min=1e-4, max=1 - 1e-4)\n",
    "            return y\n",
    "\n",
    "        # ctr branch\n",
    "        x_ctr1 = self.conv1_ctr(x)\n",
    "        x_ctr2 = self.conv2_ctr(x_ctr1)\n",
    "        x_ctr3 = self.conv3_ctr(x_ctr2)\n",
    "        x_ctr4 = self.conv4_ctr(x_ctr3)\n",
    "        score_map_ctr = self.conv5_ctr(x_ctr4)\n",
    "\n",
    "        # offset branch\n",
    "        x_offset1 = self.conv1_offset(x)\n",
    "        x_offset2 = self.conv2_offset(x_offset1)\n",
    "        x_offset3 = self.conv3_offset(x_offset2)\n",
    "        x_offset4 = self.conv4_offset(x_offset3)\n",
    "        score_map_offset = self.conv5_offset(x_offset4)\n",
    "\n",
    "        # size branch\n",
    "        x_size1 = self.conv1_size(x)\n",
    "        x_size2 = self.conv2_size(x_size1)\n",
    "        x_size3 = self.conv3_size(x_size2)\n",
    "        x_size4 = self.conv4_size(x_size3)\n",
    "        score_map_size = self.conv5_size(x_size4)\n",
    "        return _sigmoid(score_map_ctr), _sigmoid(score_map_size), score_map_offset\n",
    "    \n",
    "def build_decoder(cfg, encoder):\n",
    "    num_channels_enc = encoder.num_channels\n",
    "    stride = cfg[\"MODEL\"][\"ENCODER\"][\"STRIDE\"]\n",
    "    if cfg[\"MODEL\"][\"DECODER\"][\"TYPE\"] == \"MLP\":\n",
    "        in_channel = num_channels_enc\n",
    "        hidden_dim = cfg[\"MODEL\"][\"DECODER\"][\"NUM_CHANNELS\"]\n",
    "        feat_sz = int(cfg[\"DATA\"][\"SEARCH\"][\"SIZE\"] / stride)\n",
    "        mlp_head = MLPPredictor(inplanes=in_channel, channel=hidden_dim,\n",
    "                                feat_sz=feat_sz, stride=stride)\n",
    "        return mlp_head\n",
    "    elif \"CORNER\" in cfg[\"MODEL\"][\"DECODER\"][\"TYPE\"]:\n",
    "        feat_sz = int(cfg[\"DATA\"][\"SEARCH\"][\"SIZE\"] / stride)\n",
    "        channel = getattr(cfg[\"MODEL\"], \"NUM_CHANNELS\", 256)\n",
    "        print(\"head channel: %d\" % channel)\n",
    "        if cfg[\"MODEL\"][\"HEAD\"][\"TYPE\"] == \"CORNER\":\n",
    "            corner_head = Corner_Predictor(inplanes=cfg[\"MODEL\"][\"HIDDEN_DIM\"], channel=channel,\n",
    "                                           feat_sz=feat_sz, stride=stride)\n",
    "        else:\n",
    "            raise ValueError()\n",
    "        return corner_head\n",
    "    elif cfg[\"MODEL\"][\"DECODER\"][\"TYPE\"] == \"CENTER\":\n",
    "        in_channel = num_channels_enc\n",
    "        out_channel = cfg[\"MODEL\"][\"DECODER\"][\"NUM_CHANNELS\"]\n",
    "        feat_sz = int(cfg[\"DATA\"][\"SEARCH\"][\"SIZE\"] / stride)\n",
    "        center_head = CenterPredictor(inplanes=in_channel, channel=out_channel,\n",
    "                                      feat_sz=feat_sz, stride=stride)\n",
    "        return center_head\n",
    "    else:\n",
    "        raise ValueError(\"HEAD TYPE %s is not supported.\" % cfg[\"MODEL\"][\"HEAD_TYPE\"])\n",
    "    \n",
    "class Corner_Predictor(nn.Module):\n",
    "    \"\"\" Corner Predictor module\"\"\"\n",
    "\n",
    "    def __init__(self, inplanes=64, channel=256, feat_sz=20, stride=16, freeze_bn=False):\n",
    "        super(Corner_Predictor, self).__init__()\n",
    "        self.feat_sz = feat_sz\n",
    "        self.stride = stride\n",
    "        self.img_sz = self.feat_sz * self.stride\n",
    "        '''top-left corner'''\n",
    "        self.conv1_tl = conv(inplanes, channel, freeze_bn=freeze_bn)\n",
    "        self.conv2_tl = conv(channel, channel // 2, freeze_bn=freeze_bn)\n",
    "        self.conv3_tl = conv(channel // 2, channel // 4, freeze_bn=freeze_bn)\n",
    "        self.conv4_tl = conv(channel // 4, channel // 8, freeze_bn=freeze_bn)\n",
    "        self.conv5_tl = nn.Conv2d(channel // 8, 1, kernel_size=1)\n",
    "\n",
    "        '''bottom-right corner'''\n",
    "        self.conv1_br = conv(inplanes, channel, freeze_bn=freeze_bn)\n",
    "        self.conv2_br = conv(channel, channel // 2, freeze_bn=freeze_bn)\n",
    "        self.conv3_br = conv(channel // 2, channel // 4, freeze_bn=freeze_bn)\n",
    "        self.conv4_br = conv(channel // 4, channel // 8, freeze_bn=freeze_bn)\n",
    "        self.conv5_br = nn.Conv2d(channel // 8, 1, kernel_size=1)\n",
    "\n",
    "        '''about coordinates and indexs'''\n",
    "        with torch.no_grad():\n",
    "            self.indice = torch.arange(0, self.feat_sz).view(-1, 1) * self.stride\n",
    "            # generate mesh-grid\n",
    "            self.coord_x = self.indice.repeat((self.feat_sz, 1)) \\\n",
    "                .view((self.feat_sz * self.feat_sz,)).float().cuda()\n",
    "            self.coord_y = self.indice.repeat((1, self.feat_sz)) \\\n",
    "                .view((self.feat_sz * self.feat_sz,)).float().cuda()\n",
    "\n",
    "    def forward(self, x, return_dist=False, softmax=True):\n",
    "        \"\"\" Forward pass with input x. \"\"\"\n",
    "        score_map_tl, score_map_br = self.get_score_map(x)\n",
    "        if return_dist:\n",
    "            coorx_tl, coory_tl, prob_vec_tl = self.soft_argmax(score_map_tl, return_dist=True, softmax=softmax)\n",
    "            coorx_br, coory_br, prob_vec_br = self.soft_argmax(score_map_br, return_dist=True, softmax=softmax)\n",
    "            return torch.stack((coorx_tl, coory_tl, coorx_br, coory_br), dim=1) / self.img_sz, prob_vec_tl, prob_vec_br\n",
    "        else:\n",
    "            coorx_tl, coory_tl = self.soft_argmax(score_map_tl)\n",
    "            coorx_br, coory_br = self.soft_argmax(score_map_br)\n",
    "            return torch.stack((coorx_tl, coory_tl, coorx_br, coory_br), dim=1) / self.img_sz\n",
    "\n",
    "    def get_score_map(self, x):\n",
    "        # top-left branch\n",
    "        x_tl1 = self.conv1_tl(x)\n",
    "        x_tl2 = self.conv2_tl(x_tl1)\n",
    "        x_tl3 = self.conv3_tl(x_tl2)\n",
    "        x_tl4 = self.conv4_tl(x_tl3)\n",
    "        score_map_tl = self.conv5_tl(x_tl4)\n",
    "\n",
    "        # bottom-right branch\n",
    "        x_br1 = self.conv1_br(x)\n",
    "        x_br2 = self.conv2_br(x_br1)\n",
    "        x_br3 = self.conv3_br(x_br2)\n",
    "        x_br4 = self.conv4_br(x_br3)\n",
    "        score_map_br = self.conv5_br(x_br4)\n",
    "        return score_map_tl, score_map_br\n",
    "\n",
    "    def soft_argmax(self, score_map, return_dist=False, softmax=True):\n",
    "        \"\"\" get soft-argmax coordinate for a given heatmap \"\"\"\n",
    "        score_vec = score_map.view((-1, self.feat_sz * self.feat_sz))  # (batch, feat_sz * feat_sz)\n",
    "        prob_vec = nn.functional.softmax(score_vec, dim=1)\n",
    "        exp_x = torch.sum((self.coord_x * prob_vec), dim=1)\n",
    "        exp_y = torch.sum((self.coord_y * prob_vec), dim=1)\n",
    "        if return_dist:\n",
    "            if softmax:\n",
    "                return exp_x, exp_y, prob_vec\n",
    "            else:\n",
    "                return exp_x, exp_y, score_vec\n",
    "        else:\n",
    "            return exp_x, exp_y\n",
    "        \n",
    "class Preprocessor(object):\n",
    "    def __init__(self):\n",
    "        self.mean = torch.tensor([0.485, 0.456, 0.406]).view((1, 3, 1, 1)).cuda()\n",
    "        self.std = torch.tensor([0.229, 0.224, 0.225]).view((1, 3, 1, 1)).cuda()\n",
    "        self.mm_mean = torch.tensor([0.485, 0.456, 0.406, 0.485, 0.456, 0.406]).view((1, 6, 1, 1)).cuda()\n",
    "        self.mm_std = torch.tensor([0.229, 0.224, 0.225, 0.229, 0.224, 0.225]).view((1, 6, 1, 1)).cuda()\n",
    "\n",
    "    def process(self, img_arr: np.ndarray):\n",
    "        if img_arr.shape[-1] == 6:\n",
    "            mean = self.mm_mean\n",
    "            std = self.mm_std\n",
    "        else:\n",
    "            mean = self.mean\n",
    "            std = self.std\n",
    "        # Deal with the image patch\n",
    "        img_tensor = torch.tensor(img_arr).cuda().float().permute((2,0,1)).unsqueeze(dim=0)\n",
    "        # img_tensor = torch.tensor(img_arr).float().permute((2,0,1)).unsqueeze(dim=0)\n",
    "        img_tensor_norm = ((img_tensor / 255.0) - mean) / std  # (1,3,H,W)\n",
    "        return img_tensor_norm\n",
    "    \n",
    "def hann1d(sz: int, centered = True) -> torch.Tensor:\n",
    "    \"\"\"1D cosine window.\"\"\"\n",
    "    if centered:\n",
    "        return 0.5 * (1 - torch.cos((2 * math.pi / (sz + 1)) * torch.arange(1, sz + 1).float()))\n",
    "    w = 0.5 * (1 + torch.cos((2 * math.pi / (sz + 2)) * torch.arange(0, sz//2 + 1).float()))\n",
    "    return torch.cat([w, w[1:sz-sz//2].flip((0,))])\n",
    "    \n",
    "def hann2d(sz: torch.Tensor, centered = True) -> torch.Tensor:\n",
    "    \"\"\"2D cosine window.\"\"\"\n",
    "    return hann1d(sz[0].item(), centered).reshape(1, 1, -1, 1) * hann1d(sz[1].item(), centered).reshape(1, 1, 1, -1)    \n",
    "\n",
    "def sample_target(im, target_bb, search_area_factor, output_sz=None):\n",
    "   \n",
    "    if not isinstance(target_bb, list):\n",
    "        x, y, w, h = target_bb.tolist()\n",
    "    else:\n",
    "        x, y, w, h = target_bb\n",
    "    # Crop image\n",
    "    crop_sz = math.ceil(math.sqrt(w * h) * search_area_factor)\n",
    "\n",
    "    if crop_sz < 1:\n",
    "        raise Exception('Too small bounding box.')\n",
    "\n",
    "    x1 = round(x + 0.5 * w - crop_sz * 0.5)\n",
    "    x2 = x1 + crop_sz\n",
    "\n",
    "    y1 = round(y + 0.5 * h - crop_sz * 0.5)\n",
    "    y2 = y1 + crop_sz\n",
    "\n",
    "    x1_pad = max(0, -x1)\n",
    "    x2_pad = max(x2 - im.shape[1] + 1, 0)\n",
    "\n",
    "    y1_pad = max(0, -y1)\n",
    "    y2_pad = max(y2 - im.shape[0] + 1, 0)\n",
    "\n",
    "    # Crop target\n",
    "    im_crop = im[y1 + y1_pad:y2 - y2_pad, x1 + x1_pad:x2 - x2_pad, :]\n",
    "\n",
    "    # Pad\n",
    "    im_crop_padded = cv2.copyMakeBorder(im_crop, y1_pad, y2_pad, x1_pad, x2_pad, cv2.BORDER_CONSTANT)\n",
    "    # deal with attention mask\n",
    "    H, W, _ = im_crop_padded.shape\n",
    "\n",
    "    if output_sz is not None:\n",
    "        resize_factor = output_sz / crop_sz\n",
    "        im_crop_padded = cv2.resize(im_crop_padded, (output_sz, output_sz))\n",
    "\n",
    "        return im_crop_padded, resize_factor\n",
    "\n",
    "    else:\n",
    "        return im_crop_padded, 1.0\n",
    "def transform_image_to_crop(box_in: torch.Tensor, box_extract: torch.Tensor, resize_factor: float,\n",
    "                            crop_sz: torch.Tensor, normalize=False) -> torch.Tensor:\n",
    "   \n",
    "    box_extract_center = box_extract[0:2] + 0.5 * box_extract[2:4]\n",
    "\n",
    "    box_in_center = box_in[0:2] + 0.5 * box_in[2:4]\n",
    "\n",
    "    box_out_center = (crop_sz - 1) / 2 + (box_in_center - box_extract_center) * resize_factor\n",
    "    box_out_wh = box_in[2:4] * resize_factor\n",
    "\n",
    "    box_out = torch.cat((box_out_center - 0.5 * box_out_wh, box_out_wh))\n",
    "    if normalize:\n",
    "        return box_out / (crop_sz[0]-1)\n",
    "    else:\n",
    "        return box_out\n",
    "def clip_box(box: list, H, W, margin=0):\n",
    "    x1, y1, w, h = box\n",
    "    x2, y2 = x1 + w, y1 + h\n",
    "    x1 = min(max(0, x1), W-margin)\n",
    "    x2 = min(max(margin, x2), W)\n",
    "    y1 = min(max(0, y1), H-margin)\n",
    "    y2 = min(max(margin, y2), H)\n",
    "    w = max(margin, x2-x1)\n",
    "    h = max(margin, y2-y1)\n",
    "    return [x1, y1, w, h]\n",
    "\n",
    "class BaseTracker():\n",
    "    \"\"\"Base class for all trackers.\"\"\"\n",
    "\n",
    "    def __init__(self, params):\n",
    "        self.params = params\n",
    "        self.visdom = None\n",
    "\n",
    "    def predicts_segmentation_mask(self):\n",
    "        return False\n",
    "\n",
    "    def initialize(self, image, info: dict) -> dict:\n",
    "        \"\"\"Overload this function in your tracker. This should initialize the model.\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def track(self, image, info: dict = None) -> dict:\n",
    "        \"\"\"Overload this function in your tracker. This should track in the frame and update the model.\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def visdom_draw_tracking(self, image, box, segmentation=None):\n",
    "        # Упрощенная обработка box без OrderedDict\n",
    "        if isinstance(box, dict):  # Проверяем на обычный dict вместо OrderedDict\n",
    "            box = list(box.values())  # Берем только значения\n",
    "        elif not isinstance(box, (list, tuple)):  # Если не коллекция\n",
    "            box = (box,)  # Превращаем в кортеж\n",
    "        \n",
    "        # Визуализация\n",
    "        if segmentation is None:\n",
    "            self.visdom.register((image, *box), 'Tracking', 1, 'Tracking')\n",
    "        else:\n",
    "            self.visdom.register((image, *box, segmentation), 'Tracking', 1, 'Tracking')\n",
    "\n",
    "\n",
    "class MCITRACK(BaseTracker):\n",
    "    def __init__(self, params):\n",
    "        \n",
    "        super(MCITRACK, self).__init__(params)\n",
    "        network = build_mcitrack(params.cfg)\n",
    "        network.load_state_dict(torch.load(\"MCITRACK_ep0300.pth.tar\", map_location='cpu')['net'], strict=True)\n",
    "        \n",
    "        self.cfg = params.cfg\n",
    "        self.network = network.cuda()\n",
    "        self.network.eval()\n",
    "        self.preprocessor = Preprocessor()\n",
    "        self.state = None\n",
    "\n",
    "        self.fx_sz = self.cfg[\"TEST\"][\"SEARCH_SIZE\"] // self.cfg[\"MODEL\"][\"ENCODER\"][\"STRIDE\"]\n",
    "        if self.cfg[\"TEST\"][\"WINDOW\"] == True:  # for window penalty\n",
    "            self.output_window = hann2d(torch.tensor([self.fx_sz, self.fx_sz]).long(), centered=True).cuda()\n",
    "\n",
    "        self.num_template = self.cfg[\"TEST\"][\"NUM_TEMPLATES\"]\n",
    "\n",
    "   \n",
    "        self.frame_id = 0\n",
    "        # for update\n",
    "        self.h_state = [None] * self.cfg[\"MODEL\"][\"NECK\"][\"N_LAYERS\"]\n",
    "\n",
    "\n",
    "\n",
    "        self.memory_bank = self.cfg[\"TEST\"][\"MB\"][\"DEFAULT\"]\n",
    "        self.update_h_t = self.cfg[\"TEST\"][\"UPH\"][\"DEFAULT\"]\n",
    "        self.update_threshold = self.cfg[\"TEST\"][\"UPT\"][\"DEFAULT\"]\n",
    "        self.update_intervals = self.cfg[\"TEST\"][\"INTER\"][\"DEFAULT\"]\n",
    "        print(\"Update threshold is: \", self.memory_bank)\n",
    "\n",
    "    def initialize(self, image, info: dict):\n",
    "\n",
    "\n",
    "        # get the initial templates\n",
    "        z_patch_arr, resize_factor = sample_target(image, info['init_bbox'], self.params.template_factor,\n",
    "                                                   output_sz=self.params.template_size)\n",
    "        z_patch_arr = z_patch_arr\n",
    "        template = self.preprocessor.process(z_patch_arr)\n",
    "        self.template_list = [template] * self.num_template\n",
    "\n",
    "        self.state = info['init_bbox']\n",
    "        prev_box_crop = transform_image_to_crop(torch.tensor(info['init_bbox']),\n",
    "                                                torch.tensor(info['init_bbox']),\n",
    "                                                resize_factor,\n",
    "                                                torch.Tensor([self.params.template_size, self.params.template_size]),\n",
    "                                                normalize=True)\n",
    "        self.template_anno_list = [prev_box_crop.to(template.device).unsqueeze(0)] * self.num_template\n",
    "        self.frame_id = 0\n",
    "        self.memory_template_list = self.template_list.copy()\n",
    "        self.memory_template_anno_list = self.template_anno_list.copy()\n",
    "\n",
    "\n",
    "    def track(self, image, info: dict = None):\n",
    "        H, W, _ = image.shape\n",
    "        self.frame_id += 1\n",
    "        x_patch_arr, resize_factor = sample_target(image, self.state, self.params.search_factor,\n",
    "                                                   output_sz=self.params.search_size)  # (x1, y1, w, h)\n",
    "        search = self.preprocessor.process(x_patch_arr)\n",
    "        search_list = [search]\n",
    "\n",
    "        # run the encoder\n",
    "\n",
    "        with torch.no_grad():\n",
    "            out_dict = self.network.forward(\n",
    "                template_list=self.template_list,\n",
    "                search_list=search_list,\n",
    "                template_anno_list=self.template_anno_list,\n",
    "                \n",
    "                gt_score_map=None\n",
    "            )\n",
    "\n",
    "        \n",
    "\n",
    "        # add hann windows\n",
    "        pred_score_map = out_dict['score_map']\n",
    "        if self.cfg[\"TEST\"][\"WINDOW\"] == True:  # for window penalty\n",
    "            response = self.output_window * pred_score_map\n",
    "        else:\n",
    "            response = pred_score_map\n",
    "        if 'size_map' in out_dict.keys():\n",
    "            pred_boxes, conf_score = self.network.decoder.cal_bbox(response, out_dict['size_map'],\n",
    "                                                                   out_dict['offset_map'], return_score=True)\n",
    "        else:\n",
    "            pred_boxes, conf_score = self.network.decoder.cal_bbox(response,\n",
    "                                                                   out_dict['offset_map'],\n",
    "                                                                   return_score=True)\n",
    "        pred_boxes = pred_boxes.view(-1, 4)\n",
    "        # Baseline: Take the mean of all pred boxes as the final result\n",
    "        pred_box = (pred_boxes.mean(dim=0) * self.params.search_size / resize_factor).tolist()  # (cx, cy, w, h) [0,1]\n",
    "        # get the final box result\n",
    "        self.state = clip_box(self.map_box_back(pred_box, resize_factor), H, W, margin=10)\n",
    "        # update hiden state\n",
    "        # self.h_state = h\n",
    "        # if conf_score.item() < self.update_h_t:\n",
    "        #     self.h_state = [None] * self.cfg[\"MODEL\"][\"NECK\"][\"N_LAYERS\"]\n",
    "\n",
    "        # update the template\n",
    "        if self.num_template > 1:\n",
    "            if (conf_score > self.update_threshold):\n",
    "                z_patch_arr, resize_factor = sample_target(image, self.state, self.params.template_factor,\n",
    "                                                           output_sz=self.params.template_size)\n",
    "                template = self.preprocessor.process(z_patch_arr)\n",
    "                self.memory_template_list.append(template)\n",
    "                prev_box_crop = transform_image_to_crop(torch.tensor(self.state),\n",
    "                                                        torch.tensor(self.state),\n",
    "                                                        resize_factor,\n",
    "                                                        torch.Tensor(\n",
    "                                                            [self.params.template_size, self.params.template_size]),\n",
    "                                                        normalize=True)\n",
    "                self.memory_template_anno_list.append(prev_box_crop.to(template.device).unsqueeze(0))\n",
    "                if len(self.memory_template_list) > self.memory_bank:\n",
    "                    self.memory_template_list.pop(0)\n",
    "                    self.memory_template_anno_list.pop(0)\n",
    "        if (self.frame_id % self.update_intervals == 0):\n",
    "            assert len(self.memory_template_anno_list) == len(self.memory_template_list)\n",
    "            len_list = len(self.memory_template_anno_list)\n",
    "            interval = len_list // self.num_template\n",
    "            for i in range(1, self.num_template):\n",
    "                idx = interval * i\n",
    "                if idx > len_list:\n",
    "                    idx = len_list\n",
    "                self.template_list.append(self.memory_template_list[idx])\n",
    "                self.template_list.pop(1)\n",
    "                self.template_anno_list.append(self.memory_template_anno_list[idx])\n",
    "                self.template_anno_list.pop(1)\n",
    "        assert len(self.template_list) == self.num_template\n",
    "\n",
    "\n",
    "\n",
    "        return {\"target_bbox\": self.state,\n",
    "                \"best_score\": conf_score}\n",
    "\n",
    "    def map_box_back(self, pred_box: list, resize_factor: float):\n",
    "        cx_prev, cy_prev = self.state[0] + 0.5 * self.state[2], self.state[1] + 0.5 * self.state[3]\n",
    "        cx, cy, w, h = pred_box\n",
    "        half_side = 0.5 * self.params.search_size / resize_factor\n",
    "        cx_real = cx + (cx_prev - half_side)\n",
    "        cy_real = cy + (cy_prev - half_side)\n",
    "        return [cx_real - 0.5 * w, cy_real - 0.5 * h, w, h]\n",
    "\n",
    "    def map_box_back_batch(self, pred_box: torch.Tensor, resize_factor: float):\n",
    "        cx_prev, cy_prev = self.state[0] + 0.5 * self.state[2], self.state[1] + 0.5 * self.state[3]\n",
    "        cx, cy, w, h = pred_box.unbind(-1)  # (N,4) --> (N,)\n",
    "        half_side = 0.5 * self.params.search_size / resize_factor\n",
    "        cx_real = cx + (cx_prev - half_side)\n",
    "        cy_real = cy + (cy_prev - half_side)\n",
    "        return torch.stack([cx_real - 0.5 * w, cy_real - 0.5 * h, w, h], dim=-1)\n",
    "\n",
    "class MCITrack(nn.Module):\n",
    "    \n",
    "    def __init__(self, encoder, decoder, neck, cfg,\n",
    "                 num_frames=1, num_template=1, decoder_type=\"CENTER\"):\n",
    "      \n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder_type = decoder_type\n",
    "        self.neck = neck\n",
    "\n",
    "        self.num_patch_x = self.encoder.body.num_patches_search\n",
    "        self.num_patch_z = self.encoder.body.num_patches_template\n",
    "        self.fx_sz = int(math.sqrt(self.num_patch_x))\n",
    "        self.fz_sz = int(math.sqrt(self.num_patch_z))\n",
    "\n",
    "        self.decoder = decoder\n",
    "\n",
    "        self.num_frames = num_frames\n",
    "        self.num_template = num_template\n",
    "        self.freeze_en = cfg[\"TRAIN\"][\"FREEZE_ENCODER\"]\n",
    "        self.interaction_indexes = cfg[\"MODEL\"][\"ENCODER\"][\"INTERACTION_INDEXES\"]\n",
    "\n",
    "    def forward(self, template_list, search_list, template_anno_list, gt_score_map=None):\n",
    "    \n",
    "        # Step 1: Forward pass through the encoder\n",
    "        \n",
    "        neck_h_state=[None,None,None,None]\n",
    "        \n",
    "        xz = self.encoder(template_list, search_list, template_anno_list)\n",
    "\n",
    "        # Step 2: Forward pass through the neck\n",
    "        xs = xz[:, 0:self.num_patch_x]  # Extract patch embeddings\n",
    "        x, xs, h = self.neck(\n",
    "            xz, xs, neck_h_state, \n",
    "            self.encoder.body.blocks, \n",
    "            self.interaction_indexes\n",
    "        )\n",
    "        x = self.encoder.body.fc_norm(x)\n",
    "        xs = xs + x[:, 0:self.num_patch_x]  # Updated patch embeddings\n",
    "\n",
    "        # Step 3: Forward pass through the decoder\n",
    "        bs, HW, C = xs.size()\n",
    "        if self.decoder_type in ['CORNER', 'CENTER']:\n",
    "            xs = xs.permute((0, 2, 1)).contiguous()\n",
    "            xs = xs.view(bs, C, self.fx_sz, self.fx_sz)\n",
    "\n",
    "        if self.decoder_type == \"CORNER\":\n",
    "            # Run the corner head\n",
    "            pred_box, score_map = self.decoder(xs, True)\n",
    "            outputs_coord = box_xyxy_to_cxcywh(pred_box)\n",
    "            outputs_coord_new = outputs_coord.view(bs, 1, 4)\n",
    "            return {\n",
    "                'pred_boxes': outputs_coord_new,\n",
    "                'score_map': score_map\n",
    "            }\n",
    "\n",
    "        elif self.decoder_type == \"CENTER\":\n",
    "            # Run the center head\n",
    "            score_map_ctr, bbox, size_map, offset_map = self.decoder(xs, gt_score_map)\n",
    "            outputs_coord = bbox\n",
    "            outputs_coord_new = outputs_coord.view(bs, 1, 4)\n",
    "            return {\n",
    "                'pred_boxes': outputs_coord_new,\n",
    "                'score_map': score_map_ctr,\n",
    "                'size_map': size_map,\n",
    "                'offset_map': offset_map\n",
    "            }\n",
    "\n",
    "        elif self.decoder_type == \"MLP\":\n",
    "            # Run the MLP head\n",
    "            score_map, bbox, offset_map = self.decoder(xs, gt_score_map)\n",
    "            outputs_coord = bbox\n",
    "            outputs_coord_new = outputs_coord.view(bs, 1, 4)\n",
    "            return {\n",
    "                'pred_boxes': outputs_coord_new,\n",
    "                'score_map': score_map,\n",
    "                'offset_map': offset_map\n",
    "            }\n",
    "\n",
    "        else:\n",
    "            raise NotImplementedError(f\"Decoder type not supported: {self.decoder_type}\")\n",
    "\n",
    "def build_mcitrack(cfg):\n",
    "    encoder = build_encoder(cfg)\n",
    "    neck = build_neck(cfg,encoder)\n",
    "    decoder = build_decoder(cfg, neck)\n",
    "    model = MCITrack(\n",
    "        encoder,\n",
    "        decoder,\n",
    "        neck,\n",
    "        cfg,\n",
    "        num_frames = cfg[\"DATA\"][\"SEARCH\"][\"NUMBER\"],\n",
    "        num_template = cfg[\"DATA\"][\"TEMPLATE\"][\"NUMBER\"],\n",
    "        decoder_type=cfg[\"MODEL\"][\"DECODER\"][\"TYPE\"],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def get_tracker_class():\n",
    "    return MCITRACK\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "407a71c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = {}\n",
    "\n",
    "# MODEL\n",
    "cfg[\"MODEL\"] = {}\n",
    "\n",
    "# MODEL.ENCODER\n",
    "cfg[\"MODEL\"][\"ENCODER\"] = {\n",
    "    \"TYPE\": \"dinov2_vitb14\",  # encoder model\n",
    "    \"DROP_PATH\": 0,\n",
    "    \"PRETRAIN_TYPE\": \"mae\",  # mae, default, or scratch. This parameter is not activated for dinov2.\n",
    "    \"USE_CHECKPOINT\": False,  # to save the memory.\n",
    "    \"STRIDE\": 14,\n",
    "    \"POS_TYPE\": 'interpolate',  # type of loading the positional encoding. \"interpolate\" or \"index\".\n",
    "    \"TOKEN_TYPE_INDICATE\": False,  # add a token_type_embedding to indicate the search, template_foreground, template_background\n",
    "    \"INTERACTION_INDEXES\": [[0, 6], [6, 12], [12, 18], [18, 24]],\n",
    "    \"GRAD_CKPT\": False\n",
    "}\n",
    "\n",
    "# MODEL.NECK\n",
    "cfg[\"MODEL\"][\"NECK\"] = {\n",
    "    \"N_LAYERS\": 4,\n",
    "    \"D_MODEL\": 512,\n",
    "    \"D_STATE\": 16  # MAMABA_HIDDEN_STATE\n",
    "}\n",
    "\n",
    "# MODEL.DECODER\n",
    "cfg[\"MODEL\"][\"DECODER\"] = {\n",
    "    \"TYPE\": \"CENTER\",  # MLP, CORNER, CENTER\n",
    "    \"NUM_CHANNELS\": 256\n",
    "}\n",
    "\n",
    "# TRAIN\n",
    "cfg[\"TRAIN\"] = {\n",
    "    \"LR\": 0.0001,\n",
    "    \"WEIGHT_DECAY\": 0.0001,\n",
    "    \"EPOCH\": 500,\n",
    "    \"LR_DROP_EPOCH\": 400,\n",
    "    \"BATCH_SIZE\": 8,\n",
    "    \"NUM_WORKER\": 8,\n",
    "    \"OPTIMIZER\": \"ADAMW\",\n",
    "    \"ENCODER_MULTIPLIER\": 0.1,  # encoder's LR = this factor * LR\n",
    "    \"FREEZE_ENCODER\": False,  # for freezing the parameters of encoder\n",
    "    \"ENCODER_OPEN\": [],  # only for debug, open some layers of encoder when FREEZE_ENCODER is True\n",
    "    \"CE_WEIGHT\": 1.0,  # weight for cross-entropy loss\n",
    "    \"GIOU_WEIGHT\": 2.0,\n",
    "    \"L1_WEIGHT\": 5.0,\n",
    "    \"PRINT_INTERVAL\": 50,  # interval to print the training log\n",
    "    \"GRAD_CLIP_NORM\": 0.1,\n",
    "    \"FIX_BN\": False,\n",
    "    \"ENCODER_W\": \"\",\n",
    "    \"TYPE\": \"normal\",  # normal, peft or fft\n",
    "    \"PRETRAINED_PATH\": None\n",
    "}\n",
    "\n",
    "# TRAIN.SCHEDULER\n",
    "cfg[\"TRAIN\"][\"SCHEDULER\"] = {\n",
    "    \"TYPE\": \"step\",\n",
    "    \"DECAY_RATE\": 0.1\n",
    "}\n",
    "\n",
    "# DATA\n",
    "cfg[\"DATA\"] = {\n",
    "    \"MEAN\": [0.485, 0.456, 0.406],\n",
    "    \"STD\": [0.229, 0.224, 0.225],\n",
    "    \"MAX_SAMPLE_INTERVAL\": 200,\n",
    "    \"SAMPLER_MODE\": \"order\",\n",
    "    \"LOADER\": \"tracking\"\n",
    "}\n",
    "\n",
    "# DATA.TRAIN\n",
    "cfg[\"DATA\"][\"TRAIN\"] = {\n",
    "    \"DATASETS_NAME\": [\"LASOT\", \"GOT10K_vottrain\"],\n",
    "    \"DATASETS_RATIO\": [1, 1],\n",
    "    \"SAMPLE_PER_EPOCH\": 60000\n",
    "}\n",
    "\n",
    "# DATA.SEARCH\n",
    "cfg[\"DATA\"][\"SEARCH\"] = {\n",
    "    \"NUMBER\": 1,  # number of search region, only support 1 for now.\n",
    "    \"SIZE\": 256,\n",
    "    \"FACTOR\": 4.0,\n",
    "    \"CENTER_JITTER\": 3.5,\n",
    "    \"SCALE_JITTER\": 0.5\n",
    "}\n",
    "\n",
    "# DATA.TEMPLATE\n",
    "cfg[\"DATA\"][\"TEMPLATE\"] = {\n",
    "    \"NUMBER\": 1,\n",
    "    \"SIZE\": 128,\n",
    "    \"FACTOR\": 2.0,\n",
    "    \"CENTER_JITTER\": 0,\n",
    "    \"SCALE_JITTER\": 0\n",
    "}\n",
    "\n",
    "# TEST\n",
    "cfg[\"TEST\"] = {\n",
    "    \"TEMPLATE_FACTOR\": 8.0,\n",
    "    \"TEMPLATE_SIZE\": 256,\n",
    "    \"SEARCH_FACTOR\": 1.7,\n",
    "    \"SEARCH_SIZE\": 128,\n",
    "    \"EPOCH\": 500,\n",
    "    \"WINDOW\": False,  # window penalty\n",
    "    \"NUM_TEMPLATES\": 1\n",
    "}\n",
    "\n",
    "# TEST.UPT\n",
    "cfg[\"TEST\"][\"UPT\"] = {\n",
    "    \"DEFAULT\": 1,\n",
    "    \"LASOT\": 0,\n",
    "    \"LASOT_EXTENSION_SUBSET\": 0,\n",
    "    \"TRACKINGNET\": 0,\n",
    "    \"TNL2K\": 0,\n",
    "    \"NFS\": 0,\n",
    "    \"UAV\": 0,\n",
    "    \"VOT20\": 0,\n",
    "    \"GOT10K_TEST\": 0\n",
    "}\n",
    "\n",
    "# TEST.UPH\n",
    "cfg[\"TEST\"][\"UPH\"] = {\n",
    "    \"DEFAULT\": 1,\n",
    "    \"LASOT\": 0,\n",
    "    \"LASOT_EXTENSION_SUBSET\": 0,\n",
    "    \"TRACKINGNET\": 0,\n",
    "    \"TNL2K\": 0,\n",
    "    \"NFS\": 0,\n",
    "    \"UAV\": 0,\n",
    "    \"VOT20\": 0,\n",
    "    \"GOT10K_TEST\": 0\n",
    "}\n",
    "\n",
    "# TEST.INTER\n",
    "cfg[\"TEST\"][\"INTER\"] = {\n",
    "    \"DEFAULT\": 999999,\n",
    "    \"LASOT\": 0,\n",
    "    \"LASOT_EXTENSION_SUBSET\": 0,\n",
    "    \"TRACKINGNET\": 0,\n",
    "    \"TNL2K\": 0,\n",
    "    \"NFS\": 0,\n",
    "    \"UAV\": 0,\n",
    "    \"VOT20\": 0,\n",
    "    \"GOT10K_TEST\": 0\n",
    "}\n",
    "\n",
    "# TEST.MB\n",
    "cfg[\"TEST\"][\"MB\"] = {\n",
    "    \"DEFAULT\": 500,\n",
    "    \"LASOT\": 0,\n",
    "    \"LASOT_EXTENSION_SUBSET\": 0,\n",
    "    \"TRACKINGNET\": 0,\n",
    "    \"TNL2K\": 0,\n",
    "    \"NFS\": 0,\n",
    "    \"UAV\": 0,\n",
    "    \"VOT20\": 0,\n",
    "    \"GOT10K_TEST\": 0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5f2804bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test config:  {'MODEL': {'ENCODER': {'TYPE': 'fastitpnt', 'DROP_PATH': 0.1, 'PRETRAIN_TYPE': './fast_itpn_tiny_1600e_1k.pt', 'USE_CHECKPOINT': False, 'STRIDE': 16, 'POS_TYPE': 'index', 'TOKEN_TYPE_INDICATE': True, 'INTERACTION_INDEXES': [[4, 7], [7, 10], [10, 13], [13, 16]], 'GRAD_CKPT': False}, 'NECK': {'N_LAYERS': 4, 'D_MODEL': 384, 'D_STATE': 16}, 'DECODER': {'TYPE': 'CENTER', 'NUM_CHANNELS': 256}}, 'TRAIN': {'LR': 0.0004, 'WEIGHT_DECAY': 0.0001, 'EPOCH': 300, 'LR_DROP_EPOCH': 240, 'BATCH_SIZE': 64, 'NUM_WORKER': 10, 'OPTIMIZER': 'ADAMW', 'ENCODER_MULTIPLIER': 0.1, 'FREEZE_ENCODER': False, 'ENCODER_OPEN': [], 'CE_WEIGHT': 1.0, 'GIOU_WEIGHT': 2.0, 'L1_WEIGHT': 5.0, 'PRINT_INTERVAL': 50, 'GRAD_CLIP_NORM': 0.1, 'FIX_BN': False, 'ENCODER_W': '', 'TYPE': 'normal', 'PRETRAINED_PATH': None, 'SCHEDULER': {'TYPE': 'step', 'DECAY_RATE': 0.1}}, 'DATA': {'MEAN': [0.485, 0.456, 0.406], 'STD': [0.229, 0.224, 0.225], 'MAX_SAMPLE_INTERVAL': 400, 'SAMPLER_MODE': 'order', 'LOADER': 'tracking', 'TRAIN': {'DATASETS_NAME': ['LASOT', 'GOT10K_vottrain', 'COCO17', 'TRACKINGNET', 'VASTTRACK'], 'DATASETS_RATIO': [1, 1, 1, 1, 1], 'SAMPLE_PER_EPOCH': 60000}, 'SEARCH': {'NUMBER': 2, 'SIZE': 224, 'FACTOR': 4.0, 'CENTER_JITTER': 3.5, 'SCALE_JITTER': 0.5}, 'TEMPLATE': {'NUMBER': 5, 'SIZE': 112, 'FACTOR': 2.0, 'CENTER_JITTER': 0, 'SCALE_JITTER': 0}}, 'TEST': {'TEMPLATE_FACTOR': 2.0, 'TEMPLATE_SIZE': 112, 'SEARCH_FACTOR': 4.0, 'SEARCH_SIZE': 224, 'EPOCH': 300, 'WINDOW': True, 'NUM_TEMPLATES': 5, 'UPT': {'DEFAULT': 1, 'LASOT': 0.8, 'LASOT_EXTENSION_SUBSET': 0.85, 'TRACKINGNET': 0.5, 'TNL2K': 0.5, 'NFS': 0.8, 'UAV': 0.2, 'VOT20': 0.4, 'GOT10K_TEST': 0}, 'UPH': {'DEFAULT': 1, 'LASOT': 0.88, 'LASOT_EXTENSION_SUBSET': 0.97, 'TRACKINGNET': 0.9, 'TNL2K': 0.9, 'NFS': 0.92, 'UAV': 0.91, 'VOT20': 0.94, 'GOT10K_TEST': 0}, 'INTER': {'DEFAULT': 999999, 'LASOT': 70, 'LASOT_EXTENSION_SUBSET': 50, 'TRACKINGNET': 20, 'TNL2K': 20, 'NFS': 90, 'UAV': 1, 'VOT20': 1, 'GOT10K_TEST': 0}, 'MB': {'DEFAULT': 500, 'LASOT': 500, 'LASOT_EXTENSION_SUBSET': 500, 'TRACKINGNET': 200, 'TNL2K': 500, 'NFS': 500, 'UAV': 400, 'VOT20': 500, 'GOT10K_TEST': 0}}}\n"
     ]
    }
   ],
   "source": [
    "#Params\n",
    "class TrackerParams:\n",
    "    \"\"\"Class for tracker parameters.\"\"\"\n",
    "    def set_default_values(self, default_vals: dict):\n",
    "        for name, val in default_vals.items():\n",
    "            if not hasattr(self, name):\n",
    "                setattr(self, name, val)\n",
    "\n",
    "    def get(self, name: str, *default):\n",
    "        \"\"\"Get a parameter value with the given name. If it does not exists, it return the default value given as a\n",
    "        second argument or returns an error if no default value is given.\"\"\"\n",
    "        if len(default) > 1:\n",
    "            raise ValueError('Can only give one default value.')\n",
    "\n",
    "        if not default:\n",
    "            return getattr(self, name)\n",
    "\n",
    "        return getattr(self, name, default[0])\n",
    "\n",
    "    def has(self, name: str):\n",
    "        \"\"\"Check if there exist a parameter with the given name.\"\"\"\n",
    "        return hasattr(self, name)\n",
    "\n",
    "def _update_config(base_cfg, exp_cfg):\n",
    "    if isinstance(base_cfg, dict) and isinstance(exp_cfg, dict):\n",
    "        for k, v in exp_cfg.items():\n",
    "            if k in base_cfg:\n",
    "                if not isinstance(v, dict):\n",
    "                    base_cfg[k] = v\n",
    "                else:\n",
    "                    _update_config(base_cfg[k], v)\n",
    "            else:\n",
    "                raise ValueError(\"{} not exist in config.py\".format(k))\n",
    "    else:\n",
    "        return\n",
    "\n",
    "def update_config_from_file(filename):\n",
    "    exp_config = None\n",
    "    with open(filename) as f:\n",
    "        exp_config = yaml.safe_load(f)\n",
    "        _update_config(cfg, exp_config)\n",
    "    \n",
    "def parameters(yaml_name: str):\n",
    "    params = TrackerParams()\n",
    "\n",
    "    yaml_file = \"mcitrack_t224.yaml\"\n",
    "    update_config_from_file(yaml_file)\n",
    "    params.cfg = cfg\n",
    "    print(\"test config: \", cfg)\n",
    "\n",
    "    params.yaml_name = yaml_name\n",
    "    # template and search region\n",
    "    params.template_factor = cfg[\"TEST\"][\"TEMPLATE_FACTOR\"]\n",
    "    params.template_size = cfg[\"TEST\"][\"TEMPLATE_SIZE\"]\n",
    "    params.search_factor = cfg[\"TEST\"][\"SEARCH_FACTOR\"]\n",
    "    params.search_size = cfg[\"TEST\"][\"SEARCH_SIZE\"]\n",
    "\n",
    "    # Network checkpoint path\n",
    "    params.checkpoint = \"fast_itpn_tiny_1600e_1k.pt\"\n",
    "    # whether to save boxes from all queries\n",
    "    params.save_all_boxes = False\n",
    "\n",
    "    return params\n",
    "\n",
    "params = parameters(\"./mcitrack_t224.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ae331c67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update threshold is:  500\n"
     ]
    }
   ],
   "source": [
    "tracker = MCITRACK(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "097a57d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "network = build_mcitrack(params.cfg)\n",
    "network.load_state_dict(torch.load(\"MCITRACK_ep0300.pth.tar\", map_location='cpu')['net'], strict=True)\n",
    "cfg = params.cfg\n",
    "network = network.cuda()\n",
    "network.eval()\n",
    "list1 = [torch.zeros(1, 3, 112, 112).to('cuda') for _ in range(5)]  # 5 тензоров размером [1, 3, 112, 112]\n",
    "list2 = [torch.zeros(1, 3, 224, 224).to('cuda')]                    # 1 тензор размером [1, 3, 224, 224]\n",
    "list3 = [torch.zeros(1, 4).to('cuda') for _ in range(5)]\n",
    "res = network.forward(list1,list2,list3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa6b8c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_17528\\1057810194.py:196: TracerWarning: Converting a tensor to a Python integer might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  x = x.transpose(1,2).view(B,C,int(N**0.5),int(N**0.5)).contiguous()\n"
     ]
    }
   ],
   "source": [
    "#ModelWrapper jit\n",
    "class ModelWrapper(torch.nn.Module):\n",
    "    def __init__(self, original_model):\n",
    "        super(ModelWrapper, self).__init__()\n",
    "        self.original_model = original_model\n",
    "\n",
    "    def forward(self, template_list, search_list,template_anno_list):\n",
    "        \n",
    "        output_dict = self.original_model(template_list, search_list,template_anno_list)\n",
    "        \n",
    "        \n",
    "        return (output_dict['pred_boxes'],\n",
    "                output_dict['score_map'],\n",
    "                output_dict['size_map'],\n",
    "                output_dict['offset_map'])\n",
    "\n",
    "\n",
    "model = network\n",
    "model.eval()\n",
    "\n",
    "\n",
    "wrapped_model = ModelWrapper(model)\n",
    "\n",
    "\n",
    "template_list = [torch.zeros(1, 3, 112, 112).to('cuda') for _ in range(5)]  # 5 тензоров размером [1, 3, 112, 112]\n",
    "search_list = [torch.zeros(1, 3, 224, 224).to('cuda')]                    # 1 тензор размером [1, 3, 224, 224]\n",
    "template_anno_list = [torch.zeros(1, 4).to('cuda') for _ in range(5)]\n",
    "\n",
    "traced_model = torch.jit.trace(wrapped_model, (template_list, search_list,template_anno_list))\n",
    "\n",
    "\n",
    "optimized_model = torch.jit.optimize_for_inference(traced_model)\n",
    "\n",
    "\n",
    "optimized_model.save(\"MCITrack.pt\")\n",
    "\n",
    "\n",
    "loaded_model = torch.jit.load(\"MCITrack.pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = loaded_model(template_list, search_list,template_anno_list)\n",
    "\n",
    "for output in outputs:\n",
    "    print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "99787837",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_16344\\2631578009.py:196: TracerWarning: Converting a tensor to a Python integer might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  x = x.transpose(1,2).view(B,C,int(N**0.5),int(N**0.5)).contiguous()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model has been exported to MCITrack.onnx\n"
     ]
    }
   ],
   "source": [
    "#ModelWrapper onnx\n",
    "import torch\n",
    "\n",
    "\n",
    "class ModelWrapper(torch.nn.Module):\n",
    "    def __init__(self, original_model):\n",
    "        super(ModelWrapper, self).__init__()\n",
    "        self.original_model = original_model\n",
    "\n",
    "    def forward(self, template_list, search_list,template_anno_list):\n",
    "        \n",
    "        output_dict = self.original_model(template_list, search_list,template_anno_list)\n",
    "        \n",
    "        \n",
    "        return (output_dict['pred_boxes'],\n",
    "                output_dict['score_map'],\n",
    "                output_dict['size_map'],\n",
    "                output_dict['offset_map'])\n",
    "\n",
    "\n",
    "\n",
    "model = network\n",
    "model.eval()\n",
    "\n",
    "\n",
    "wrapped_model = ModelWrapper(model)\n",
    "\n",
    "template_list = [torch.zeros(1, 3, 112, 112).to('cuda') for _ in range(5)]  # 5 тензоров размером [1, 3, 112, 112]\n",
    "search_list = [torch.zeros(1, 3, 224, 224).to('cuda')]                    # 1 тензор размером [1, 3, 224, 224]\n",
    "template_anno_list = [torch.zeros(1, 4).to('cuda') for _ in range(5)]\n",
    "\n",
    "# Важно: для onnx-модели модель должна быть на cpu или cuda, и входы должны быть на том же устройстве.\n",
    "wrapped_model = wrapped_model.to('cuda')\n",
    "wrapped_model.eval()\n",
    "\n",
    "# Указываем пути для сохранения\n",
    "onnx_path = \"MCITrack.onnx\"\n",
    "\n",
    "# Экспортируем модель в ONNX\n",
    "torch.onnx.export(\n",
    "    wrapped_model,                                   # Модель\n",
    "    (template_list, search_list,template_anno_list),                                # Входные данные (tuple)\n",
    "    onnx_path,                                       # Имя файла\n",
    "    export_params=True,                              # Экспортировать параметры (веса)\n",
    "    opset_version=17,                                # Версия ONNX opset\n",
    "    do_constant_folding=True,                        # Оптимизация констант\n",
    "    input_names = ['template_list', 'search_list','template_anno_list'],                        # Имена входов\n",
    "    output_names = ['pred_boxes','score_map','size_map','offset_map'],                   # Имена выходов\n",
    "    #dynamic_axes={'z': {0: 'batch_size'},            # Динамическая ось для батча\n",
    "    #              'x': {0: 'batch_size'},\n",
    "    #              'pred_boxes': {0: 'batch_size'}},\n",
    "    verbose=True                                     # Показывать подробности\n",
    ")\n",
    "\n",
    "print(f'Model has been exported to {onnx_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1666b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRT\n",
    "import tensorrt as trt\n",
    "#trtexec --onnx=MCITrack.onnx  --saveEngine=MCITrack.trt  --fp16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "1ae7ecd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bbox: [502.4700000000001, 428.75, 56.379999999999995, 20.96]\n",
      "Can't read frame\n",
      "{'metadata': [{'frame': 1, 'bbox': [0.4145781250000001, 0.6100416666666667, 0.044046875, 0.029111111111111112]}, {'frame': 2, 'bbox': [0.41640625, 0.6097222222222223, 0.04375, 0.027777777777777776]}, {'frame': 3, 'bbox': [0.41875, 0.6104166666666667, 0.04375, 0.029166666666666667]}, {'frame': 4, 'bbox': [0.421875, 0.6097222222222223, 0.04375, 0.027777777777777776]}, {'frame': 5, 'bbox': [0.42421875, 0.6097222222222223, 0.04375, 0.027777777777777776]}, {'frame': 6, 'bbox': [0.42734375, 0.6083333333333333, 0.04375, 0.027777777777777776]}, {'frame': 7, 'bbox': [0.42890625, 0.6083333333333333, 0.04375, 0.027777777777777776]}, {'frame': 8, 'bbox': [0.43125, 0.6097222222222223, 0.04375, 0.027777777777777776]}, {'frame': 9, 'bbox': [0.434375, 0.6083333333333333, 0.04375, 0.027777777777777776]}, {'frame': 10, 'bbox': [0.43671875, 0.6083333333333333, 0.04375, 0.027777777777777776]}, {'frame': 11, 'bbox': [0.438671875, 0.6083333333333333, 0.04296875, 0.027777777777777776]}, {'frame': 12, 'bbox': [0.441796875, 0.6083333333333333, 0.04296875, 0.027777777777777776]}, {'frame': 13, 'bbox': [0.444140625, 0.6083333333333333, 0.04296875, 0.027777777777777776]}, {'frame': 14, 'bbox': [0.445703125, 0.6090277777777777, 0.04296875, 0.029166666666666667]}, {'frame': 15, 'bbox': [0.4484375, 0.6083333333333333, 0.0421875, 0.027777777777777776]}, {'frame': 16, 'bbox': [0.451171875, 0.6083333333333333, 0.04296875, 0.027777777777777776]}, {'frame': 17, 'bbox': [0.453515625, 0.6083333333333333, 0.04296875, 0.027777777777777776]}, {'frame': 18, 'bbox': [0.455859375, 0.6076388888888888, 0.04296875, 0.029166666666666667]}, {'frame': 19, 'bbox': [0.458203125, 0.6090277777777777, 0.04296875, 0.029166666666666667]}, {'frame': 20, 'bbox': [0.46015625, 0.6090277777777777, 0.0421875, 0.029166666666666667]}, {'frame': 21, 'bbox': [0.4625, 0.6069444444444444, 0.0421875, 0.027777777777777776]}, {'frame': 22, 'bbox': [0.4640625, 0.6090277777777777, 0.0421875, 0.029166666666666667]}, {'frame': 23, 'bbox': [0.46640625, 0.6083333333333333, 0.0421875, 0.027777777777777776]}, {'frame': 24, 'bbox': [0.468359375, 0.6083333333333333, 0.04140625, 0.027777777777777776]}, {'frame': 25, 'bbox': [0.469921875, 0.6083333333333333, 0.04140625, 0.027777777777777776]}, {'frame': 26, 'bbox': [0.472265625, 0.6083333333333333, 0.04140625, 0.027777777777777776]}, {'frame': 27, 'bbox': [0.474609375, 0.6076388888888888, 0.04140625, 0.029166666666666667]}, {'frame': 28, 'bbox': [0.476171875, 0.6076388888888888, 0.04140625, 0.029166666666666667]}, {'frame': 29, 'bbox': [0.478515625, 0.6090277777777777, 0.04140625, 0.029166666666666667]}, {'frame': 30, 'bbox': [0.4796875, 0.6090277777777777, 0.040625, 0.029166666666666667]}, {'frame': 31, 'bbox': [0.483203125, 0.6076388888888888, 0.04140625, 0.029166666666666667]}, {'frame': 32, 'bbox': [0.484375, 0.6069444444444444, 0.040625, 0.027777777777777776]}, {'frame': 33, 'bbox': [0.48671875, 0.6069444444444444, 0.040625, 0.027777777777777776]}, {'frame': 34, 'bbox': [0.48828125, 0.6069444444444444, 0.040625, 0.027777777777777776]}, {'frame': 35, 'bbox': [0.489453125, 0.6069444444444444, 0.03984375, 0.027777777777777776]}, {'frame': 36, 'bbox': [0.491015625, 0.6083333333333333, 0.03984375, 0.027777777777777776]}, {'frame': 37, 'bbox': [0.49296875, 0.6069444444444444, 0.0390625, 0.027777777777777776]}, {'frame': 38, 'bbox': [0.49453125, 0.6069444444444444, 0.0390625, 0.027777777777777776]}, {'frame': 39, 'bbox': [0.496875, 0.6083333333333333, 0.0390625, 0.027777777777777776]}, {'frame': 40, 'bbox': [0.4984375, 0.6069444444444444, 0.0390625, 0.027777777777777776]}, {'frame': 41, 'bbox': [0.500390625, 0.6069444444444444, 0.03828125, 0.027777777777777776]}, {'frame': 42, 'bbox': [0.501953125, 0.6069444444444444, 0.03828125, 0.027777777777777776]}, {'frame': 43, 'bbox': [0.503515625, 0.6083333333333333, 0.03828125, 0.027777777777777776]}, {'frame': 44, 'bbox': [0.505078125, 0.6069444444444444, 0.03828125, 0.027777777777777776]}, {'frame': 45, 'bbox': [0.50625, 0.6069444444444444, 0.0375, 0.027777777777777776]}, {'frame': 46, 'bbox': [0.5078125, 0.6083333333333333, 0.0375, 0.027777777777777776]}, {'frame': 47, 'bbox': [0.509765625, 0.6069444444444444, 0.03671875, 0.027777777777777776]}, {'frame': 48, 'bbox': [0.511328125, 0.6069444444444444, 0.03671875, 0.027777777777777776]}, {'frame': 49, 'bbox': [0.512890625, 0.6069444444444444, 0.03671875, 0.027777777777777776]}, {'frame': 50, 'bbox': [0.514453125, 0.6069444444444444, 0.03671875, 0.027777777777777776]}, {'frame': 51, 'bbox': [0.515625, 0.6069444444444444, 0.0359375, 0.027777777777777776]}, {'frame': 52, 'bbox': [0.516796875, 0.6069444444444444, 0.03515625, 0.027777777777777776]}, {'frame': 53, 'bbox': [0.518359375, 0.6069444444444444, 0.03515625, 0.027777777777777776]}, {'frame': 54, 'bbox': [0.519921875, 0.6069444444444444, 0.03515625, 0.027777777777777776]}, {'frame': 55, 'bbox': [0.52109375, 0.6069444444444444, 0.034375, 0.027777777777777776]}, {'frame': 56, 'bbox': [0.52265625, 0.60625, 0.034375, 0.02638888888888889]}, {'frame': 57, 'bbox': [0.523046875, 0.6069444444444444, 0.03359375, 0.027777777777777776]}, {'frame': 58, 'bbox': [0.524609375, 0.6055555555555555, 0.03359375, 0.027777777777777776]}, {'frame': 59, 'bbox': [0.526171875, 0.60625, 0.03359375, 0.02638888888888889]}, {'frame': 60, 'bbox': [0.52734375, 0.6048611111111111, 0.0328125, 0.02638888888888889]}, {'frame': 61, 'bbox': [0.52890625, 0.60625, 0.0328125, 0.02638888888888889]}, {'frame': 62, 'bbox': [0.529296875, 0.6048611111111111, 0.03203125, 0.02638888888888889]}, {'frame': 63, 'bbox': [0.530859375, 0.60625, 0.03203125, 0.02638888888888889]}, {'frame': 64, 'bbox': [0.532421875, 0.60625, 0.03203125, 0.02638888888888889]}, {'frame': 65, 'bbox': [0.5328125, 0.6048611111111111, 0.03125, 0.02638888888888889]}, {'frame': 66, 'bbox': [0.533984375, 0.6048611111111111, 0.03046875, 0.02638888888888889]}, {'frame': 67, 'bbox': [0.5359375, 0.6048611111111111, 0.03125, 0.02638888888888889]}, {'frame': 68, 'bbox': [0.536328125, 0.6048611111111111, 0.03046875, 0.02638888888888889]}, {'frame': 69, 'bbox': [0.537109375, 0.6048611111111111, 0.03046875, 0.02638888888888889]}, {'frame': 70, 'bbox': [0.538671875, 0.6041666666666666, 0.03046875, 0.027777777777777776]}, {'frame': 71, 'bbox': [0.53984375, 0.6048611111111111, 0.0296875, 0.02638888888888889]}, {'frame': 72, 'bbox': [0.540625, 0.6048611111111111, 0.0296875, 0.02638888888888889]}, {'frame': 73, 'bbox': [0.541796875, 0.6048611111111111, 0.02890625, 0.02638888888888889]}, {'frame': 74, 'bbox': [0.542578125, 0.6048611111111111, 0.02890625, 0.02638888888888889]}, {'frame': 75, 'bbox': [0.543359375, 0.6041666666666666, 0.02890625, 0.027777777777777776]}, {'frame': 76, 'bbox': [0.54453125, 0.6034722222222222, 0.028125, 0.02638888888888889]}, {'frame': 77, 'bbox': [0.5453125, 0.6041666666666666, 0.028125, 0.027777777777777776]}, {'frame': 78, 'bbox': [0.54609375, 0.6041666666666666, 0.028125, 0.027777777777777776]}, {'frame': 79, 'bbox': [0.547265625, 0.6027777777777777, 0.02734375, 0.027777777777777776]}, {'frame': 80, 'bbox': [0.548046875, 0.6034722222222222, 0.02734375, 0.02638888888888889]}, {'frame': 81, 'bbox': [0.548828125, 0.6034722222222222, 0.02734375, 0.02638888888888889]}, {'frame': 82, 'bbox': [0.55, 0.6034722222222222, 0.0265625, 0.02638888888888889]}, {'frame': 83, 'bbox': [0.55078125, 0.6034722222222222, 0.0265625, 0.02638888888888889]}, {'frame': 84, 'bbox': [0.55234375, 0.6034722222222222, 0.0265625, 0.02638888888888889]}, {'frame': 85, 'bbox': [0.551953125, 0.6034722222222222, 0.02578125, 0.02638888888888889]}, {'frame': 86, 'bbox': [0.553515625, 0.6034722222222222, 0.02578125, 0.02638888888888889]}, {'frame': 87, 'bbox': [0.553125, 0.6034722222222222, 0.025, 0.02638888888888889]}, {'frame': 88, 'bbox': [0.5546875, 0.6034722222222222, 0.025, 0.02638888888888889]}, {'frame': 89, 'bbox': [0.5546875, 0.6020833333333333, 0.025, 0.02638888888888889]}, {'frame': 90, 'bbox': [0.55625, 0.6013888888888889, 0.025, 0.025]}, {'frame': 91, 'bbox': [0.55703125, 0.6027777777777777, 0.025, 0.025]}, {'frame': 92, 'bbox': [0.55703125, 0.6020833333333333, 0.025, 0.02638888888888889]}, {'frame': 93, 'bbox': [0.558203125, 0.6020833333333333, 0.02421875, 0.02638888888888889]}, {'frame': 94, 'bbox': [0.55859375, 0.6020833333333333, 0.0234375, 0.02638888888888889]}, {'frame': 95, 'bbox': [0.55859375, 0.6006944444444444, 0.0234375, 0.02638888888888889]}, {'frame': 96, 'bbox': [0.56015625, 0.6006944444444444, 0.0234375, 0.02638888888888889]}, {'frame': 97, 'bbox': [0.560546875, 0.6020833333333333, 0.02265625, 0.02638888888888889]}, {'frame': 98, 'bbox': [0.560546875, 0.6006944444444444, 0.02265625, 0.02638888888888889]}, {'frame': 99, 'bbox': [0.562109375, 0.6, 0.02265625, 0.025]}, {'frame': 100, 'bbox': [0.562109375, 0.6, 0.02265625, 0.025]}, {'frame': 101, 'bbox': [0.562890625, 0.6013888888888889, 0.02265625, 0.025]}, {'frame': 102, 'bbox': [0.56328125, 0.6013888888888889, 0.021875, 0.025]}, {'frame': 103, 'bbox': [0.5640625, 0.6013888888888889, 0.021875, 0.025]}, {'frame': 104, 'bbox': [0.5640625, 0.6, 0.021875, 0.025]}, {'frame': 105, 'bbox': [0.565625, 0.6, 0.021875, 0.025]}, {'frame': 106, 'bbox': [0.565625, 0.6, 0.021875, 0.025]}, {'frame': 107, 'bbox': [0.566015625, 0.6, 0.02109375, 0.025]}, {'frame': 108, 'bbox': [0.566796875, 0.6, 0.02109375, 0.025]}, {'frame': 109, 'bbox': [0.566796875, 0.6, 0.02109375, 0.025]}, {'frame': 110, 'bbox': [0.567578125, 0.5986111111111111, 0.02109375, 0.025]}, {'frame': 111, 'bbox': [0.568359375, 0.6, 0.02109375, 0.025]}, {'frame': 112, 'bbox': [0.568359375, 0.6, 0.02109375, 0.025]}, {'frame': 113, 'bbox': [0.569140625, 0.6, 0.02109375, 0.025]}, {'frame': 114, 'bbox': [0.569140625, 0.5993055555555555, 0.02109375, 0.02361111111111111]}, {'frame': 115, 'bbox': [0.56953125, 0.5993055555555555, 0.0203125, 0.02361111111111111]}, {'frame': 116, 'bbox': [0.5703125, 0.6006944444444444, 0.0203125, 0.02361111111111111]}, {'frame': 117, 'bbox': [0.570703125, 0.5993055555555555, 0.02109375, 0.02361111111111111]}, {'frame': 118, 'bbox': [0.571875, 0.5993055555555555, 0.0203125, 0.02361111111111111]}, {'frame': 119, 'bbox': [0.571875, 0.5979166666666667, 0.0203125, 0.02361111111111111]}, {'frame': 120, 'bbox': [0.57265625, 0.5979166666666667, 0.0203125, 0.02361111111111111]}, {'frame': 121, 'bbox': [0.57265625, 0.5986111111111111, 0.0203125, 0.025]}, {'frame': 122, 'bbox': [0.5734375, 0.5986111111111111, 0.0203125, 0.025]}, {'frame': 123, 'bbox': [0.57421875, 0.5986111111111111, 0.0203125, 0.025]}, {'frame': 124, 'bbox': [0.57421875, 0.5986111111111111, 0.0203125, 0.025]}, {'frame': 125, 'bbox': [0.575, 0.5972222222222222, 0.0203125, 0.025]}, {'frame': 126, 'bbox': [0.579296875, 0.5986111111111111, 0.02734375, 0.025]}, {'frame': 127, 'bbox': [0.583203125, 0.5979166666666667, 0.03515625, 0.02638888888888889]}, {'frame': 128, 'bbox': [0.58671875, 0.5965277777777778, 0.0421875, 0.029166666666666667]}, {'frame': 129, 'bbox': [0.57890625, 0.5979166666666667, 0.0234375, 0.02361111111111111]}, {'frame': 130, 'bbox': [0.580859375, 0.5972222222222222, 0.02734375, 0.025]}, {'frame': 131, 'bbox': [0.577734375, 0.5979166666666667, 0.01953125, 0.02361111111111111]}, {'frame': 132, 'bbox': [0.578125, 0.5972222222222222, 0.0203125, 0.025]}, {'frame': 133, 'bbox': [0.58046875, 0.5972222222222222, 0.0234375, 0.025]}, {'frame': 134, 'bbox': [0.583203125, 0.5972222222222222, 0.02734375, 0.025]}, {'frame': 135, 'bbox': [0.57890625, 0.5972222222222222, 0.01875, 0.025]}, {'frame': 136, 'bbox': [0.58203125, 0.5958333333333333, 0.0234375, 0.025]}, {'frame': 137, 'bbox': [0.585546875, 0.5958333333333333, 0.03046875, 0.025]}, {'frame': 138, 'bbox': [0.583984375, 0.5958333333333333, 0.02578125, 0.025]}, {'frame': 139, 'bbox': [0.582421875, 0.5951388888888889, 0.02109375, 0.02361111111111111]}, {'frame': 140, 'bbox': [0.588671875, 0.5958333333333333, 0.03359375, 0.025]}, {'frame': 141, 'bbox': [0.58671875, 0.5958333333333333, 0.028125, 0.025]}, {'frame': 142, 'bbox': [0.59140625, 0.5965277777777778, 0.0375, 0.02638888888888889]}, {'frame': 143, 'bbox': [0.58359375, 0.5979166666666667, 0.0203125, 0.02361111111111111]}, {'frame': 144, 'bbox': [0.583203125, 0.5965277777777778, 0.01953125, 0.02361111111111111]}, {'frame': 145, 'bbox': [0.58359375, 0.5965277777777778, 0.01875, 0.02361111111111111]}, {'frame': 146, 'bbox': [0.584375, 0.5965277777777778, 0.01875, 0.02361111111111111]}, {'frame': 147, 'bbox': [0.58515625, 0.5979166666666667, 0.01875, 0.02361111111111111]}, {'frame': 148, 'bbox': [0.58515625, 0.5965277777777778, 0.01875, 0.02361111111111111]}, {'frame': 149, 'bbox': [0.585546875, 0.5958333333333333, 0.01796875, 0.022222222222222223]}, {'frame': 150, 'bbox': [0.5859375, 0.5958333333333333, 0.01875, 0.022222222222222223]}, {'frame': 151, 'bbox': [0.586328125, 0.5958333333333333, 0.01796875, 0.022222222222222223]}, {'frame': 152, 'bbox': [0.586328125, 0.5958333333333333, 0.01796875, 0.022222222222222223]}, {'frame': 153, 'bbox': [0.587109375, 0.5958333333333333, 0.01796875, 0.022222222222222223]}, {'frame': 154, 'bbox': [0.587109375, 0.5958333333333333, 0.01796875, 0.022222222222222223]}, {'frame': 155, 'bbox': [0.587890625, 0.5965277777777778, 0.01796875, 0.02361111111111111]}, {'frame': 156, 'bbox': [0.587890625, 0.5965277777777778, 0.01796875, 0.02361111111111111]}, {'frame': 157, 'bbox': [0.589453125, 0.5972222222222222, 0.01796875, 0.022222222222222223]}, {'frame': 158, 'bbox': [0.589453125, 0.5958333333333333, 0.01796875, 0.022222222222222223]}, {'frame': 159, 'bbox': [0.589453125, 0.5958333333333333, 0.01796875, 0.022222222222222223]}, {'frame': 160, 'bbox': [0.590234375, 0.5958333333333333, 0.01796875, 0.022222222222222223]}, {'frame': 161, 'bbox': [0.590234375, 0.5958333333333333, 0.01796875, 0.022222222222222223]}, {'frame': 162, 'bbox': [0.591015625, 0.5958333333333333, 0.01796875, 0.022222222222222223]}, {'frame': 163, 'bbox': [0.591796875, 0.5958333333333333, 0.01796875, 0.022222222222222223]}, {'frame': 164, 'bbox': [0.591796875, 0.5965277777777778, 0.01796875, 0.02361111111111111]}, {'frame': 165, 'bbox': [0.592578125, 0.5965277777777778, 0.01796875, 0.02361111111111111]}, {'frame': 166, 'bbox': [0.592578125, 0.5958333333333333, 0.01796875, 0.022222222222222223]}, {'frame': 167, 'bbox': [0.592578125, 0.5958333333333333, 0.01796875, 0.022222222222222223]}, {'frame': 168, 'bbox': [0.593359375, 0.5958333333333333, 0.01796875, 0.022222222222222223]}, {'frame': 169, 'bbox': [0.59296875, 0.5951388888888889, 0.0171875, 0.020833333333333332]}, {'frame': 170, 'bbox': [0.59375, 0.5951388888888889, 0.0171875, 0.020833333333333332]}, {'frame': 171, 'bbox': [0.594140625, 0.5951388888888889, 0.01796875, 0.020833333333333332]}, {'frame': 172, 'bbox': [0.594140625, 0.5951388888888889, 0.01796875, 0.020833333333333332]}, {'frame': 173, 'bbox': [0.594921875, 0.5951388888888889, 0.01796875, 0.020833333333333332]}, {'frame': 174, 'bbox': [0.59453125, 0.5944444444444444, 0.01875, 0.022222222222222223]}, {'frame': 175, 'bbox': [0.5953125, 0.5930555555555556, 0.01875, 0.022222222222222223]}, {'frame': 176, 'bbox': [0.596484375, 0.5930555555555556, 0.01953125, 0.022222222222222223]}, {'frame': 177, 'bbox': [0.59609375, 0.5944444444444444, 0.0203125, 0.022222222222222223]}, {'frame': 178, 'bbox': [0.5984375, 0.59375, 0.0171875, 0.020833333333333332]}, {'frame': 179, 'bbox': [0.5984375, 0.5923611111111111, 0.0171875, 0.020833333333333332]}, {'frame': 180, 'bbox': [0.59765625, 0.59375, 0.01875, 0.020833333333333332]}, {'frame': 181, 'bbox': [0.5984375, 0.5923611111111111, 0.0171875, 0.020833333333333332]}, {'frame': 182, 'bbox': [0.598828125, 0.59375, 0.01796875, 0.020833333333333332]}, {'frame': 183, 'bbox': [0.598828125, 0.5923611111111111, 0.01640625, 0.020833333333333332]}, {'frame': 184, 'bbox': [0.5984375, 0.5923611111111111, 0.0171875, 0.020833333333333332]}, {'frame': 185, 'bbox': [0.599609375, 0.59375, 0.01640625, 0.020833333333333332]}, {'frame': 186, 'bbox': [0.6, 0.5923611111111111, 0.0171875, 0.020833333333333332]}, {'frame': 187, 'bbox': [0.600390625, 0.59375, 0.01640625, 0.020833333333333332]}, {'frame': 188, 'bbox': [0.6, 0.5923611111111111, 0.0171875, 0.020833333333333332]}, {'frame': 189, 'bbox': [0.60078125, 0.59375, 0.0171875, 0.020833333333333332]}, {'frame': 190, 'bbox': [0.6015625, 0.59375, 0.0171875, 0.020833333333333332]}, {'frame': 191, 'bbox': [0.601171875, 0.59375, 0.01640625, 0.020833333333333332]}, {'frame': 192, 'bbox': [0.601953125, 0.59375, 0.01640625, 0.020833333333333332]}, {'frame': 193, 'bbox': [0.602734375, 0.5923611111111111, 0.01640625, 0.020833333333333332]}, {'frame': 194, 'bbox': [0.602734375, 0.5923611111111111, 0.01640625, 0.020833333333333332]}, {'frame': 195, 'bbox': [0.603515625, 0.5923611111111111, 0.01640625, 0.020833333333333332]}, {'frame': 196, 'bbox': [0.60390625, 0.59375, 0.015625, 0.020833333333333332]}, {'frame': 197, 'bbox': [0.60390625, 0.59375, 0.015625, 0.020833333333333332]}, {'frame': 198, 'bbox': [0.6046875, 0.5923611111111111, 0.015625, 0.020833333333333332]}, {'frame': 199, 'bbox': [0.6046875, 0.5923611111111111, 0.015625, 0.020833333333333332]}, {'frame': 200, 'bbox': [0.6046875, 0.5916666666666667, 0.015625, 0.019444444444444445]}, {'frame': 201, 'bbox': [0.60546875, 0.5916666666666667, 0.015625, 0.019444444444444445]}, {'frame': 202, 'bbox': [0.60546875, 0.5930555555555556, 0.015625, 0.019444444444444445]}, {'frame': 203, 'bbox': [0.60625, 0.5916666666666667, 0.015625, 0.019444444444444445]}, {'frame': 204, 'bbox': [0.60625, 0.5930555555555556, 0.015625, 0.019444444444444445]}, {'frame': 205, 'bbox': [0.60703125, 0.5930555555555556, 0.015625, 0.019444444444444445]}, {'frame': 206, 'bbox': [0.6078125, 0.5916666666666667, 0.015625, 0.019444444444444445]}, {'frame': 207, 'bbox': [0.607421875, 0.5930555555555556, 0.01484375, 0.019444444444444445]}, {'frame': 208, 'bbox': [0.6078125, 0.5916666666666667, 0.015625, 0.019444444444444445]}, {'frame': 209, 'bbox': [0.6078125, 0.5930555555555556, 0.015625, 0.019444444444444445]}, {'frame': 210, 'bbox': [0.60859375, 0.5916666666666667, 0.015625, 0.019444444444444445]}, {'frame': 211, 'bbox': [0.609375, 0.5930555555555556, 0.015625, 0.019444444444444445]}, {'frame': 212, 'bbox': [0.609375, 0.5930555555555556, 0.015625, 0.019444444444444445]}, {'frame': 213, 'bbox': [0.608984375, 0.5930555555555556, 0.01484375, 0.019444444444444445]}, {'frame': 214, 'bbox': [0.61015625, 0.5930555555555556, 0.015625, 0.019444444444444445]}, {'frame': 215, 'bbox': [0.61015625, 0.5930555555555556, 0.015625, 0.019444444444444445]}, {'frame': 216, 'bbox': [0.61015625, 0.5916666666666667, 0.015625, 0.019444444444444445]}, {'frame': 217, 'bbox': [0.6109375, 0.5930555555555556, 0.015625, 0.019444444444444445]}, {'frame': 218, 'bbox': [0.61171875, 0.5916666666666667, 0.015625, 0.019444444444444445]}, {'frame': 219, 'bbox': [0.611328125, 0.5916666666666667, 0.01484375, 0.019444444444444445]}, {'frame': 220, 'bbox': [0.612109375, 0.5930555555555556, 0.01484375, 0.019444444444444445]}, {'frame': 221, 'bbox': [0.612109375, 0.5902777777777778, 0.01484375, 0.019444444444444445]}, {'frame': 222, 'bbox': [0.61171875, 0.5916666666666667, 0.0140625, 0.019444444444444445]}, {'frame': 223, 'bbox': [0.61171875, 0.5909722222222222, 0.0140625, 0.018055555555555554]}, {'frame': 224, 'bbox': [0.611328125, 0.5916666666666667, 0.01328125, 0.019444444444444445]}, {'frame': 225, 'bbox': [0.612109375, 0.5909722222222222, 0.01328125, 0.018055555555555554]}, {'frame': 226, 'bbox': [0.6125, 0.5916666666666667, 0.0125, 0.019444444444444445]}, {'frame': 227, 'bbox': [0.6125, 0.5916666666666667, 0.0125, 0.019444444444444445]}, {'frame': 228, 'bbox': [0.612109375, 0.5916666666666667, 0.01171875, 0.019444444444444445]}, {'frame': 229, 'bbox': [0.612890625, 0.5909722222222222, 0.01171875, 0.018055555555555554]}, {'frame': 230, 'bbox': [0.613671875, 0.5909722222222222, 0.01171875, 0.018055555555555554]}, {'frame': 231, 'bbox': [0.613671875, 0.5909722222222222, 0.01171875, 0.018055555555555554]}, {'frame': 232, 'bbox': [0.61328125, 0.5909722222222222, 0.0109375, 0.018055555555555554]}, {'frame': 233, 'bbox': [0.6140625, 0.5916666666666667, 0.0109375, 0.019444444444444445]}, {'frame': 234, 'bbox': [0.613671875, 0.5909722222222222, 0.01015625, 0.018055555555555554]}, {'frame': 235, 'bbox': [0.61484375, 0.5909722222222222, 0.0109375, 0.018055555555555554]}, {'frame': 236, 'bbox': [0.614453125, 0.5902777777777778, 0.01015625, 0.019444444444444445]}, {'frame': 237, 'bbox': [0.61484375, 0.5909722222222222, 0.009375, 0.018055555555555554]}, {'frame': 238, 'bbox': [0.615625, 0.5909722222222222, 0.0109375, 0.018055555555555554]}, {'frame': 239, 'bbox': [0.616015625, 0.5916666666666667, 0.01171875, 0.019444444444444445]}, {'frame': 240, 'bbox': [0.61640625, 0.5895833333333333, 0.0109375, 0.018055555555555554]}, {'frame': 241, 'bbox': [0.6171875, 0.5916666666666667, 0.0140625, 0.019444444444444445]}, {'frame': 242, 'bbox': [0.61796875, 0.5902777777777778, 0.0140625, 0.019444444444444445]}, {'frame': 243, 'bbox': [0.61875, 0.5909722222222222, 0.0140625, 0.018055555555555554]}, {'frame': 244, 'bbox': [0.61953125, 0.5909722222222222, 0.0140625, 0.018055555555555554]}, {'frame': 245, 'bbox': [0.619140625, 0.5909722222222222, 0.01328125, 0.018055555555555554]}, {'frame': 246, 'bbox': [0.61953125, 0.5895833333333333, 0.0140625, 0.018055555555555554]}, {'frame': 247, 'bbox': [0.6203125, 0.5902777777777778, 0.0140625, 0.016666666666666666]}, {'frame': 248, 'bbox': [0.61953125, 0.5909722222222222, 0.0140625, 0.018055555555555554]}, {'frame': 249, 'bbox': [0.62109375, 0.5909722222222222, 0.0140625, 0.018055555555555554]}, {'frame': 250, 'bbox': [0.6203125, 0.5909722222222222, 0.0140625, 0.018055555555555554]}, {'frame': 251, 'bbox': [0.620703125, 0.5909722222222222, 0.01328125, 0.018055555555555554]}, {'frame': 252, 'bbox': [0.621875, 0.5902777777777778, 0.0140625, 0.016666666666666666]}, {'frame': 253, 'bbox': [0.621484375, 0.5902777777777778, 0.01328125, 0.016666666666666666]}, {'frame': 254, 'bbox': [0.622265625, 0.5902777777777778, 0.01328125, 0.016666666666666666]}, {'frame': 255, 'bbox': [0.621875, 0.5902777777777778, 0.0140625, 0.016666666666666666]}, {'frame': 256, 'bbox': [0.62265625, 0.5902777777777778, 0.0140625, 0.016666666666666666]}, {'frame': 257, 'bbox': [0.6234375, 0.5909722222222222, 0.0140625, 0.018055555555555554]}, {'frame': 258, 'bbox': [0.6234375, 0.5909722222222222, 0.0140625, 0.018055555555555554]}, {'frame': 259, 'bbox': [0.623046875, 0.5902777777777778, 0.01328125, 0.016666666666666666]}, {'frame': 260, 'bbox': [0.6234375, 0.5895833333333333, 0.0140625, 0.018055555555555554]}, {'frame': 261, 'bbox': [0.623046875, 0.5909722222222222, 0.01328125, 0.018055555555555554]}, {'frame': 262, 'bbox': [0.623828125, 0.5909722222222222, 0.01328125, 0.018055555555555554]}, {'frame': 263, 'bbox': [0.625, 0.5888888888888889, 0.0125, 0.013888888888888888]}, {'frame': 264, 'bbox': [0.624609375, 0.5875, 0.01328125, 0.013888888888888888]}, {'frame': 265, 'bbox': [0.625, 0.5875, 0.0125, 0.013888888888888888]}, {'frame': 266, 'bbox': [0.625390625, 0.5875, 0.01328125, 0.013888888888888888]}, {'frame': 267, 'bbox': [0.62578125, 0.5875, 0.0109375, 0.013888888888888888]}, {'frame': 268, 'bbox': [0.6265625, 0.5888888888888889, 0.0109375, 0.013888888888888888]}, {'frame': 269, 'bbox': [0.6265625, 0.5875, 0.0125, 0.013888888888888888]}, {'frame': 270, 'bbox': [0.626953125, 0.5875, 0.01171875, 0.013888888888888888]}, {'frame': 271, 'bbox': [0.6265625, 0.5875, 0.0125, 0.013888888888888888]}, {'frame': 272, 'bbox': [0.6265625, 0.5875, 0.0125, 0.013888888888888888]}, {'frame': 273, 'bbox': [0.626953125, 0.5888888888888889, 0.01328125, 0.013888888888888888]}, {'frame': 274, 'bbox': [0.626953125, 0.5888888888888889, 0.01328125, 0.013888888888888888]}, {'frame': 275, 'bbox': [0.627734375, 0.5875, 0.01796875, 0.016666666666666666]}, {'frame': 276, 'bbox': [0.627734375, 0.5888888888888889, 0.01796875, 0.016666666666666666]}, {'frame': 277, 'bbox': [0.6265625, 0.5881944444444445, 0.0171875, 0.015277777777777777]}, {'frame': 278, 'bbox': [0.62734375, 0.5888888888888889, 0.0171875, 0.016666666666666666]}, {'frame': 279, 'bbox': [0.62734375, 0.5888888888888889, 0.01875, 0.016666666666666666]}, {'frame': 280, 'bbox': [0.626953125, 0.5875, 0.01796875, 0.016666666666666666]}, {'frame': 281, 'bbox': [0.628125, 0.5868055555555556, 0.01875, 0.018055555555555554]}, {'frame': 282, 'bbox': [0.627734375, 0.5888888888888889, 0.01953125, 0.016666666666666666]}, {'frame': 283, 'bbox': [0.62734375, 0.5861111111111111, 0.01875, 0.019444444444444445]}, {'frame': 284, 'bbox': [0.627734375, 0.5868055555555556, 0.01953125, 0.020833333333333332]}, {'frame': 285, 'bbox': [0.6296875, 0.5881944444444445, 0.015625, 0.015277777777777777]}, {'frame': 286, 'bbox': [0.630078125, 0.5888888888888889, 0.01484375, 0.016666666666666666]}, {'frame': 287, 'bbox': [0.630859375, 0.5895833333333333, 0.01484375, 0.015277777777777777]}, {'frame': 288, 'bbox': [0.6296875, 0.5881944444444445, 0.0140625, 0.015277777777777777]}, {'frame': 289, 'bbox': [0.6296875, 0.5875, 0.015625, 0.013888888888888888]}, {'frame': 290, 'bbox': [0.63125, 0.5895833333333333, 0.0125, 0.015277777777777777]}, {'frame': 291, 'bbox': [0.63203125, 0.5895833333333333, 0.0125, 0.015277777777777777]}, {'frame': 292, 'bbox': [0.63203125, 0.5888888888888889, 0.0125, 0.013888888888888888]}, {'frame': 293, 'bbox': [0.6328125, 0.5888888888888889, 0.0125, 0.013888888888888888]}, {'frame': 294, 'bbox': [0.63203125, 0.5888888888888889, 0.0125, 0.013888888888888888]}, {'frame': 295, 'bbox': [0.6328125, 0.5888888888888889, 0.0125, 0.013888888888888888]}, {'frame': 296, 'bbox': [0.63359375, 0.5888888888888889, 0.0125, 0.013888888888888888]}, {'frame': 297, 'bbox': [0.63359375, 0.5888888888888889, 0.0125, 0.013888888888888888]}, {'frame': 298, 'bbox': [0.63359375, 0.5888888888888889, 0.0125, 0.013888888888888888]}, {'frame': 299, 'bbox': [0.63359375, 0.5902777777777778, 0.0125, 0.013888888888888888]}, {'frame': 300, 'bbox': [0.63359375, 0.5888888888888889, 0.0125, 0.013888888888888888]}, {'frame': 301, 'bbox': [0.634375, 0.5902777777777778, 0.0125, 0.013888888888888888]}, {'frame': 302, 'bbox': [0.634375, 0.5888888888888889, 0.0125, 0.013888888888888888]}, {'frame': 303, 'bbox': [0.634375, 0.5888888888888889, 0.0125, 0.013888888888888888]}, {'frame': 304, 'bbox': [0.635546875, 0.5888888888888889, 0.01171875, 0.013888888888888888]}, {'frame': 305, 'bbox': [0.63515625, 0.5902777777777778, 0.0125, 0.013888888888888888]}, {'frame': 306, 'bbox': [0.635546875, 0.5888888888888889, 0.01171875, 0.013888888888888888]}, {'frame': 307, 'bbox': [0.6359375, 0.5888888888888889, 0.0125, 0.013888888888888888]}, {'frame': 308, 'bbox': [0.6359375, 0.5902777777777778, 0.0125, 0.013888888888888888]}, {'frame': 309, 'bbox': [0.636328125, 0.5888888888888889, 0.01171875, 0.013888888888888888]}, {'frame': 310, 'bbox': [0.6375, 0.5888888888888889, 0.0125, 0.013888888888888888]}, {'frame': 311, 'bbox': [0.636328125, 0.5902777777777778, 0.01171875, 0.013888888888888888]}, {'frame': 312, 'bbox': [0.637109375, 0.5888888888888889, 0.01171875, 0.013888888888888888]}, {'frame': 313, 'bbox': [0.6375, 0.5902777777777778, 0.0125, 0.013888888888888888]}, {'frame': 314, 'bbox': [0.6375, 0.5888888888888889, 0.0125, 0.013888888888888888]}, {'frame': 315, 'bbox': [0.637109375, 0.5888888888888889, 0.01171875, 0.013888888888888888]}, {'frame': 316, 'bbox': [0.637890625, 0.5902777777777778, 0.01171875, 0.013888888888888888]}, {'frame': 317, 'bbox': [0.637890625, 0.5888888888888889, 0.01171875, 0.013888888888888888]}, {'frame': 318, 'bbox': [0.6390625, 0.5902777777777778, 0.0125, 0.013888888888888888]}, {'frame': 319, 'bbox': [0.638671875, 0.5888888888888889, 0.01171875, 0.013888888888888888]}, {'frame': 320, 'bbox': [0.6390625, 0.5902777777777778, 0.0125, 0.013888888888888888]}, {'frame': 321, 'bbox': [0.63828125, 0.5888888888888889, 0.0109375, 0.013888888888888888]}, {'frame': 322, 'bbox': [0.639453125, 0.5902777777777778, 0.01171875, 0.013888888888888888]}, {'frame': 323, 'bbox': [0.63984375, 0.5888888888888889, 0.0125, 0.013888888888888888]}, {'frame': 324, 'bbox': [0.63984375, 0.5902777777777778, 0.0125, 0.013888888888888888]}, {'frame': 325, 'bbox': [0.640234375, 0.5902777777777778, 0.01171875, 0.013888888888888888]}, {'frame': 326, 'bbox': [0.640234375, 0.5902777777777778, 0.01171875, 0.013888888888888888]}, {'frame': 327, 'bbox': [0.640234375, 0.5902777777777778, 0.01171875, 0.013888888888888888]}, {'frame': 328, 'bbox': [0.640234375, 0.5902777777777778, 0.01171875, 0.013888888888888888]}, {'frame': 329, 'bbox': [0.641015625, 0.5902777777777778, 0.01171875, 0.013888888888888888]}, {'frame': 330, 'bbox': [0.641015625, 0.5902777777777778, 0.01171875, 0.013888888888888888]}, {'frame': 331, 'bbox': [0.6421875, 0.5916666666666667, 0.0125, 0.013888888888888888]}, {'frame': 332, 'bbox': [0.64140625, 0.5902777777777778, 0.0125, 0.013888888888888888]}, {'frame': 333, 'bbox': [0.6421875, 0.5916666666666667, 0.0125, 0.013888888888888888]}, {'frame': 334, 'bbox': [0.643359375, 0.5916666666666667, 0.01484375, 0.013888888888888888]}, {'frame': 335, 'bbox': [0.644921875, 0.5951388888888889, 0.02578125, 0.02361111111111111]}, {'frame': 336, 'bbox': [0.645703125, 0.6013888888888889, 0.03671875, 0.03611111111111111]}, {'frame': 337, 'bbox': [0.645703125, 0.6027777777777777, 0.03828125, 0.03611111111111111]}, {'frame': 338, 'bbox': [0.645703125, 0.6048611111111111, 0.03828125, 0.03194444444444444]}, {'frame': 339, 'bbox': [0.6453125, 0.6027777777777777, 0.0375, 0.03611111111111111]}, {'frame': 340, 'bbox': [0.645703125, 0.60625, 0.03828125, 0.03194444444444444]}, {'frame': 341, 'bbox': [0.645703125, 0.6034722222222222, 0.03828125, 0.0375]}, {'frame': 342, 'bbox': [0.646484375, 0.6020833333333333, 0.03671875, 0.0375]}, {'frame': 343, 'bbox': [0.646484375, 0.6069444444444444, 0.03671875, 0.030555555555555555]}, {'frame': 344, 'bbox': [0.64453125, 0.6027777777777777, 0.040625, 0.03611111111111111]}, {'frame': 345, 'bbox': [0.6453125, 0.6027777777777777, 0.0375, 0.03611111111111111]}, {'frame': 346, 'bbox': [0.6453125, 0.6041666666666666, 0.0375, 0.03611111111111111]}, {'frame': 347, 'bbox': [0.64609375, 0.6027777777777777, 0.0375, 0.03611111111111111]}, {'frame': 348, 'bbox': [0.646484375, 0.6041666666666666, 0.03671875, 0.03611111111111111]}, {'frame': 349, 'bbox': [0.646484375, 0.6027777777777777, 0.03671875, 0.03611111111111111]}, {'frame': 350, 'bbox': [0.645703125, 0.6027777777777777, 0.03671875, 0.03611111111111111]}, {'frame': 351, 'bbox': [0.646484375, 0.6027777777777777, 0.03671875, 0.03611111111111111]}, {'frame': 352, 'bbox': [0.645703125, 0.6048611111111111, 0.03671875, 0.034722222222222224]}, {'frame': 353, 'bbox': [0.645703125, 0.6034722222222222, 0.03828125, 0.0375]}, {'frame': 354, 'bbox': [0.646484375, 0.6048611111111111, 0.03671875, 0.0375]}, {'frame': 355, 'bbox': [0.645703125, 0.6041666666666666, 0.03671875, 0.03611111111111111]}, {'frame': 356, 'bbox': [0.644921875, 0.6041666666666666, 0.03828125, 0.03611111111111111]}, {'frame': 357, 'bbox': [0.64609375, 0.6069444444444444, 0.0375, 0.03333333333333333]}, {'frame': 358, 'bbox': [0.644921875, 0.6027777777777777, 0.03828125, 0.03611111111111111]}, {'frame': 359, 'bbox': [0.644921875, 0.6048611111111111, 0.03671875, 0.034722222222222224]}, {'frame': 360, 'bbox': [0.644921875, 0.6048611111111111, 0.03828125, 0.0375]}, {'frame': 361, 'bbox': [0.644921875, 0.6055555555555555, 0.03671875, 0.03333333333333333]}, {'frame': 362, 'bbox': [0.64453125, 0.6027777777777777, 0.0390625, 0.03611111111111111]}, {'frame': 363, 'bbox': [0.645703125, 0.6083333333333333, 0.03671875, 0.027777777777777776]}, {'frame': 364, 'bbox': [0.64453125, 0.6048611111111111, 0.0390625, 0.034722222222222224]}, {'frame': 365, 'bbox': [0.645703125, 0.6097222222222223, 0.03671875, 0.027777777777777776]}, {'frame': 366, 'bbox': [0.64375, 0.60625, 0.0390625, 0.034722222222222224]}, {'frame': 367, 'bbox': [0.645703125, 0.6048611111111111, 0.03828125, 0.034722222222222224]}, {'frame': 368, 'bbox': [0.644921875, 0.6055555555555555, 0.03828125, 0.03333333333333333]}, {'frame': 369, 'bbox': [0.6453125, 0.6048611111111111, 0.0390625, 0.034722222222222224]}, {'frame': 370, 'bbox': [0.644921875, 0.6055555555555555, 0.03828125, 0.03333333333333333]}, {'frame': 371, 'bbox': [0.6453125, 0.60625, 0.0375, 0.034722222222222224]}, {'frame': 372, 'bbox': [0.64453125, 0.6055555555555555, 0.0390625, 0.03333333333333333]}, {'frame': 373, 'bbox': [0.64453125, 0.6055555555555555, 0.0375, 0.03333333333333333]}, {'frame': 374, 'bbox': [0.644921875, 0.6048611111111111, 0.03828125, 0.034722222222222224]}, {'frame': 375, 'bbox': [0.644921875, 0.6055555555555555, 0.03828125, 0.03333333333333333]}, {'frame': 376, 'bbox': [0.644921875, 0.60625, 0.03828125, 0.034722222222222224]}, {'frame': 377, 'bbox': [0.644921875, 0.60625, 0.03828125, 0.034722222222222224]}, {'frame': 378, 'bbox': [0.64453125, 0.6069444444444444, 0.0375, 0.03333333333333333]}, {'frame': 379, 'bbox': [0.644921875, 0.60625, 0.03828125, 0.034722222222222224]}, {'frame': 380, 'bbox': [0.64453125, 0.60625, 0.0390625, 0.034722222222222224]}, {'frame': 381, 'bbox': [0.64453125, 0.6076388888888888, 0.0375, 0.03194444444444444]}, {'frame': 382, 'bbox': [0.644921875, 0.60625, 0.03828125, 0.034722222222222224]}, {'frame': 383, 'bbox': [0.644921875, 0.60625, 0.03828125, 0.03194444444444444]}, {'frame': 384, 'bbox': [0.644921875, 0.6048611111111111, 0.03828125, 0.034722222222222224]}, {'frame': 385, 'bbox': [0.644921875, 0.6055555555555555, 0.03828125, 0.03333333333333333]}, {'frame': 386, 'bbox': [0.644921875, 0.60625, 0.03671875, 0.034722222222222224]}, {'frame': 387, 'bbox': [0.6453125, 0.60625, 0.0375, 0.034722222222222224]}, {'frame': 388, 'bbox': [0.64453125, 0.6069444444444444, 0.0375, 0.03333333333333333]}, {'frame': 389, 'bbox': [0.6453125, 0.6055555555555555, 0.0375, 0.03333333333333333]}, {'frame': 390, 'bbox': [0.6453125, 0.6069444444444444, 0.0375, 0.03333333333333333]}, {'frame': 391, 'bbox': [0.64453125, 0.60625, 0.0390625, 0.034722222222222224]}, {'frame': 392, 'bbox': [0.64375, 0.6048611111111111, 0.0390625, 0.034722222222222224]}, {'frame': 393, 'bbox': [0.64453125, 0.6083333333333333, 0.0375, 0.030555555555555555]}, {'frame': 394, 'bbox': [0.64375, 0.60625, 0.040625, 0.034722222222222224]}, {'frame': 395, 'bbox': [0.645703125, 0.6076388888888888, 0.03671875, 0.03194444444444444]}, {'frame': 396, 'bbox': [0.644140625, 0.6055555555555555, 0.03828125, 0.03611111111111111]}, {'frame': 397, 'bbox': [0.644921875, 0.6055555555555555, 0.03828125, 0.03611111111111111]}, {'frame': 398, 'bbox': [0.64453125, 0.6048611111111111, 0.0375, 0.034722222222222224]}, {'frame': 399, 'bbox': [0.645703125, 0.6069444444444444, 0.03671875, 0.03611111111111111]}, {'frame': 400, 'bbox': [0.64609375, 0.6055555555555555, 0.0359375, 0.03611111111111111]}, {'frame': 401, 'bbox': [0.644921875, 0.6055555555555555, 0.03671875, 0.03611111111111111]}, {'frame': 402, 'bbox': [0.645703125, 0.60625, 0.03671875, 0.0375]}, {'frame': 403, 'bbox': [0.64609375, 0.6055555555555555, 0.0359375, 0.03611111111111111]}, {'frame': 404, 'bbox': [0.6453125, 0.6055555555555555, 0.0375, 0.03611111111111111]}, {'frame': 405, 'bbox': [0.645703125, 0.6055555555555555, 0.03671875, 0.03611111111111111]}, {'frame': 406, 'bbox': [0.644921875, 0.6055555555555555, 0.03671875, 0.03611111111111111]}, {'frame': 407, 'bbox': [0.645703125, 0.60625, 0.03671875, 0.0375]}, {'frame': 408, 'bbox': [0.644921875, 0.6055555555555555, 0.03828125, 0.03611111111111111]}, {'frame': 409, 'bbox': [0.6453125, 0.60625, 0.0375, 0.0375]}, {'frame': 410, 'bbox': [0.644921875, 0.6055555555555555, 0.03671875, 0.03611111111111111]}, {'frame': 411, 'bbox': [0.644921875, 0.6055555555555555, 0.03671875, 0.03611111111111111]}, {'frame': 412, 'bbox': [0.6453125, 0.6055555555555555, 0.0375, 0.03611111111111111]}, {'frame': 413, 'bbox': [0.6453125, 0.60625, 0.0375, 0.034722222222222224]}, {'frame': 414, 'bbox': [0.64296875, 0.6020833333333333, 0.03125, 0.034722222222222224]}, {'frame': 415, 'bbox': [0.644921875, 0.6055555555555555, 0.03671875, 0.03611111111111111]}, {'frame': 416, 'bbox': [0.64453125, 0.6069444444444444, 0.0375, 0.03611111111111111]}, {'frame': 417, 'bbox': [0.644140625, 0.60625, 0.03828125, 0.0375]}, {'frame': 418, 'bbox': [0.64375, 0.60625, 0.0390625, 0.0375]}, {'frame': 419, 'bbox': [0.644921875, 0.6055555555555555, 0.03828125, 0.03611111111111111]}, {'frame': 420, 'bbox': [0.644140625, 0.6076388888888888, 0.03828125, 0.0375]}, {'frame': 421, 'bbox': [0.643359375, 0.6048611111111111, 0.03984375, 0.04027777777777778]}, {'frame': 422, 'bbox': [0.64375, 0.6069444444444444, 0.040625, 0.03611111111111111]}, {'frame': 423, 'bbox': [0.64296875, 0.6069444444444444, 0.040625, 0.03888888888888889]}, {'frame': 424, 'bbox': [0.641015625, 0.6041666666666666, 0.03671875, 0.03611111111111111]}, {'frame': 425, 'bbox': [0.6421875, 0.60625, 0.040625, 0.0375]}, {'frame': 426, 'bbox': [0.642578125, 0.6069444444444444, 0.04140625, 0.03611111111111111]}, {'frame': 427, 'bbox': [0.63828125, 0.6041666666666666, 0.0375, 0.03611111111111111]}, {'frame': 428, 'bbox': [0.632421875, 0.6020833333333333, 0.02265625, 0.02638888888888889]}, {'frame': 429, 'bbox': [0.629296875, 0.6, 0.01796875, 0.022222222222222223]}, {'frame': 430, 'bbox': [0.628125, 0.5972222222222222, 0.0171875, 0.019444444444444445]}, {'frame': 431, 'bbox': [0.626953125, 0.5965277777777778, 0.01953125, 0.018055555555555554]}, {'frame': 432, 'bbox': [0.626171875, 0.5944444444444444, 0.01953125, 0.016666666666666666]}, {'frame': 433, 'bbox': [0.62578125, 0.5958333333333333, 0.0203125, 0.016666666666666666]}, {'frame': 434, 'bbox': [0.625390625, 0.5944444444444444, 0.02109375, 0.016666666666666666]}, {'frame': 435, 'bbox': [0.625, 0.5951388888888889, 0.0203125, 0.015277777777777777]}, {'frame': 436, 'bbox': [0.62421875, 0.5965277777777778, 0.0203125, 0.015277777777777777]}, {'frame': 437, 'bbox': [0.62265625, 0.5923611111111111, 0.0203125, 0.020833333333333332]}, {'frame': 438, 'bbox': [0.622265625, 0.5958333333333333, 0.01953125, 0.013888888888888888]}, {'frame': 439, 'bbox': [0.620703125, 0.5958333333333333, 0.01953125, 0.013888888888888888]}, {'frame': 440, 'bbox': [0.61953125, 0.5951388888888889, 0.0203125, 0.015277777777777777]}, {'frame': 441, 'bbox': [0.618359375, 0.5951388888888889, 0.01953125, 0.015277777777777777]}, {'frame': 442, 'bbox': [0.617578125, 0.5958333333333333, 0.01953125, 0.013888888888888888]}, {'frame': 443, 'bbox': [0.6171875, 0.5965277777777778, 0.0203125, 0.015277777777777777]}, {'frame': 444, 'bbox': [0.61640625, 0.5965277777777778, 0.0203125, 0.015277777777777777]}, {'frame': 445, 'bbox': [0.615625, 0.5958333333333333, 0.0203125, 0.013888888888888888]}, {'frame': 446, 'bbox': [0.61484375, 0.5958333333333333, 0.0203125, 0.013888888888888888]}, {'frame': 447, 'bbox': [0.611328125, 0.5958333333333333, 0.01640625, 0.013888888888888888]}, {'frame': 448, 'bbox': [0.6109375, 0.5958333333333333, 0.0171875, 0.013888888888888888]}, {'frame': 449, 'bbox': [0.609765625, 0.5972222222222222, 0.01796875, 0.013888888888888888]}, {'frame': 450, 'bbox': [0.609375, 0.5958333333333333, 0.01875, 0.013888888888888888]}, {'frame': 451, 'bbox': [0.60859375, 0.5958333333333333, 0.01875, 0.013888888888888888]}, {'frame': 452, 'bbox': [0.607421875, 0.5965277777777778, 0.01953125, 0.015277777777777777]}, {'frame': 453, 'bbox': [0.60625, 0.5958333333333333, 0.0203125, 0.013888888888888888]}, {'frame': 454, 'bbox': [0.60546875, 0.5958333333333333, 0.0203125, 0.013888888888888888]}, {'frame': 455, 'bbox': [0.60390625, 0.5958333333333333, 0.021875, 0.013888888888888888]}, {'frame': 456, 'bbox': [0.602734375, 0.5958333333333333, 0.02109375, 0.013888888888888888]}, {'frame': 457, 'bbox': [0.601171875, 0.5958333333333333, 0.02109375, 0.013888888888888888]}, {'frame': 458, 'bbox': [0.6015625, 0.5958333333333333, 0.01875, 0.013888888888888888]}, {'frame': 459, 'bbox': [0.60078125, 0.5958333333333333, 0.0171875, 0.013888888888888888]}, {'frame': 460, 'bbox': [0.600390625, 0.5958333333333333, 0.01640625, 0.013888888888888888]}, {'frame': 461, 'bbox': [0.59921875, 0.5958333333333333, 0.0171875, 0.013888888888888888]}, {'frame': 462, 'bbox': [0.598828125, 0.5958333333333333, 0.01640625, 0.013888888888888888]}, {'frame': 463, 'bbox': [0.597265625, 0.5958333333333333, 0.01484375, 0.013888888888888888]}, {'frame': 464, 'bbox': [0.597265625, 0.5958333333333333, 0.01484375, 0.013888888888888888]}, {'frame': 465, 'bbox': [0.59453125, 0.5958333333333333, 0.015625, 0.013888888888888888]}, {'frame': 466, 'bbox': [0.594140625, 0.5958333333333333, 0.01484375, 0.013888888888888888]}, {'frame': 467, 'bbox': [0.58984375, 0.5958333333333333, 0.0203125, 0.013888888888888888]}, {'frame': 468, 'bbox': [0.589453125, 0.5944444444444444, 0.01953125, 0.013888888888888888]}, {'frame': 469, 'bbox': [0.587890625, 0.5958333333333333, 0.01953125, 0.013888888888888888]}, {'frame': 470, 'bbox': [0.586328125, 0.5958333333333333, 0.01953125, 0.013888888888888888]}, {'frame': 471, 'bbox': [0.586328125, 0.5944444444444444, 0.01796875, 0.013888888888888888]}, {'frame': 472, 'bbox': [0.585546875, 0.5958333333333333, 0.01640625, 0.013888888888888888]}, {'frame': 473, 'bbox': [0.584765625, 0.5958333333333333, 0.01796875, 0.013888888888888888]}, {'frame': 474, 'bbox': [0.583203125, 0.5958333333333333, 0.01171875, 0.013888888888888888]}, {'frame': 475, 'bbox': [0.577734375, 0.5972222222222222, 0.01640625, 0.013888888888888888]}, {'frame': 476, 'bbox': [0.578515625, 0.5958333333333333, 0.01640625, 0.013888888888888888]}, {'frame': 477, 'bbox': [0.5796875, 0.5958333333333333, 0.0171875, 0.013888888888888888]}, {'frame': 478, 'bbox': [0.576953125, 0.5958333333333333, 0.01484375, 0.013888888888888888]}, {'frame': 479, 'bbox': [0.574609375, 0.5958333333333333, 0.01796875, 0.013888888888888888]}, {'frame': 480, 'bbox': [0.571484375, 0.5958333333333333, 0.01953125, 0.013888888888888888]}, {'frame': 481, 'bbox': [0.56875, 0.5958333333333333, 0.0171875, 0.013888888888888888]}, {'frame': 482, 'bbox': [0.57109375, 0.5972222222222222, 0.0125, 0.013888888888888888]}, {'frame': 483, 'bbox': [0.570703125, 0.5972222222222222, 0.01484375, 0.013888888888888888]}, {'frame': 484, 'bbox': [0.56640625, 0.5972222222222222, 0.01875, 0.013888888888888888]}, {'frame': 485, 'bbox': [0.564453125, 0.5958333333333333, 0.01953125, 0.013888888888888888]}, {'frame': 486, 'bbox': [0.5640625, 0.5958333333333333, 0.0203125, 0.013888888888888888]}, {'frame': 487, 'bbox': [0.5625, 0.5958333333333333, 0.0203125, 0.013888888888888888]}, {'frame': 488, 'bbox': [0.561328125, 0.5958333333333333, 0.01953125, 0.013888888888888888]}, {'frame': 489, 'bbox': [0.558984375, 0.5958333333333333, 0.02109375, 0.013888888888888888]}, {'frame': 490, 'bbox': [0.557421875, 0.5958333333333333, 0.02109375, 0.013888888888888888]}, {'frame': 491, 'bbox': [0.555859375, 0.5958333333333333, 0.02109375, 0.013888888888888888]}, {'frame': 492, 'bbox': [0.554296875, 0.5958333333333333, 0.02109375, 0.013888888888888888]}, {'frame': 493, 'bbox': [0.552734375, 0.5958333333333333, 0.02109375, 0.013888888888888888]}, {'frame': 494, 'bbox': [0.5515625, 0.5958333333333333, 0.0203125, 0.013888888888888888]}, {'frame': 495, 'bbox': [0.551171875, 0.5958333333333333, 0.01953125, 0.013888888888888888]}, {'frame': 496, 'bbox': [0.5484375, 0.5958333333333333, 0.0203125, 0.013888888888888888]}, {'frame': 497, 'bbox': [0.546875, 0.5958333333333333, 0.0203125, 0.013888888888888888]}, {'frame': 498, 'bbox': [0.544921875, 0.5958333333333333, 0.02109375, 0.013888888888888888]}, {'frame': 499, 'bbox': [0.54453125, 0.5958333333333333, 0.0203125, 0.013888888888888888]}, {'frame': 500, 'bbox': [0.542578125, 0.5958333333333333, 0.01953125, 0.013888888888888888]}, {'frame': 501, 'bbox': [0.540625, 0.5958333333333333, 0.0203125, 0.013888888888888888]}, {'frame': 502, 'bbox': [0.5390625, 0.5958333333333333, 0.0203125, 0.013888888888888888]}, {'frame': 503, 'bbox': [0.537109375, 0.5944444444444444, 0.02109375, 0.013888888888888888]}, {'frame': 504, 'bbox': [0.5359375, 0.5944444444444444, 0.0203125, 0.013888888888888888]}, {'frame': 505, 'bbox': [0.533984375, 0.5958333333333333, 0.02109375, 0.013888888888888888]}, {'frame': 506, 'bbox': [0.5328125, 0.5944444444444444, 0.0203125, 0.013888888888888888]}, {'frame': 507, 'bbox': [0.53046875, 0.5958333333333333, 0.0203125, 0.013888888888888888]}, {'frame': 508, 'bbox': [0.52890625, 0.5958333333333333, 0.0203125, 0.013888888888888888]}, {'frame': 509, 'bbox': [0.528125, 0.5958333333333333, 0.0203125, 0.013888888888888888]}, {'frame': 510, 'bbox': [0.525390625, 0.5958333333333333, 0.02109375, 0.013888888888888888]}, {'frame': 511, 'bbox': [0.523828125, 0.5958333333333333, 0.01953125, 0.013888888888888888]}, {'frame': 512, 'bbox': [0.522265625, 0.5944444444444444, 0.01953125, 0.013888888888888888]}, {'frame': 513, 'bbox': [0.5203125, 0.5958333333333333, 0.0203125, 0.013888888888888888]}, {'frame': 514, 'bbox': [0.519921875, 0.5958333333333333, 0.01796875, 0.013888888888888888]}, {'frame': 515, 'bbox': [0.519140625, 0.5958333333333333, 0.01640625, 0.013888888888888888]}, {'frame': 516, 'bbox': [0.518359375, 0.5958333333333333, 0.01484375, 0.013888888888888888]}, {'frame': 517, 'bbox': [0.513671875, 0.5944444444444444, 0.01953125, 0.013888888888888888]}, {'frame': 518, 'bbox': [0.517578125, 0.5944444444444444, 0.01171875, 0.013888888888888888]}, {'frame': 519, 'bbox': [0.515234375, 0.5944444444444444, 0.01015625, 0.013888888888888888]}, {'frame': 520, 'bbox': [0.51328125, 0.5958333333333333, 0.0109375, 0.013888888888888888]}, {'frame': 521, 'bbox': [0.50625, 0.5958333333333333, 0.0203125, 0.013888888888888888]}, {'frame': 522, 'bbox': [0.503125, 0.5972222222222222, 0.0171875, 0.013888888888888888]}, {'frame': 523, 'bbox': [0.500390625, 0.5958333333333333, 0.01484375, 0.013888888888888888]}, {'frame': 524, 'bbox': [0.499609375, 0.5972222222222222, 0.01640625, 0.013888888888888888]}, {'frame': 525, 'bbox': [0.4984375, 0.5958333333333333, 0.01875, 0.013888888888888888]}, {'frame': 526, 'bbox': [0.496484375, 0.5958333333333333, 0.01796875, 0.013888888888888888]}, {'frame': 527, 'bbox': [0.4953125, 0.5958333333333333, 0.0203125, 0.013888888888888888]}, {'frame': 528, 'bbox': [0.494140625, 0.5958333333333333, 0.01796875, 0.013888888888888888]}, {'frame': 529, 'bbox': [0.491796875, 0.5972222222222222, 0.01953125, 0.013888888888888888]}, {'frame': 530, 'bbox': [0.490234375, 0.5958333333333333, 0.01953125, 0.013888888888888888]}, {'frame': 531, 'bbox': [0.4875, 0.5958333333333333, 0.0203125, 0.013888888888888888]}, {'frame': 532, 'bbox': [0.485546875, 0.5958333333333333, 0.01953125, 0.013888888888888888]}, {'frame': 533, 'bbox': [0.4828125, 0.5958333333333333, 0.0234375, 0.013888888888888888]}, {'frame': 534, 'bbox': [0.48046875, 0.5951388888888889, 0.0234375, 0.015277777777777777]}, {'frame': 535, 'bbox': [0.480078125, 0.5958333333333333, 0.02109375, 0.013888888888888888]}, {'frame': 536, 'bbox': [0.476171875, 0.5958333333333333, 0.01640625, 0.013888888888888888]}, {'frame': 537, 'bbox': [0.47578125, 0.5958333333333333, 0.01875, 0.013888888888888888]}, {'frame': 538, 'bbox': [0.473828125, 0.5958333333333333, 0.01953125, 0.013888888888888888]}, {'frame': 539, 'bbox': [0.47265625, 0.5958333333333333, 0.0203125, 0.013888888888888888]}, {'frame': 540, 'bbox': [0.4703125, 0.5958333333333333, 0.0203125, 0.013888888888888888]}, {'frame': 541, 'bbox': [0.468359375, 0.5958333333333333, 0.01953125, 0.013888888888888888]}, {'frame': 542, 'bbox': [0.46640625, 0.5958333333333333, 0.0203125, 0.013888888888888888]}, {'frame': 543, 'bbox': [0.464453125, 0.5958333333333333, 0.01953125, 0.013888888888888888]}, {'frame': 544, 'bbox': [0.462109375, 0.5958333333333333, 0.01953125, 0.013888888888888888]}, {'frame': 545, 'bbox': [0.460546875, 0.5958333333333333, 0.01953125, 0.013888888888888888]}, {'frame': 546, 'bbox': [0.45859375, 0.5958333333333333, 0.0203125, 0.013888888888888888]}, {'frame': 547, 'bbox': [0.45625, 0.5958333333333333, 0.0203125, 0.013888888888888888]}, {'frame': 548, 'bbox': [0.454296875, 0.5958333333333333, 0.02109375, 0.013888888888888888]}, {'frame': 549, 'bbox': [0.452734375, 0.5944444444444444, 0.01953125, 0.013888888888888888]}, {'frame': 550, 'bbox': [0.45, 0.5958333333333333, 0.0203125, 0.013888888888888888]}, {'frame': 551, 'bbox': [0.448046875, 0.5944444444444444, 0.02109375, 0.013888888888888888]}, {'frame': 552, 'bbox': [0.446484375, 0.5972222222222222, 0.02109375, 0.013888888888888888]}, {'frame': 553, 'bbox': [0.44375, 0.5958333333333333, 0.0203125, 0.013888888888888888]}, {'frame': 554, 'bbox': [0.441015625, 0.5958333333333333, 0.02109375, 0.013888888888888888]}, {'frame': 555, 'bbox': [0.439453125, 0.5958333333333333, 0.02109375, 0.013888888888888888]}, {'frame': 556, 'bbox': [0.437109375, 0.5958333333333333, 0.02109375, 0.013888888888888888]}, {'frame': 557, 'bbox': [0.43515625, 0.5958333333333333, 0.0203125, 0.013888888888888888]}, {'frame': 558, 'bbox': [0.4328125, 0.5958333333333333, 0.0203125, 0.013888888888888888]}, {'frame': 559, 'bbox': [0.430859375, 0.5958333333333333, 0.02109375, 0.013888888888888888]}, {'frame': 560, 'bbox': [0.429296875, 0.5958333333333333, 0.02109375, 0.013888888888888888]}, {'frame': 561, 'bbox': [0.426171875, 0.5958333333333333, 0.02109375, 0.013888888888888888]}, {'frame': 562, 'bbox': [0.424609375, 0.5958333333333333, 0.02109375, 0.013888888888888888]}, {'frame': 563, 'bbox': [0.422265625, 0.5958333333333333, 0.02109375, 0.013888888888888888]}, {'frame': 564, 'bbox': [0.419921875, 0.5958333333333333, 0.02109375, 0.013888888888888888]}, {'frame': 565, 'bbox': [0.41796875, 0.5958333333333333, 0.0203125, 0.013888888888888888]}, {'frame': 566, 'bbox': [0.416015625, 0.5958333333333333, 0.02109375, 0.013888888888888888]}, {'frame': 567, 'bbox': [0.413671875, 0.5958333333333333, 0.02109375, 0.013888888888888888]}, {'frame': 568, 'bbox': [0.412109375, 0.5958333333333333, 0.02109375, 0.013888888888888888]}, {'frame': 569, 'bbox': [0.40859375, 0.5958333333333333, 0.0203125, 0.013888888888888888]}, {'frame': 570, 'bbox': [0.406640625, 0.5958333333333333, 0.02109375, 0.013888888888888888]}, {'frame': 571, 'bbox': [0.405078125, 0.5958333333333333, 0.02109375, 0.013888888888888888]}, {'frame': 572, 'bbox': [0.402734375, 0.5958333333333333, 0.02109375, 0.013888888888888888]}, {'frame': 573, 'bbox': [0.40078125, 0.5958333333333333, 0.0203125, 0.013888888888888888]}, {'frame': 574, 'bbox': [0.39765625, 0.5958333333333333, 0.0203125, 0.013888888888888888]}, {'frame': 575, 'bbox': [0.396484375, 0.5958333333333333, 0.02109375, 0.013888888888888888]}, {'frame': 576, 'bbox': [0.39375, 0.5958333333333333, 0.0203125, 0.013888888888888888]}, {'frame': 577, 'bbox': [0.39140625, 0.5958333333333333, 0.0203125, 0.013888888888888888]}, {'frame': 578, 'bbox': [0.3890625, 0.5958333333333333, 0.0203125, 0.013888888888888888]}, {'frame': 579, 'bbox': [0.3875, 0.5958333333333333, 0.0203125, 0.013888888888888888]}, {'frame': 580, 'bbox': [0.38515625, 0.5958333333333333, 0.0203125, 0.013888888888888888]}, {'frame': 581, 'bbox': [0.3828125, 0.5958333333333333, 0.0203125, 0.013888888888888888]}, {'frame': 582, 'bbox': [0.3796875, 0.5958333333333333, 0.0203125, 0.013888888888888888]}, {'frame': 583, 'bbox': [0.37734375, 0.5958333333333333, 0.0203125, 0.013888888888888888]}, {'frame': 584, 'bbox': [0.37578125, 0.5972222222222222, 0.0203125, 0.013888888888888888]}, {'frame': 585, 'bbox': [0.373046875, 0.5958333333333333, 0.01953125, 0.013888888888888888]}, {'frame': 586, 'bbox': [0.371484375, 0.5972222222222222, 0.01953125, 0.013888888888888888]}, {'frame': 587, 'bbox': [0.369140625, 0.5958333333333333, 0.01953125, 0.013888888888888888]}, {'frame': 588, 'bbox': [0.36796875, 0.5972222222222222, 0.0171875, 0.013888888888888888]}, {'frame': 589, 'bbox': [0.3640625, 0.5958333333333333, 0.01875, 0.013888888888888888]}, {'frame': 590, 'bbox': [0.36328125, 0.5958333333333333, 0.0171875, 0.013888888888888888]}, {'frame': 591, 'bbox': [0.361328125, 0.5958333333333333, 0.01640625, 0.013888888888888888]}, {'frame': 592, 'bbox': [0.357421875, 0.5972222222222222, 0.01796875, 0.013888888888888888]}, {'frame': 593, 'bbox': [0.355078125, 0.5972222222222222, 0.01953125, 0.013888888888888888]}, {'frame': 594, 'bbox': [0.35390625, 0.5958333333333333, 0.0171875, 0.013888888888888888]}, {'frame': 595, 'bbox': [0.34765625, 0.5881944444444445, 0.025, 0.02361111111111111]}, {'frame': 596, 'bbox': [0.34765625, 0.5881944444444445, 0.028125, 0.02638888888888889]}, {'frame': 597, 'bbox': [0.3484375, 0.5888888888888889, 0.028125, 0.025]}, {'frame': 598, 'bbox': [0.346875, 0.5868055555555556, 0.0296875, 0.02638888888888889]}, {'frame': 599, 'bbox': [0.34609375, 0.5888888888888889, 0.03125, 0.027777777777777776]}, {'frame': 600, 'bbox': [0.3375, 0.5958333333333333, 0.0203125, 0.013888888888888888]}, {'frame': 601, 'bbox': [0.3359375, 0.5972222222222222, 0.0203125, 0.013888888888888888]}, {'frame': 602, 'bbox': [0.33359375, 0.5972222222222222, 0.0203125, 0.013888888888888888]}, {'frame': 603, 'bbox': [0.33046875, 0.5958333333333333, 0.01875, 0.013888888888888888]}, {'frame': 604, 'bbox': [0.328125, 0.5972222222222222, 0.01875, 0.013888888888888888]}, {'frame': 605, 'bbox': [0.3265625, 0.5972222222222222, 0.0203125, 0.013888888888888888]}, {'frame': 606, 'bbox': [0.3234375, 0.5958333333333333, 0.0203125, 0.013888888888888888]}, {'frame': 607, 'bbox': [0.321875, 0.5972222222222222, 0.0203125, 0.013888888888888888]}, {'frame': 608, 'bbox': [0.31875, 0.5958333333333333, 0.0203125, 0.013888888888888888]}, {'frame': 609, 'bbox': [0.31640625, 0.5972222222222222, 0.0203125, 0.013888888888888888]}, {'frame': 610, 'bbox': [0.3140625, 0.5958333333333333, 0.0203125, 0.013888888888888888]}, {'frame': 611, 'bbox': [0.3109375, 0.5958333333333333, 0.0203125, 0.013888888888888888]}, {'frame': 612, 'bbox': [0.30859375, 0.5972222222222222, 0.0203125, 0.013888888888888888]}, {'frame': 613, 'bbox': [0.30703125, 0.5972222222222222, 0.0203125, 0.013888888888888888]}, {'frame': 614, 'bbox': [0.30390625, 0.5972222222222222, 0.0203125, 0.013888888888888888]}, {'frame': 615, 'bbox': [0.301953125, 0.5972222222222222, 0.02109375, 0.013888888888888888]}, {'frame': 616, 'bbox': [0.29921875, 0.5972222222222222, 0.0203125, 0.013888888888888888]}, {'frame': 617, 'bbox': [0.296484375, 0.5972222222222222, 0.02109375, 0.013888888888888888]}, {'frame': 618, 'bbox': [0.29375, 0.5972222222222222, 0.0203125, 0.013888888888888888]}, {'frame': 619, 'bbox': [0.29140625, 0.5972222222222222, 0.0203125, 0.013888888888888888]}, {'frame': 620, 'bbox': [0.2890625, 0.5972222222222222, 0.0203125, 0.013888888888888888]}, {'frame': 621, 'bbox': [0.28671875, 0.5972222222222222, 0.0203125, 0.013888888888888888]}, {'frame': 622, 'bbox': [0.284375, 0.5972222222222222, 0.0203125, 0.013888888888888888]}, {'frame': 623, 'bbox': [0.28125, 0.5972222222222222, 0.0203125, 0.013888888888888888]}, {'frame': 624, 'bbox': [0.278515625, 0.5972222222222222, 0.02109375, 0.013888888888888888]}, {'frame': 625, 'bbox': [0.2765625, 0.5972222222222222, 0.0203125, 0.013888888888888888]}, {'frame': 626, 'bbox': [0.27421875, 0.5986111111111111, 0.0203125, 0.013888888888888888]}, {'frame': 627, 'bbox': [0.271484375, 0.5972222222222222, 0.01953125, 0.013888888888888888]}, {'frame': 628, 'bbox': [0.26875, 0.5986111111111111, 0.0203125, 0.013888888888888888]}, {'frame': 629, 'bbox': [0.266796875, 0.5972222222222222, 0.01953125, 0.013888888888888888]}, {'frame': 630, 'bbox': [0.2640625, 0.5986111111111111, 0.0203125, 0.013888888888888888]}, {'frame': 631, 'bbox': [0.26171875, 0.5972222222222222, 0.0203125, 0.013888888888888888]}, {'frame': 632, 'bbox': [0.258984375, 0.5986111111111111, 0.01953125, 0.013888888888888888]}, {'frame': 633, 'bbox': [0.25625, 0.5972222222222222, 0.0203125, 0.013888888888888888]}, {'frame': 634, 'bbox': [0.25390625, 0.5972222222222222, 0.0203125, 0.013888888888888888]}, {'frame': 635, 'bbox': [0.25078125, 0.5972222222222222, 0.0203125, 0.013888888888888888]}, {'frame': 636, 'bbox': [0.24921875, 0.5972222222222222, 0.0203125, 0.013888888888888888]}, {'frame': 637, 'bbox': [0.24609375, 0.5986111111111111, 0.0203125, 0.013888888888888888]}, {'frame': 638, 'bbox': [0.24296875, 0.5986111111111111, 0.0203125, 0.013888888888888888]}, {'frame': 639, 'bbox': [0.240234375, 0.5972222222222222, 0.02109375, 0.013888888888888888]}, {'frame': 640, 'bbox': [0.23828125, 0.5986111111111111, 0.0203125, 0.013888888888888888]}, {'frame': 641, 'bbox': [0.23515625, 0.5972222222222222, 0.0203125, 0.013888888888888888]}, {'frame': 642, 'bbox': [0.233203125, 0.5986111111111111, 0.02109375, 0.013888888888888888]}, {'frame': 643, 'bbox': [0.230859375, 0.5972222222222222, 0.02109375, 0.013888888888888888]}, {'frame': 644, 'bbox': [0.228125, 0.5986111111111111, 0.0203125, 0.013888888888888888]}, {'frame': 645, 'bbox': [0.225, 0.5986111111111111, 0.0203125, 0.013888888888888888]}, {'frame': 646, 'bbox': [0.22265625, 0.5986111111111111, 0.0203125, 0.013888888888888888]}, {'frame': 647, 'bbox': [0.21953125, 0.5986111111111111, 0.0203125, 0.013888888888888888]}, {'frame': 648, 'bbox': [0.2171875, 0.5972222222222222, 0.0203125, 0.013888888888888888]}, {'frame': 649, 'bbox': [0.21484375, 0.5986111111111111, 0.0203125, 0.013888888888888888]}, {'frame': 650, 'bbox': [0.21171875, 0.5986111111111111, 0.0203125, 0.013888888888888888]}, {'frame': 651, 'bbox': [0.209375, 0.5972222222222222, 0.0203125, 0.013888888888888888]}, {'frame': 652, 'bbox': [0.206640625, 0.5986111111111111, 0.02109375, 0.013888888888888888]}, {'frame': 653, 'bbox': [0.20390625, 0.5986111111111111, 0.0203125, 0.013888888888888888]}, {'frame': 654, 'bbox': [0.2015625, 0.5986111111111111, 0.0203125, 0.013888888888888888]}, {'frame': 655, 'bbox': [0.1984375, 0.5986111111111111, 0.0203125, 0.013888888888888888]}, {'frame': 656, 'bbox': [0.196484375, 0.5986111111111111, 0.02109375, 0.013888888888888888]}, {'frame': 657, 'bbox': [0.193359375, 0.6, 0.02109375, 0.013888888888888888]}, {'frame': 658, 'bbox': [0.19140625, 0.6, 0.0203125, 0.013888888888888888]}, {'frame': 659, 'bbox': [0.18828125, 0.6, 0.0203125, 0.013888888888888888]}, {'frame': 660, 'bbox': [0.185546875, 0.6, 0.02109375, 0.013888888888888888]}, {'frame': 661, 'bbox': [0.183203125, 0.6, 0.02109375, 0.013888888888888888]}, {'frame': 662, 'bbox': [0.180078125, 0.6, 0.02109375, 0.013888888888888888]}, {'frame': 663, 'bbox': [0.17734375, 0.6, 0.0203125, 0.013888888888888888]}, {'frame': 664, 'bbox': [0.175, 0.6, 0.0203125, 0.013888888888888888]}, {'frame': 665, 'bbox': [0.171875, 0.6, 0.0203125, 0.013888888888888888]}, {'frame': 666, 'bbox': [0.169921875, 0.6, 0.02109375, 0.013888888888888888]}, {'frame': 667, 'bbox': [0.166015625, 0.6, 0.02109375, 0.013888888888888888]}, {'frame': 668, 'bbox': [0.16484375, 0.6, 0.0203125, 0.013888888888888888]}, {'frame': 669, 'bbox': [0.161328125, 0.6, 0.02109375, 0.013888888888888888]}, {'frame': 670, 'bbox': [0.158984375, 0.6013888888888889, 0.02109375, 0.013888888888888888]}, {'frame': 671, 'bbox': [0.156640625, 0.6, 0.02109375, 0.013888888888888888]}, {'frame': 672, 'bbox': [0.153125, 0.6013888888888889, 0.021875, 0.013888888888888888]}, {'frame': 673, 'bbox': [0.150390625, 0.6013888888888889, 0.02109375, 0.013888888888888888]}, {'frame': 674, 'bbox': [0.148046875, 0.6013888888888889, 0.02109375, 0.013888888888888888]}, {'frame': 675, 'bbox': [0.144921875, 0.6, 0.02109375, 0.013888888888888888]}, {'frame': 676, 'bbox': [0.142578125, 0.6013888888888889, 0.02109375, 0.013888888888888888]}, {'frame': 677, 'bbox': [0.13984375, 0.6, 0.0203125, 0.013888888888888888]}, {'frame': 678, 'bbox': [0.13828125, 0.6013888888888889, 0.01875, 0.013888888888888888]}, {'frame': 679, 'bbox': [0.134765625, 0.6, 0.01953125, 0.013888888888888888]}, {'frame': 680, 'bbox': [0.1328125, 0.6, 0.0171875, 0.013888888888888888]}, {'frame': 681, 'bbox': [0.131640625, 0.6013888888888889, 0.01484375, 0.013888888888888888]}, {'frame': 682, 'bbox': [0.130859375, 0.6013888888888889, 0.01171875, 0.013888888888888888]}, {'frame': 683, 'bbox': [0.12890625, 0.6, 0.009375, 0.013888888888888888]}, {'frame': 684, 'bbox': [0.131640625, 0.5916666666666667, 0.01484375, 0.016666666666666666]}, {'frame': 685, 'bbox': [0.13046875, 0.5944444444444444, 0.015625, 0.022222222222222223]}]}\n"
     ]
    }
   ],
   "source": [
    "#Трекинг по видео\n",
    "import json\n",
    "results = {\n",
    "    \"metadata\": []\n",
    "}\n",
    "file = \"8car28s.mp4\"\n",
    "video = cv2.VideoCapture(file)\n",
    "# fourcc = cv2.VideoWriter_fourcc(*'XVID')fps=video.get(cv2.CAP_PROP_FPS)\n",
    "# video_vriter = cv2.VideoWriter(file.split('.')[0]+\"_\"+\".avi\", fourcc, fps, (1280, 720))\n",
    "bbox = [(0.4145781250000001*1280 - 0.044046875*1280/2), (0.6100416666666667*720 - 0.029111111111111112*720/2), 0.044046875*1280, 0.029111111111111112*720]\n",
    "ok, image = video.read()\n",
    "if not video.isOpened():\n",
    "    print(\"Could not open video\")\n",
    "    sys.exit()\n",
    "    \n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "print(f\"bbox: {bbox}\")\n",
    "# x, y, w, h = cv2.selectROI( image, fromCenter=False)\n",
    "# init_state = [x, y, w, h]\n",
    "\n",
    "init_state = bbox\n",
    "def _build_init_info(box):\n",
    "            return {'init_bbox': box}\n",
    "tracker.initialize(image, _build_init_info(init_state))\n",
    "frame_number = 1\n",
    "results[\"metadata\"].append({\n",
    "    \"frame\": frame_number,\n",
    "    \"bbox\": [0.4145781250000001, 0.6100416666666667, 0.044046875, 0.029111111111111112]\n",
    "})\n",
    "while True:\n",
    "            ok, image = video.read()\n",
    "            if not ok:\n",
    "                print(\"Can't read frame\")\n",
    "                break\n",
    "\n",
    "            frame_number += 1\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            start = time.time() \n",
    "            out  = tracker.track(image)\n",
    "            state = [int(s) for s in out['target_bbox']]\n",
    "            best_score=out[\"best_score\"].cpu().numpy()[0][0]\n",
    "            end_time = (time.time() - start)\n",
    "            \n",
    "            \n",
    "            org = (50, 50)\n",
    "\n",
    "            # fontScale\n",
    "            fontScale = 1\n",
    "            font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "            # Blue color in BGR\n",
    "            color = (255, 0, 0)\n",
    "            # Line thickness of 2 px\n",
    "            thickness = 2              \n",
    "            # Using cv2.putText() method\n",
    "            image = cv2.putText(image, str(best_score), org, font, \n",
    "                            fontScale, color, thickness, cv2.LINE_AA)\n",
    "            image = cv2.putText(image, str(end_time), (50,100), font, \n",
    "                            fontScale, color, thickness, cv2.LINE_AA)\n",
    "\n",
    "            # x, y, w, h = [int(x) for x in state]\n",
    "            x, y, w, h = [x for x in state]\n",
    "            # print(x, y, w, h)\n",
    "\n",
    "            color = (0, 0, 255)  # Цвет в формате BGR\n",
    "            cv2.rectangle(image, (x, y), (x + w, y + h), color, 2)\n",
    "\n",
    "\n",
    "            cv2.imshow(\"tracking\", image)\n",
    "            # video_vriter.write(image)\n",
    "            \n",
    "            \n",
    "            \n",
    "            results[\"metadata\"].append({\n",
    "                \"frame\": frame_number,\n",
    "                \"bbox\": [(x + w/2)/1280, (y + h/2)/720, w/1280, h/720]\n",
    "            })\n",
    "            \n",
    "            \n",
    "\n",
    "\n",
    "            k = cv2.waitKey(1)            \n",
    "            if k == 32:  # SPACE\n",
    "                ok, image = video.read()                             \n",
    "                x, y, w, h = cv2.selectROI( image, fromCenter=False)\n",
    "                init_state = [x, y, w, h]\n",
    "                tracker.initialize(image, _build_init_info(init_state))\n",
    "            if k == 27:  # ESC\n",
    "                print(results)\n",
    "                break\n",
    "        \n",
    "                \n",
    "                \n",
    "# Сохраняем в JSON файл\n",
    "with open('8car28s_MCITrack.json', 'w') as f:\n",
    "    json.dump(results, f, indent=4)\n",
    "print(results)    \n",
    "cv2.destroyAllWindows()\n",
    "# video.release()\n",
    "# video_vriter.release()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "baf64b99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "502.4700000000001\n",
      "0.4145781250000001\n"
     ]
    }
   ],
   "source": [
    "print(0.4145781250000001*1280 - 0.044046875*1280/2)\n",
    "0.044046875*1280\n",
    "print((502.4700000000001 + 56.379999999999995/2)/1280)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab1d032c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Метрики\n",
    "import numpy as np\n",
    "\n",
    "def iou(boxA, boxB):\n",
    "    # boxA, boxB: [x, y, w, h]\n",
    "    xA = max(boxA[0], boxB[0])\n",
    "    yA = max(boxA[1], boxB[1])\n",
    "    xB = min(boxA[0] + boxA[2], boxB[0] + boxB[2])\n",
    "    yB = min(boxA[1] + boxA[3], boxB[1] + boxB[3])\n",
    "\n",
    "    interW = max(0, xB - xA)\n",
    "    interH = max(0, yB - yA)\n",
    "    interArea = interW * interH\n",
    "\n",
    "    boxAArea = boxA[2] * boxA[3]\n",
    "    boxBArea = boxB[2] * boxB[3]\n",
    "    unionArea = boxAArea + boxBArea - interArea\n",
    "\n",
    "    if unionArea == 0:\n",
    "        return 0.0\n",
    "    return interArea / unionArea\n",
    "\n",
    "def precision(boxA, boxB):\n",
    "    # центры bbox\n",
    "    centerA = (boxA[0] + boxA[2]/2, boxA[1] + boxA[3]/2)\n",
    "    centerB = (boxB[0] + boxB[2]/2, boxB[1] + boxB[3]/2)\n",
    "    dist = np.sqrt((centerA[0] - centerB[0])**2 + (centerA[1] - centerB[1])**2)\n",
    "    return dist\n",
    "sr_thresh = 0.5\n",
    "prec_thresh = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b0fbe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FPS: 27.70\n",
      "Average Overlap (AO): 0.96\n",
      "Success Rate (SR@0.5): 1.00\n",
      "Precision @20px: 1.00\n"
     ]
    }
   ],
   "source": [
    "#Трекинг got10k с метриками\n",
    "import glob\n",
    "import time\n",
    "import  os\n",
    "gt_bboxes = []\n",
    "pred_bboxes = []\n",
    "seq_path = \"val/GOT-10k_Val_000002\"\n",
    "txt_files = glob.glob(os.path.join(seq_path, '*.txt'))\n",
    "if not txt_files:\n",
    "    raise FileNotFoundError(f\"No .txt files found in {seq_path}\")\n",
    "\n",
    "img_files = sorted(glob.glob(os.path.join(seq_path, '*.jpg')))\n",
    "with open(txt_files[0], 'r') as f:\n",
    "    gt_bboxes = [list(map(float, line.strip().split(','))) for line in f]\n",
    "\n",
    "# Получаем размер первого изображения\n",
    "sample_img = cv2.imread(img_files[0])\n",
    "if sample_img is None:\n",
    "    raise ValueError(f\"Failed to read sample image: {img_files[0]}\")\n",
    "\n",
    "# height, width = sample_img.shape[:2]\n",
    "# fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "# output_filename = f\"{seq_path.split('/')[-1]}_output.avi\"\n",
    "# video_vriter = cv2.VideoWriter(output_filename, fourcc, 10, (width, height))  \n",
    "\n",
    "assert len(img_files) == len(gt_bboxes), \"Количество кадров и bbox'ов не совпадает\"\n",
    "\n",
    "x, y, w, h = map(int, gt_bboxes[0])\n",
    "init_state = [x, y, w, h]\n",
    "\n",
    "def _build_init_info(box):\n",
    "            return {'init_bbox': box}\n",
    "\n",
    "counter = 0\n",
    "\n",
    "\n",
    "tracker.initialize(sample_img, _build_init_info(init_state))\n",
    "\n",
    "start_time = time.time()  # Начало замера\n",
    "\n",
    "for img_file, bbox in zip(img_files, gt_bboxes):\n",
    "        \n",
    "        # Читаем изображение\n",
    "        img = cv2.imread(img_file)\n",
    "        if img is None:\n",
    "            print(f\"Не удалось загрузить изображение: {img_file}\")\n",
    "            continue\n",
    "        \n",
    "        \n",
    "        out  = tracker.track(img)\n",
    "        state = [int(s) for s in out['target_bbox']]   \n",
    "                           \n",
    "        # Рисуем bounding box        \n",
    "        x, y, w, h = [int(x) for x in state]\n",
    "\n",
    "        cv2.rectangle(img, (x, y), (x + w, y + h), (0, 0, 200), 2)\n",
    "        \n",
    "        x1, y1, w1, h1 = map(int, bbox)\n",
    "        cv2.rectangle(img, (x1, y1), (x1+w1, y1+h1), (0, 200, 0), 2)\n",
    "        bbox_pred = x, y, w, h\n",
    "        \n",
    "        gt_bboxes.append(bbox)\n",
    "        pred_bboxes.append(bbox_pred)\n",
    "\n",
    "        cv2.imshow(seq_path, img)\n",
    "        # video_vriter.write(img)\n",
    "        counter+=1\n",
    "\n",
    "\n",
    "        # Выход по нажатию 'q' или ESC\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "        if key == ord('q') or key == 27:\n",
    "            break\n",
    "       \n",
    "        \n",
    "                \n",
    "end_time = time.time()    # Конец замера    \n",
    "total_frames = counter       # Общее количество обработанных кадров\n",
    "total_time = end_time - start_time\n",
    "fps = total_frames / total_time\n",
    "ious = [iou(gt, pred) for gt, pred in zip(gt_bboxes, pred_bboxes)]\n",
    "ao = np.mean(ious)\n",
    "sr = np.mean([1 if val >= sr_thresh else 0 for val in ious])\n",
    "precisions = [precision(gt, pred) for gt, pred in zip(gt_bboxes, pred_bboxes)]\n",
    "prec = np.mean([1 if d <= prec_thresh else 0 for d in precisions])\n",
    "\n",
    "print(f\"FPS: {fps:.2f}\")\n",
    "print(f'Average Overlap (AO): {ao:.2f}')\n",
    "print(f'Success Rate (SR@0.5): {sr:.2f}')\n",
    "print(f'Precision @20px: {prec:.2f}')\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "# video_vriter.release()\n",
    "#print(f\"Video saved as: {output_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fde457f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA доступна: True\n",
      "Версия CUDA (PyTorch): 12.8\n",
      "PyTorch version: 2.7.1+cu128\n",
      "TensorRT версия: 10.11.0.33\n",
      "ONNX версия: 1.18.0\n",
      "cuDNN включён в PyTorch: True\n",
      "Версия cuDNN (из PyTorch): 90701\n",
      "Версия onnxruntime: 1.20.1\n",
      "Device onnxruntime: GPU\n",
      "tensorflow не установлен\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(f\"CUDA доступна: {torch.cuda.is_available()}\")\n",
    "print(f\"Версия CUDA (PyTorch): {torch.version.cuda}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")    \n",
    "try:\n",
    "    import tensorrt as trt\n",
    "    print(f\"TensorRT версия: {trt.__version__}\")\n",
    "except:\n",
    "    print(\"TensorRT не установлен\")\n",
    "    \n",
    "try:\n",
    "    import onnx\n",
    "    print(f\"ONNX версия: {onnx.__version__}\")\n",
    "except:\n",
    "    print(\"ONNX не установлен\") \n",
    "print(f\"cuDNN включён в PyTorch: {torch.backends.cudnn.enabled}\")\n",
    "print(f\"Версия cuDNN (из PyTorch): {torch.backends.cudnn.version()}\")\n",
    "try:\n",
    "    import onnxruntime as ort\n",
    "    print(f\"Версия onnxruntime: {ort.__version__}\")\n",
    "    print(f\"Device onnxruntime: {ort.get_device()}\")  # Должно вернуть 'GPU'\n",
    "except:\n",
    "    print(\"onnxruntime не установлен\")\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    print(f\"Версия tensorflow: {tf.__version__}\")\n",
    "except:\n",
    "    print(\"tensorflow не установлен\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f63014f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Доступные провайдеры: ['TensorrtExecutionProvider', 'CUDAExecutionProvider', 'CPUExecutionProvider']\n",
      "Используется: ['CPUExecutionProvider']\n"
     ]
    }
   ],
   "source": [
    "import onnxruntime as ort\n",
    "\n",
    "# Проверка провайдеров\n",
    "print(\"Доступные провайдеры:\", ort.get_available_providers())\n",
    "\n",
    "# Создание сессии с TensorRT\n",
    "try:\n",
    "    sess = ort.InferenceSession(\n",
    "        \"MCITrack.onnx\",\n",
    "        providers=['CUDAExecutionProvider'],\n",
    "        provider_options = [{}],\n",
    "        )\n",
    "    print(\"Используется:\", sess.get_providers())\n",
    "except Exception as e:\n",
    "    print(\"Ошибка:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6b41ae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Доступно GPU: 1\n",
      "Название GPU: NVIDIA GeForce RTX 3060 Ti\n",
      "Вычислительная способность: (8, 6)\n",
      "Общая память: 8191.50 МБ\n",
      "Контекст GPU активен: True\n"
     ]
    }
   ],
   "source": [
    "import pycuda.driver as cuda\n",
    "import pycuda.autoinit  # Автоматически инициализирует GPU\n",
    "\n",
    "# Проверка количества устройств\n",
    "print(f\"Доступно GPU: {cuda.Device.count()}\")\n",
    "\n",
    "# Информация о GPU\n",
    "gpu = cuda.Device(0)\n",
    "print(f\"Название GPU: {gpu.name()}\")\n",
    "print(f\"Вычислительная способность: {gpu.compute_capability()}\")\n",
    "print(f\"Общая память: {gpu.total_memory() / 1024**2:.2f} МБ\")\n",
    "\n",
    "# Проверка контекста (должен быть создан pycuda.autoinit)\n",
    "ctx = cuda.Context.get_current()\n",
    "print(f\"Контекст GPU активен: {ctx is not None}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0974c844",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Проход по всему got10k\n",
    "import glob\n",
    "import time\n",
    "import  os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "metrics = pd.DataFrame(columns=['Path', 'FPS', 'Success Rate (SR@0.5)', \"Average Overlap (AO)\", \"Precision @20px\"])\n",
    "base_dir = \"val/\"\n",
    "folders = os.listdir(f'{base_dir}')\n",
    "counter_test = 0\n",
    "for folder in tqdm(folders):\n",
    "    if folder == \"list.txt\":\n",
    "        print(f\"{'*' * 20} Завершено! {'*' * 20}\")\n",
    "        break\n",
    "    gt_bboxes = []\n",
    "    pred_bboxes = []\n",
    "    seq_path = os.path.join(base_dir, folder)\n",
    "    txt_files = glob.glob(os.path.join(seq_path, '*.txt'))\n",
    "    if not txt_files:\n",
    "        raise FileNotFoundError(f\"No .txt files found in {seq_path}\")\n",
    "\n",
    "    img_files = sorted(glob.glob(os.path.join(seq_path, '*.jpg')))\n",
    "    with open(txt_files[0], 'r') as f:\n",
    "        gt_bboxes = [list(map(float, line.strip().split(','))) for line in f]\n",
    "\n",
    "    # Получаем размер первого изображения\n",
    "    sample_img = cv2.imread(img_files[0])\n",
    "    if sample_img is None:\n",
    "        raise ValueError(f\"Failed to read sample image: {img_files[0]}\")  \n",
    "\n",
    "    assert len(img_files) == len(gt_bboxes), \"Количество кадров и bbox'ов не совпадает\"\n",
    "\n",
    "    x, y, w, h = map(int, gt_bboxes[0])\n",
    "    init_state = [x, y, w, h]\n",
    "\n",
    "    def _build_init_info(box):\n",
    "                return {'init_bbox': box}\n",
    "\n",
    "    counter = 0\n",
    "    counter_test += 1\n",
    "\n",
    "\n",
    "    tracker.initialize(sample_img, _build_init_info(init_state))\n",
    "\n",
    "    start_time = time.time()  # Начало замера\n",
    "\n",
    "    for img_file, bbox in zip(img_files, gt_bboxes):\n",
    "            \n",
    "            # Читаем изображение\n",
    "            img = cv2.imread(img_file)\n",
    "            if img is None:\n",
    "                print(f\"Не удалось загрузить изображение: {img_file}\")\n",
    "                continue\n",
    "                        \n",
    "            out  = tracker.track(img)\n",
    "            state = [int(s) for s in out['target_bbox']]   \n",
    "                            \n",
    "            # Рисуем bounding box        \n",
    "            x, y, w, h = [int(x) for x in state]\n",
    "\n",
    "            cv2.rectangle(img, (x, y), (x + w, y + h), (0, 0, 200), 2)\n",
    "            \n",
    "            x1, y1, w1, h1 = map(int, bbox)\n",
    "            cv2.rectangle(img, (x1, y1), (x1+w1, y1+h1), (0, 200, 0), 2)\n",
    "            bbox_pred = x, y, w, h\n",
    "            \n",
    "            gt_bboxes.append(bbox)\n",
    "            pred_bboxes.append(bbox_pred)\n",
    "   \n",
    "            counter+=1\n",
    "\n",
    "            # Выход по нажатию 'q' или ESC\n",
    "            key = cv2.waitKey(1) & 0xFF\n",
    "            if key == ord('q') or key == 27:\n",
    "                break\n",
    "        \n",
    "            \n",
    "                    \n",
    "    end_time = time.time()    # Конец замера    \n",
    "    total_frames = counter       # Общее количество обработанных кадров\n",
    "    total_time = end_time - start_time\n",
    "    fps = round(total_frames / total_time)\n",
    "    ious = [iou(gt, pred) for gt, pred in zip(gt_bboxes, pred_bboxes)]\n",
    "    ao = np.mean(ious)\n",
    "    sr = np.mean([1 if val >= sr_thresh else 0 for val in ious])\n",
    "    precisions = [precision(gt, pred) for gt, pred in zip(gt_bboxes, pred_bboxes)]\n",
    "    prec = np.mean([1 if d <= prec_thresh else 0 for d in precisions])\n",
    "       \n",
    "    if metrics.empty:\n",
    "        metrics = pd.DataFrame(dict(zip(metrics.columns,\n",
    "        [folder, fps, sr, ao, prec])), index=[0])\n",
    "    else:\n",
    "        metrics = metrics._append(pd.Series(dict(zip(metrics.columns,\n",
    "        [folder, fps, sr, ao, prec]))), ignore_index=True)\n",
    "    print(f\"folder={folder}, fps={fps}, sr={sr}, ao={ao}, prec={prec}\")\n",
    "    # if counter_test == 5:\n",
    "    #     print(f\"{'*' * 20} Прервано на 5! {'*' * 20}\")\n",
    "    #     break\n",
    "\n",
    "metrics = metrics._append(pd.Series(dict(zip(metrics.columns,\n",
    "        [\"Average\", metrics['FPS'].mean(), metrics['Success Rate (SR@0.5)'].mean(), metrics['Average Overlap (AO)'].mean(), metrics['Precision @20px'].mean()]))), ignore_index=True)\n",
    "metrics.to_csv('metrics_base.csv', index=False)\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1caefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Поиск граничных условий\n",
    "import glob\n",
    "import time\n",
    "import  os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "threshold_metrics = pd.DataFrame(columns=['Path', 'FPS', 'Success Rate (SR@0.5)', \"Average Overlap (AO)\", \"Precision @20px\"])\n",
    "base_dir = \"input_data/\"\n",
    "folders = os.listdir(f'{base_dir}')\n",
    "counter_test = 0\n",
    "\n",
    "def bbox_center(bbox):\n",
    "    \"\"\"Вычисляет центр bounding box в формате [x1, y1, x2, y2]\"\"\"\n",
    "    return [(bbox[0] + bbox[2]) / 2, (bbox[1] + bbox[3]) / 2]\n",
    "\n",
    "def mean_euclidean_distance(gt_bboxes, pred_bboxes):  \n",
    "    if len(gt_bboxes) != len(pred_bboxes):\n",
    "        raise ValueError(\"Количество gt_bboxes и pred_bboxes должно совпадать\")\n",
    "    \n",
    "    distances = []\n",
    "    for gt_bbox, pred_bbox in zip(gt_bboxes, pred_bboxes):\n",
    "        # Получаем центры bbox\n",
    "        gt_center = np.array(bbox_center(gt_bbox))\n",
    "        pred_center = np.array(bbox_center(pred_bbox))\n",
    "        \n",
    "        # Вычисляем евклидово расстояние между центрами\n",
    "        distance = np.linalg.norm(gt_center - pred_center)\n",
    "        distances.append(distance)\n",
    "    \n",
    "    return np.mean(distances)\n",
    "\n",
    "for folder in tqdm(folders):   \n",
    "    counter = 0 \n",
    "    gt_bboxes = []\n",
    "    pred_bboxes = []    \n",
    "    seq_path = os.path.join(base_dir, folder)    \n",
    "    video_files = glob.glob(os.path.join(seq_path, '*.mp4'))\n",
    "    txt_files = glob.glob(os.path.join(seq_path, '*.txt'))\n",
    "    if not video_files:\n",
    "        raise FileNotFoundError(f\"No files found in {seq_path}\")    \n",
    "    for files in folder:\n",
    "        file = video_files[counter]\n",
    "        with open(txt_files[counter], 'r') as f:\n",
    "            # Пропускаем заголовок\n",
    "            next(f)\n",
    "            for line in f:\n",
    "                # Разбиваем строку на части\n",
    "                parts = line.strip().split()\n",
    "                # Извлекаем нужные значения (object_x, object_y, object_width, object_height)\n",
    "                x = int(parts[1])\n",
    "                y = int(parts[2])\n",
    "                width = float(parts[3])\n",
    "                height = float(parts[4])\n",
    "                # Добавляем bounding box в список\n",
    "                gt_bboxes.append([x, y, width, height])\n",
    "\n",
    "        video = cv2.VideoCapture(file)\n",
    "        #fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "        #fps=video.get(cv2.CAP_PROP_FPS)\n",
    "        #video_vriter = cv2.VideoWriter(file.split('.')[0]+\"_\"+\".avi\", fourcc, fps, (1920, 1080))\n",
    "\n",
    "        ok, image = video.read()\n",
    "        if not video.isOpened():\n",
    "            print(\"Could not open video\")\n",
    "            sys.exit()\n",
    "            \n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        x, y, w, h = cv2.selectROI( image, fromCenter=False)\n",
    "        init_state = [x, y, w, h]\n",
    "        def _build_init_info(box):\n",
    "                    return {'init_bbox': box}\n",
    "        tracker.initialize(image, _build_init_info(init_state))\n",
    "        \n",
    "        while True:\n",
    "                    ok, image = video.read()\n",
    "                    if not ok:\n",
    "                        print(\"Can't read frame\")\n",
    "                        break\n",
    "                    \n",
    "                    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "                    start = time.time() \n",
    "                    out  = tracker.track(image)\n",
    "                    state = [int(s) for s in out['target_bbox']]\n",
    "                    best_score=out[\"best_score\"].cpu().numpy()[0][0]\n",
    "                    end_time = (time.time() - start)\n",
    "                                    \n",
    "                    org = (50, 50)\n",
    "\n",
    "                    # fontScale\n",
    "                    fontScale = 1\n",
    "                    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "                    # Blue color in BGR\n",
    "                    color = (255, 0, 0)\n",
    "                    # Line thickness of 2 px\n",
    "                    thickness = 2              \n",
    "                    # Using cv2.putText() method\n",
    "                    image = cv2.putText(image, str(best_score), org, font, \n",
    "                                    fontScale, color, thickness, cv2.LINE_AA)\n",
    "                    image = cv2.putText(image, str(end_time), (50,100), font, \n",
    "                                    fontScale, color, thickness, cv2.LINE_AA)\n",
    "\n",
    "                    x, y, w, h = [int(x) for x in state]\n",
    "\n",
    "                    \n",
    "                    cv2.rectangle(image, (x, y), (x + w, y + h), (0, 0, 200), 2)\n",
    "                    \n",
    "                    x1, y1, w1, h1 = map(int, bbox)\n",
    "                    cv2.rectangle(image, (x1, y1), (x1+w1, y1+h1), (0, 200, 0), 2)\n",
    "\n",
    "                    cv2.imshow(\"tracking\", image)\n",
    "                    #video_vriter.write(image)\n",
    "\n",
    "                    k = cv2.waitKey(1)            \n",
    "                    if k == 32:  # SPACE\n",
    "                        ok, image = video.read()                             \n",
    "                        x, y, w, h = cv2.selectROI( image, fromCenter=False)\n",
    "                        init_state = [x, y, w, h]\n",
    "                        tracker.initialize(image, _build_init_info(init_state))\n",
    "                    if k == 27:  # ESC\n",
    "                        break\n",
    "        counter += 1\n",
    "        \n",
    "                \n",
    "                \n",
    "metrics = metrics._append(pd.Series(dict(zip(metrics.columns,\n",
    "        [\"Average\", metrics['FPS'].mean(), metrics['Success Rate (SR@0.5)'].mean(), metrics['Average Overlap (AO)'].mean(), metrics['Precision @20px'].mean()]))), ignore_index=True)\n",
    "metrics.to_csv('metrics_base.csv', index=False)\n",
    "metrics\n",
    "cv2.destroyAllWindows()\n",
    "#video.release()\n",
    "#video_vriter.release()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
